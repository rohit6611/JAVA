1. Session is the main interface that is used to establish the physical connection between the application and the database.
   Its main function is to perform read,update,delete and insert operation for instances of mapped classes in db.
   Lifecycle of session is bounded to the beginning and end of the transaction. associated insatances may exist in three states:
   transient - when object is not associated to any of the session.
   persistent - when object is associated to session. object can be persisted using persist(),save(),saveOrUpdate().
   detached - previously persistent, not associated to any session.delete()
   Session is not thread safe ie each thread/transaction should obtain its instance from sessionFactory.
   Session instance is serializable if its associated entity class is serializable.
   flush() forces the hibernate to execute the statement in db but if dont commit you won't see the changes in the database.
   flush() is the process of synchronization of the state of the persistence context or object and database.

2. EntityManager can be used to interact with the persistence context in JPA. It provides methods for managing entity lifecycle, 
   querying entities, and performing CRUD operations on the database.
   The flush modes in JPA determine when the changes made to entities in the persistence context are synchronized with the database.
   The two main flush modes provided by JPA are AUTO and COMMIT.
   AUTO is the default flush mode. It means that changes made to entities are automatically synchronized with the database when necessary,
   such as when a transaction is committed, or when a query is executed that requires up-to-date data.
   On the other hand COMMIT mode delays synchronization until the transaction is committed. This means that changes made to entities will not
   be visible to other transactions until the current transaction is committed.	

3. Serialization in Java is the process of converting an object's state into a byte stream, which can then be used to save the object or transmit
   it over a network.
   Serialization   Converts an object into a byte stream.
   Deserialization Converts a byte stream back into an object.
   Serialization is useful for saving data, transmitting information, and storing objects in a database. 
   To avoid compromising a class, you can mark fields that contain sensitive data as private transient.
   Some objects, such as threads, sockets, and file handles, cannot be serialized.
   It is because SerialVersionUID is used to ensure that during deserialization the same class (that was used during serialize process) is loaded.

4. Composite key is a combination of two or more columns to form a primary key.
   it has 4 rules - composite primary key class must be public.
   There should be no args constructor.
   must provide equals and hashcode method.
   must implement serializable.

   @Embeddable
public class BookId implements Serializable {
    private String title;
    private String language;

    // default constructor

    public BookId(String title, String language) {
        this.title = title;
        this.language = language;
    }

    // getters, equals() and hashCode() methods
}  

@Entity
public class Book {
    @EmbeddedId
    private BookId bookId;

    // constructors, other fields, getters and setters
}

CREATE TABLE book (
    title VARCHAR(255) NOT NULL,
    language VARCHAR(255) NOT NULL,
    -- other columns
    PRIMARY KEY (title, language)
);

/**
     * This method returns a List object containing Person objects querying their age range and ending mail using
     * a native query.
     * @param minAge the minimum age
     * @param maxAge the maximum age
     * @param endingMail the ending mail
     * @return a List object containing Person objects
     */
    @Query(value = "SELECT * FROM person " +
            "WHERE TIMESTAMPDIFF(YEAR, date_of_birth, CURRENT_DATE) BETWEEN :minAge AND :maxAge " +
            "AND LOWER(email) LIKE LOWER(CONCAT('%', :endingMail))", nativeQuery = true)
    List<Person> retrievePersonsByAgeAndEMailNative(@Param("minAge") int minAge, @Param("maxAge") int maxAge,
                                   @Param("endingMail") String endingMail);

5. Spring Data JPA uses Hibernate as the default ORM (Object-Relational Mapping), which provides an inbuilt First-Level-Cache. 
   Each session has its own cache, which we refer to as the First-Level-Cache.
   The first-level cache is localized to a single session and helps to reduce database queries by caching retrieved data. The second-level
   cache is shared across sessions and enables data to be cached across multiple requests.
   The first-level cache in Hibernate is implemented using an internal HashMap that maps the entity identifier to the corresponding entity instance. 
   When Hibernate needs to retrieve an entity, it first checks the first-level cache to see if the entity is already present. If it is, Hibernate returns
   the cached entity instance instead of querying the database. If the entity is not present in the first-level cache, Hibernate queries the database and 
   stores the retrieved entity in the first-level cache before returning it to the application.	

6. CORS is a security measure that lets to share resources such as web pages, api calls data from other domain.

7. Even though Spring Data JPA abstracts away the EntityManager, the first-level cache is still tied to the EntityManager. Each repository method invocation
   typically operates within the context of a single EntityManager.
   How it works:
Every @Transactional method (or repository method call) in Spring uses an EntityManager provided by the EntityManagerFactory.
The first-level cache is scoped to this EntityManager.
Hibernate keeps track of the entities retrieved or persisted within the current EntityManager and avoids hitting the database for the same entity within the transaction or session.	
@Transactional
public void someServiceMethod() {
    Book book1 = bookRepository.findById(1L).get(); // Database hit
    Book book2 = bookRepository.findById(1L).get(); // Served from first-level cache (same transaction)
}

The first-level cache is automatically managed by Hibernate within the transaction boundary.
When the transaction ends, the EntityManager is closed, and the first-level cache is cleared.

Spring Data JPA can leverage Hibernate's second-level caching if explicitly enabled. The second-level cache is shared across sessions (EntityManagers) and is independent of individual 
transaction lifecycles.

JPARepository is responsible for converting findById() into appropriate query.

8. Run method on Spring Boot startup
  1 Using CommandLineRunner interface CommandLineRunner is a spring boot functional interface which is used to run code at application startup. It is present under      
 package org.springframework.boot.In startup process after the context is initialized, spring boot calls its run() method with command-line arguments provided to the  application.

 @Component
public class CommandLineRunnerImpl implements CommandLineRunner {

    @Override
    public void run(String... args) throws Exception {
        System.out.println("In CommandLineRunnerImpl ");

        for (String arg : args) {
            System.out.println(arg);
        }
    }
}
  2 Using application runner interface 
@Component
public class ApplicationRunnerImpl implements ApplicationRunner {

   @Override
   public void run(ApplicationArguments args) throws Exception {

      System.out.println("ApplicationRunnerImpl Called");

  3  @Postconstruct annotation on a method
 Whenever a method is marked with this annotation, it will be called immediately after the dependency injection.

Using CommandLineRunner or ApplicationRunner
These interfaces provide a way to execute code after the application context is fully initialized.
Basically command line runner is used in case we want to log some message or start any method like for sending notification after the spring context is fully initialized.
and post construct is bean specific annotation and is used when to execute logic immediately after the bean's properties are set and dependency injection is complete, but before the 
Spring Application Context is fully initialized.eg validating bean properties.
If you need application-wide startup logic, prefer CommandLineRunner.
If you need bean-specific initialization logic, prefer @PostConstruct.

9. Spring Bean Scopes
   There are five types of spring bean scopes:

	singleton - only one instance of the spring bean will be created for the spring container. This is the default spring bean scope. While using this scope, make 	sure bean doesn’t
        have shared instance variables otherwise it might lead to data inconsistency issues.
	prototype – A new instance will be created every time the bean is requested from the spring container.
	request – This is same as prototype scope, however it’s meant to be used for web applications. A new instance of the bean will be created for each HTTP request.
	session – A new bean will be created for each HTTP session by the container.
	global-session – This is used to create global session beans for Portlet applications.

@Bean
@Scope(value="prototype")
public MyBean myBean() {
	return new MyBean();
}

10. Java equals() and hashCode() methods are present in Object class. So every java class gets the default implementation of equals() and hashCode().
    char c = 'a';
    char b = 'e';
    System.out.println(c - b); // Output: -4

	Java Object hashCode() is a native method and returns the integer hash code value of the object.
	If two objects are equal according to equals() method, then their hash code must be same.
	If two objects are unequal according to equals() method, their hash code are not required to be different. Their hash code value may or may-not be equal.

if you override only one that will lead to inconsistence behaviour like if generating hashcode on desired requirement and not overriding equals method will return false

According to the Java specification, if two objects are considered equal by the equals() method, they must have the same hashCode() and vice versa is not true.
the default implementation of equals() check the reference of the object.

11. TRACE	Detailed tracing information, method calls, and variable states. Useful during development.	"Entering method XYZ with parameters..."
    DEBUG	General debugging information for developers, showing key internal operations or behaviors.	"Processed payment for user {} with amount {}"
    INFO	General application flow and important events (e.g., start, shutdown, user actions).	"User {} logged in" 
    WARN	Potential issues that don't stop execution but should be checked.	"Cache miss for user {}. Using fallback."
    ERROR	Serious errors that cause failures or issues requiring attention.	"Error processing payment for user {}: {}", exception details"
    FATAL	Critical, application-halting errors (not always used in every framework).	"System crash, cannot continue execution"

Best Practices for Choosing Log Levels:
Use TRACE and DEBUG for development: These levels are typically turned off in production because they can generate too much information, which might hurt performance.
Use INFO for normal operation messages: Important milestones, events, or statuses that don't indicate issues but are important for understanding application flow.
Use WARN for potential issues that don’t need immediate fixing but could lead to problems.
Use ERROR for failures that need attention from developers or administrators, indicating a serious problem in the system.
Use FATAL (if available) for catastrophic failures that require immediate attention and typically result in the application shutting down.

Set the default logging level to INFO: This means that any log statements with a severity level of INFO or higher (i.e., WARN, ERROR, or FATAL) will be logged, but lower levels like 
DEBUG and TRACE will not be logged.

12. Elastic search is a NoSQL database that is based on lucene search engine and it is search and analytics based engine used for data storage and indexing of large
    amount of data at real time.
    Logstash is a pipeline that reads data from various source , processed it and sends to elastic search.
    Kibana is used in conjuction with elastic search and used for visualizing and analyzing logs, metrics and certain events.

13. JMM - out of order execution 
	a = 3         load a - set value of a - load b - set value of b - load a - increment a by 1;
	b = 2
	a = a+1  

	but jvm or compiler can optimize the performance by
	a=3
	a=a+1
	b=2

14. If you serialize class and have not implemented serializable interface it will give object not serializable error as java is very secure language and you need to 
    define that class is serializable.
    
	Save obj= new Save();
	
	File f = new File("obj.txt")
	FileOutputStream fos = new FileOutputStream(f);
	ObjectOutputStream oos =new ObjectOutputStream(fos);
	oos.write(obj);

	FileInputStream fis = new FileInputStream(f);
	ObjectInputStream ois = new ObjectInputStream(fis);
	Save ob = (Save)ois.readData();

	ObjectOutputStream in Java is used for serialization, which is the process of converting an object into a byte stream so that it can be saved to a file, 	
	transmitted over a network, or otherwise persisted or transferred. This allows the object's state to be saved and later reconstructed.

15. Dynamic method dispatch allow java to choose which method to choose when method are overridden in child class.
	Can constructors be polymorphic?
	No, constructors cannot be polymorphic. We can have many constructors in a class
	with different inputs, but they don’t behave differently based on the object type like
	methods do.

16. How does the concept of default methods in interfaces help resolve the diamond problem?
    Default methods allow interfaces to provide method implementations, and in case of
    conflicts (multiple interfaces with the same default method), the implementing class
    must override the method, resolving ambiguity. 

17. Is it possible to serialize static fields in Java? Why or why not?
	No, static fields are not serialized in Java because they belong to the class, not to
	individual instances. Serialization is intended to capture the state of an object, and
	static fields are part of the class's state, not the object's state
	To override a method, the new method in the subclass must have the same name, return
	type, and parameters as the method in the parent class. Also, the new method should not
	be less accessible than the original.
	Can 'this' keyword be assigned a new value in Java?
	No, this keyword cannot be assigned a new value in Java. It is a read-only reference
	that always points to the current object.
	What happens if you attempt to use the "super" keyword in a class that doesn't have a superclass?
	If we attempt to use the "super" keyword in a class that doesn't have a superclass, a
	compilation error occurs. The "super" keyword is only applicable within subclasses to
	refer to members of the superclass.

18. Static methods, however, are resolved at compile-time based on the type of the reference, not the object.
	class Parent {
    static void display() {
        System.out.println("Parent static method");
    }
}

class Child extends Parent {
    static void display() {
        System.out.println("Child static method");
    }
}

public class Main {
    public static void main(String[] args) {
        Parent obj1 = new Parent();
        Parent obj2 = new Child();

        obj1.display(); // Output: Parent static method
        obj2.display(); // Output: Parent static method (NOT Child static method)
    }
}

public class Main {
    public static void main(String[] args) {
        Parent obj1 = new Parent();
        Child obj2 = new Child();

        obj1.display(); // Output: Parent static method
        obj2.display(); // Output: Child static method
    }
}

19. Advantages of using a functional interface.
	Functional interfaces, which contain only one abstract method, are key to enabling
	functional programming in Java. They offer concise and readable code through
	lambda expressions and method references, improving code simplicity. Functional
	interfaces allow easy parallel processing, better abstraction, and reusability,
	especially in scenarios like streams and event handling, promoting a cleaner and
	more expressive programming style.

20.  Which is faster, traditional for loop or Streams?
	Traditional for loops are generally faster due to less overhead, but Streams provide
	better readability and are optimized for parallel processing in large datasets.
	In which scenarios would you prefer traditional for loops and streams?
	Use traditional loops for simple, small datasets requiring maximum performance.
	Use Streams for more complex data transformations or when working with large
	datasets where readability, maintainability, and potential parallelism are prioritized.

21. Stream intermediate operation are lazy meaning that they define what and how to performed and not processed until terminal operation is called.
	List<Integer> numbers = List.of(1, 2, 3, 4, 5);

Stream<Integer> stream = numbers.stream()
                                .filter(n -> {
                                    System.out.println("Filter: " + n);
                                    return n % 2 == 0;
                                }) // Lazy
                                .map(n -> {
                                    System.out.println("Map: " + n);
                                    return n * 2;
                                }); // Lazy

// No output yet because no terminal operation is called.

stream.forEach(n -> System.out.println("Result: " + n));

// Output:
// Filter: 1
// Filter: 2
// Map: 2
// Result: 4
// Filter: 3
// Filter: 4
// Map: 4
// Result: 8
// Filter: 5

22. If a class implements two interfaces that have static methods with the same name, there is no conflict because static methods in interfaces are not inherited by the implementing class. Static methods in interfaces are associated with the interface itself and can only be called using the interface name.

If a class implements two interfaces that both define default methods with the same name and signature, a conflict arises, and the compiler will force the implementing class to explicitly 
resolve the ambiguity by overriding the method.

23. String.join() - String[] words = {"Java", "Python", "C++"};
	String result = String.join(", ", words);
	System.out.println(result); // Output: Java, Python, C++

24. What is the difference between Iterator and listIterator?
	Iterator allows forward traversal of a collection, while ListIterator extends Iterator
	functionality to allow bidirectional traversal of lists and also supports element
	modification.

25. How does a HashSet ensure that there are no duplicates?
	A HashSet in Java uses a HashMap under the hood. Each element you add is treated
	as a key in this HashMap. Since keys in a HashMap are unique, HashSet automatically
	prevents any duplicate entries.
	Can you describe how hashCode() and equals() work together in a collection
	hashCode() determines which bucket an object goes into, while equals() checks
	equality between objects in the same bucket to handle collisions, ensuring that each
	key is unique.
	Yes, you can use a Class as a key in a HashMap in Java. The Class object is a reference to the runtime representation of a Java class, and it can be used as a 	key in a HashMap 
	because it overrides the hashCode() and equals() methods, which are required for proper functioning as a key.

26. HashMap uses an array of nodes, where each node is a linked list or Tree depend
	upon the collisions and java versions ( From Java 8 onwards, if there is high hash
	collisons then linkedList gets converted to Balanced Tree).
	TreeMap uses a Red-Black tree, which is a type of self-balancing binary search tree.
	Each node in the Red-Black tree stores a key-value pair.
	HashSet internally uses a HashMap whereas TreeSet internally uses TreeMap.

27. Design patterns are proven solutions for common software design problems. They provide
	standardized approaches to organize code in a way that is maintainable, scalable, and
	understandable.

28.	What are SOLID Principles?
	'S' stands for Single Responsibility Principle: It means a class should only have one reason
	to change, meaning it should handle just one part of the functionality.
	For Example: A class VehicleRegistration should only handle vehicle registration details. If it
	also takes care of vehicle insurance, then it will violates this.
	'O' stands for Open/Closed Principle: It means Classes should be open for extension but
	closed for modification.
	For Example: We have a VehicleService class that provides maintenance services. Later, we
	need to add a new service type for electric vehicles and if without modifying VehicleService,
	we are able to extend it from a subclass ElectricVehicleService then it follows this priciple.
	'L' stands for Liskov Substitution Principle: It means Objects of a superclass should be
	replaceable with objects of its subclasses without affecting the program’s correctness.
	For Example: If we have a superclass Vehicle with a method startEngine(), and subclasses
	like Car and ElectricCar, we should be able to replace Vehicle with Car or ElectricCar in our
	system without any functionality breaking. If ElectricCar can't implement startEngine()
	because it doesn’t have a traditional engine, it should still work with the interface to not
	break the system.
	'I' for Interface Segregation Principle: It means do not force any client to depend on
	methods it does not use; split large interfaces into smaller ones.
	For Example: Instead of one large interface VehicleOperations with methods like drive,
	refuel, charge, and navigate, split it into focused interfaces like Drivable, Refuelable, and
	Navigable. An ElectricCar wouldn't need to implement Refuelable, just Chargeable and
	Navigable.
	'D' stands for Dependency Inversion Principle: It means High-level modules should not
	depend directly on low-level modules but should communicate through abstractions like
	interfaces.
	For Example: If a VehicleTracker class needs to log vehicle positions, it shouldn't depend
	directly on a specific GPS device model. Instead, it should interact through a GPSDevice
	interface, allowing any GPS device that implements this interface to be used without
	changing the VehicleTracker class.

20.	The Young Generation stores newly created objects. The Old Generation holds
	objects that have survived several garbage collection cycles in the Young Generation.

21.  	Thread is a lightweight process and is the smallest executable unt. Thread has its own path of execution. A process can have multiple threads.
  Parent thread is that thread from which any thread is started.
  
  A process is an executable instance of an application.
  
  An application is a program that is designed to perform a specific task.
  
  User Threads are threads that are created by the application or user and they have high priority and jvm can't exit without completing user threads. jvm will wait
  for user threads to complete their execution.
  On the other hand daemon threads are threads that are created by the jvm and run in the background like garbage collection. they are less priority threads.jvm will exit as 
  soon as all user threads complete their execution.
  
  When the JVM shuts down after all user threads have completed their execution, it means your program has terminated.
  The JVM releases all resources, including memory, open file descriptors, and any other resources allocated during the program's execution.
  If your Java application interacts with native libraries or external processes, those connections or processes might terminate unless explicitly handled.
  If you register a shutdown hook, it will execute just before the JVM shuts down. This can be used to clean up resources, close connections, or save state.
  Runtime.getRuntime().addShutdownHook(new Thread(() -> {
    System.out.println("JVM is shutting down. Performing cleanup...");
  }));
  Default priority of thread is same as of its parent thread.
  
  Thread states - new() - thread is in new state before calling start() method.
    runnable() - when thread.start() is called.
	blocked() - when the thread is waiting for object lock to enter into synchronized block or in case of deadlock scenario.
	waiting() - thread is in waiting state when sleep() or join() method is called.
	timed waiting() - when sleep() or wait() or join() with timeout is called.
	terminated() - when thread completes its execution.

   Springboot app doesn't stops automatically because it remains alive till application context is alive and by by default its internal structure is that its life
   cycle creates non-deamon threads that keeps it alive and till when application context is alive . to do so you need manage spring lifecycle explicitly.

    Two ways of creating thread.
    1. Extending thread class.  public class MyThread extends Thread{
		@Override
		public void run(){
		// my own task
	}
	}

	In main class - MyThread thread = new MyThread();
			thread.start();
	
    2. Implementing runnable interface.
	for creating instance MyThread thread = new MyThread(new Thread());

In most cases, implementing the Runnable interface is considered a better practice than extending the Thread class when creating threads in Java because it allows for greater flexibility 
and avoids limitations related to single inheritance in Java; meaning you can still inherit from another class if needed.

    Any Thread wants to enter into Synchronized method , block must acquire the lock first to enter into synchronized block , method.
	
    join() - allows for currently executing thread to wait for some other thread to finish its execution.(in case eg we want to result in order)

    yield() - allows currently executing thread to temporarily pause its execution and allows some other thread to execute.

    wait() - allows currently executing thread to release this object lock and wait until other thread notifies it.

    notify() - this wakes up any of the object that is waiting for the this object lock.

    notifyAll() - this wakes up all threads that are waiting for this object lock and at a time only one thread acquires the lock.

22.	Generics was added in Java 5 to provide compile-time type checking and removing risk of ClassCastException that was common while working with collection classes. The whole 
        collection framework was re-written to use generics for type-safety. Let’s see how generics help us using collection classes safely.	 
	List list = new ArrayList();
list.add("abc");
list.add(new Integer(5)); //OK

for(Object obj : list){
	//type casting leading to ClassCastException at runtime
    String str=(String) obj; 
}

Above code compiles fine but throws ClassCastException at runtime because we are trying to cast Object in the list to String whereas one of the element is of type Integer. After Java 5, 
we use collection classes like below.

List<String> list1 = new ArrayList<String>(); // java 7 ? List<String> list1 = new ArrayList<>(); 
list1.add("abc");
//list1.add(new Integer(5)); //compiler error

for(String str : list1){
     //no type casting needed, avoids ClassCastException
}

If we don’t provide the type at the time of creation, the compiler will produce a warning that “GenericsType is a raw type. References to generic type GenericsType<T> should be 
parameterized”. When we don’t provide the type, the type becomes Object and hence it’s allowing both String and Integer objects.

we can have generic class and interface.

23.   As the name suggests, wrapper classes are objects encapsulating primitive Java types.

Each Java primitive has a corresponding wrapper:

boolean, byte, short, char, int, long, float, double 
Boolean, Byte, Short, Character, Integer, Long, Float, Double
These are all defined in the java.lang package, hence we don’t need to import them manually.

	What’s the purpose of a wrapper class?”. It’s one of the most common Java interview questions.

Basically, generic classes ( <T> ) only work with objects and don’t support primitives. As a result, if we want to work with them, we have to convert primitive values into wrapper objects.

24. In composition one class contains object of another class and if it is strong aggregation then it is called composition else aggregation.
  Association is a process which determines how an object is associated with other object it is of type onetoone onetomany manytoone manytomany.
  in compostion the classes can not exist independently but in aggregation they can exist independently.like room can't exist without house so it is strong reference.
  but departments in school may exist independently.
  when you want parent class to own child class then use composition else use aggregation.
  
  Steps to create class as immutable - make class final,make its member as private and final , dont provide setter and initialize instance sing constructor.
  import java.util.Date;

 public final class ImmutableClass {
    private final String name;
    private final int age;
    private final Date dateOfBirth; // Mutable object

    // Constructor to initialize all fields
    public ImmutableClass(String name, int age, Date dateOfBirth) {
        this.name = name;
        this.age = age;
        // Create a deep copy of the mutable object
        this.dateOfBirth = new Date(dateOfBirth.getTime());
    }

    // Getters
    public String getName() {
        return name;
    }

    public int getAge() {
        return age;
    }

    public Date getDateOfBirth() {
        // Return a copy of the mutable object
        return new Date(dateOfBirth.getTime());
    }

    @Override
    public String toString() {
        return "ImmutableClass{" +
                "name='" + name + '\'' +
                ", age=" + age +
                ", dateOfBirth=" + dateOfBirth +
                '}';
    }
 }

 Use Static method in interface if you want to associate any method to interface and that can be accesed using interface name.
 Default method is used to provide some default implementation in the interface.
 If a class implements two interfaces that have the same default method, a conflict arises because the compiler cannot determine 
 which default method to inherit. This is often referred to as the "diamond problem" in interfaces.
 
 Use an abstract class when classes share a significant amount of common code or state.
 Example: If you have common fields (e.g., id, name) or methods (e.g., toString()), an abstract class is more appropriate.If there 
 is a "is-a" relationship and you want to enforce a shared base class.
 An interface defines a contract that implementing classes must follow. It specifies what a class should do but not how it should do it.
 Use an interface when unrelated classes need to implement the same functionality.
 Example: Classes like Car, Boat, and Airplane can implement a Vehicle interface with a move() method
 
 The Reflection API in Java allows a program to inspect and manipulate the runtime behavior of classes, interfaces, methods, fields, and
 constructors. It is part of the java.lang.reflect package and enables dynamic access to an object's structure and behavior.

25. The main use of java static keyword is as follows:

The static keyword can be used when we want to access the data, method, or block of the class without any object creation.
It can be used to make the programs more memory efficient.

When a class is loaded into the memory at runtime, the static variable is created and initialized into the common memory location only once.
All static variables are stored in PermGen space of the heap memory.
Static variable cannot be serialized in Java whereas, instance variable can be serialized.
Static instace variable can be accesed inside instantiation block.
Yes, we can access static members from an instance method in java.
No, it is not possible to access instance members like instance variable and instance method from a static method.
You can call static method from instance method but from static method you can't call to instance method.
Yes, Duplicate method error. This is because we cannot declare a static method and instance method with the same signature in the same class.
When the dot class file is loaded into memory, static block is executed. After executing the static block, JVM calls the main method to start execution. Therefore, static block is 
executed before the main method.

26. No, we cannot define private and protected modifiers for variables in interface because the fields (data members) declared in an interface are by default public, static, and final.
    Only abstract and public modifiers are allowed for methods in interfaces.
    we cannot create an object of interface using new operator. But we can create a reference of interface type and interface reference refers to objects of its 
    implementation classes.

	interface A {
  int x = 10;	
  void m1();
}
public class B implements A {
  int x = 20;
  public void m1(){
     System.out.println("One"); 	
  }
}
public class Test {
public static void main(String[] args){
   A a = new B();
   a.m1();
   System.out.println(a.x);
  }
}
Output: One, 10.

 An Interface that doesn’t have any data members or methods is called marker interface in java. For example, Serializable, Cloneable.
	No, while overriding any interface methods, we must use public only. This is because all interface methods are public by default. We cannot reduce the visibility while overriding 
	them.

27 . public class A 
{ 
private void m1()
{ 
 System.out.println("m1-A"); 
 } 
} 
public class B extends A
{ 
private void m1()
{ 
 System.out.println("m1-B"); 
} 
public static void main(String[] args) 
{ 
 B b = new B(); 
 b.m1(); 
 } 
}
Ans: No, Compile time error.

Explanation: The overriding concept is not applicable to the private method. Parent class private method is not visible in the child class. Keep in mind. Based on our requirement, we can 
define exactly the same private method in the child class. The code is valid but not overriding.

28. setter di provides more flexibility as the beans can be change even after creation while durig constructor injection dependencies are provided only once at the time of object creation.

 No, singleton beans in Spring are not thread-safe by default. Because they are shared by multiple
parts of the application at the same time, we need to add extra code to make them safe for use by
multiple threads. This usually means using synchronized methods or thread-safe data structures.

 The prototype scope in Spring means that a new instance of a bean is created each time it is needed.
Unlike the singleton scope, which uses the same instance, the prototype scope gives a fresh,
separate bean for every request. This is useful when we need a new instance for each user or
operation.

 Spring Boot CLI (Command Line Interface) helps us quickly create and run Spring applications using
simple scripts. It makes development easier by reducing setup and configuration. Common
commands are ‘spring init’ to start a new project, ‘spring run’ to run scripts, ‘spring test’ to run tests,
and ‘spring install’ to add libraries. These commands make building and testing Spring apps faster
and simpler.

 If a starter dependency includes conflicting versions of libraries with other dependencies, Spring
Boot's dependency management resolves this by using a concept called "dependency resolution." It
ensures that only one version of each library is included in the final application, prioritizing the most
compatible version. This helps prevent runtime errors caused by conflicting dependencies and
ensures the smooth functioning of the application.

 We can disable specific auto-configuration classes in Spring Boot by using the exclude attribute of the
@EnableAutoConfiguration annotation or by setting the spring.autoconfigure.exclude property in our
application.properties or application.yml file.
@EnableAutoConfiguration(exclude = MailSenderAutoConfiguration.class)

 To list all the beans loaded by the Spring ApplicationContext, we can inject the ApplicationContext
into any Spring-managed bean and call the getBeanDefinitionNames() method. This will return a
String array containing the names of all beans managed by the context.

What is the purpose of having a spring-boot-starter-parent?
The spring-boot-starter-parent in a Spring Boot project provides a set of default configurations for
Maven. It simplifies dependency management, specifies common properties like Java version, and
includes useful plugins. This parent POM ensures consistent versions of dependencies and plugins,
reducing the need for manual configuration and helping maintain uniformity across Spring Boot
projects.

How does Spring Boot decide which embedded server to use if multiple options are available
in the classpath?
Spring Boot decides which embedded server to use based on the order of dependencies in the
classpath. If multiple server dependencies are present, it selects the first one found. For example, if
both Tomcat and Jetty are present, it will use the one that appears first in the dependency list.

29. Transaction helps to achieve acid properties.
   we can use @EnableTransactionManagement for handling transactions but this thing is by default auto configure by springboot.
   If above annotation is not auto configured by springboot then @Transactional will not work as supposed.
   @Transactional can be used to apply over method and class and not if applied over method then it will work only for public method and not for private method.
   @Transactional annotation works by creating a proxy around the target class. However, Spring's proxy mechanism only applies to public methods. This is why the      
   @Transactional annotation does not work on private methods.
It internaly uses AOP.
One pointcut expression matches and run around type advice and advice is invokeWihinTransaction method present inside TransactionalInterceptor class.
Isolation level in context to transaction tells how the changes made by one transaction are visible are visible to other transactions running in parallel.
isolation level depends on the db which we are using but in most relational db by default isolation level is read_commited.

Dirty read occurs when some other transaction reads uncommited transaction data from this transaction.
Suppose if a transaction A reads the same row data multiple times and there is a chance that it gets different value then it is known as non-repeatable read problem.
eg you are reading row 1 status and everytime you got its status free and thats status is changed by some other transaction to booked then when same transaction reads
that row 1 it will get status booked that is non repeatable read problem.
If suppose a transaction A queries the db multiple times and there is a chance that you can get different rows in result set than it is know as phantom read problem.
A data item is locked for reading using a Shared Lock (S), sometimes referred to as a read lock. Other transactions can get a shared lock on a data item that a transaction is holding, but 
they are not able to obtain an exclusive lock to alter it.
A data item is locked for writing using an Exclusive Lock (X), sometimes referred to as a write lock. No other transaction is able to access or alter a data item that it has an exclusive 
lock on until the lock is released.
If a transaction has acquired an exclusive lock then any other transaction can't read it until that transaction is commited or rollbacked.
read_uncommited is only used when you have only read application as it has all above 3 problems.
read_commited solve dirty read problem. as for reading data it acquires the shared lock and releases as soon as read is possible and for write it used exclusive lock and release at the end of the transaction.
repeatable read solves dirty read and repeatable read problem.as it acquires the shared lock for reading and only releases the lock only at the end of the transaction.
serializable uses same strategy + apply range based locking and and releases the lock at the end of the transaction.

30. JPA (Java Persistence API, sometimes also referred to as Jakarta Persistence API) is a tool to connect Java applications with databases for optimizing memory storage for data, 
thus providing a smoother User Experience (UX). In simple terms, JPA simplifies database access from within the Java application, by allowing to work with objects rather than SQL 
statements. JPA allows working with POJOs (Plain Old Java Objects) right from desktop apps, providing developers with an ORM (Object Relational Mapping) facility for managing relational 
data.

@Id

Sets the following variable as the primary key of the table in the database to which this Entity class is mapped.

@GeneratedValue(strategy = GenerationType.IDENTITY)

Generates unique, auto-incrementing values for the variable mapped to the primary key of the table in the database.

Bootstrapping
When the application starts, the persistence.xml file is read to initialize the EntityManagerFactory.
JPA interacts with the underlying JPA provider (e.g., Hibernate, EclipseLink) to set up connections to the database.

EntityManager
The EntityManager serves as the primary interface for interactions with the Persistence Context, allowing CRUD (Create-Read-Update-Delete) operations on entities.

The EntityManagerFactory is a factory for EntityManager instances. The createEntityManager() method creates a new EntityManager instance.
JPQL is used to execute queries for CRUD operations on the database.
The EntityManager is responsible for managing entities and interacts with the persistence context.
Persistence Context
The persistence context acts as a first-level cache.
When an entity is loaded, JPA checks if it exists in the persistence context:
If yes, it returns the cached entity.
If no, it queries the database and stores the result in the persistence context.

JPA operations are typically wrapped in transactions. Transactions ensure the ACID properties:
JPA uses the @Transactional annotation or EntityTransaction for transaction boundaries.
Operations like persist, merge, and remove only take effect when the transaction commits.

JPA allows queries using:
JPQL (Java Persistence Query Language): Object-oriented queries.
Native Queries: Raw SQL queries.
The Query or TypedQuery interface is used to execute queries, and results are mapped back to entities.

The persistence context synchronizes with the database during:
Flush: Writes changes from the persistence context to the database. This can happen:
Explicitly by calling entityManager.flush().
Automatically before a query or transaction commit.
Commit: Finalizes the transaction, ensuring all changes are persisted.

Caching
First-Level Cache: Managed by the EntityManager, ensuring entities are retrieved from memory during a session.
Second-Level Cache (Optional): Provided by the JPA provider for caching entities across sessions (e.g., Hibernate’s second-level cache).

Spring Data JPA provides a repository abstraction where method names are analyzed and translated into queries. When you define a method like findByName(String name), the framework 
interprets the method name and constructs a query based on the rules defined in its method naming convention.


Query Execution Lifecycle
Method Invocation: When you call findByName("Alice"), Spring Data JPA intercepts the call.
Query Parsing: It parses the method name and generates the query dynamically.
JPQL Translation: The query is translated into JPQL.
SQL Generation: The JPA provider converts the JPQL query into SQL.
Execution: The SQL query is executed against the database, and the result is mapped back to the entity.

Spring Data JPA is part of the Spring Data project, which aims to simplify data access in Spring-based
applications. It provides a layer of abstraction on top of JPA (Java Persistence API) to reduce
boilerplate code and simplify database operations, allowing developers to focus more on business
logic rather than database interaction details.

JPA is a great example of abstraction and also string constant pool.

Spring Data JPA offers features such as automatic repository creation, query method generation,
pagination support, and support for custom queries. It provides a set of powerful CRUD methods
out-of-the-box, simplifies the implementation of JPA repositories, and supports integration with
other Spring projects like Spring Boot and Spring MVC.

CrudRepository provides basic CRUD operations, while JpaRepository provides JPA-specific methods
like flushing changes to the database, deleting records in a batch, and more. JpaRepository extends
CrudRepository, so it inherits all its methods and adds JPA-specific ones.

The @Modifying annotation is used in conjunction with query methods to indicate that the query
modifies the state of the database. It is typically used with update or delete queries to inform the
persistence provider that the query should be executed as a write operation, ensuring that the
changes are propagated to the database.

Spring Boot's auto-configuration sets up the infrastructure for transaction management, such as:

Creating a transaction manager (e.g., JpaTransactionManager).
Enabling support for the @Transactional annotation.
However, it does not automatically start transactions for your methods. You must explicitly annotate the methods or classes where transactional behavior is needed.

Best Practices
Use @Transactional on service-layer methods rather than on repository methods. This keeps your business logic transactional and separates it from persistence concerns.
Annotate methods or classes that perform multiple database operations or require rollback on failure.
For read-only operations, consider using @Transactional(readOnly = true) to optimize performance.

findById() returns an Optional containing the entity with the given ID, fetching it from the database
immediately. getOne() returns a proxy for the entity with the given ID, allowing lazy loading of its
state. If the entity is not found, getOne() throws an EntityNotFoundException.

The @Temporal annotation is used to specify the type of temporal data (date, time, or timestamp) to
be stored in a database column. It is typically applied to fields of type java.util.Date or
java.util.Calendar to specify whether they should be treated as DATE, TIME, or TIMESTAMP.

FetchType.Eager specifies that the related entities should be fetched eagerly along with the main
entity, potentially leading to performance issues due to loading unnecessary data. FetchType.Lazy
specifies that the related entities should be fetched lazily on demand, improving performance by
loading them only when needed.

The @EnableJpaRepositories annotation is used to enable JPA repositories in a Spring application. It
specifies the base package(s) where Spring should look for repository interfaces and configures the
necessary beans to enable Spring Data JPA functionality.

Pagination is a technique used to divide large result sets into smaller, manageable chunks called
pages. In Spring Data, pagination can be implemented using Pageable as a method parameter in
repository query methods. Spring Data automatically handles the pagination details, allowing you to
specify the page number, page size, sorting, etc.

Some commonly used methods in CrudRepository include save() to save or update entities,
findById() to find entities by their primary key, deleteById() to delete entities by their primary key,
findAll() to retrieve all entities, and count() to count the number of entities.

delete() method deletes a single entity from the database, while deleteInBatch() method deletes all
entities passed as a collection in a single batch operation. The latter is more efficient for deleting
multiple entities at once, as it reduces the number of database round trips.

To optimize batch processing in Spring JPA, I enable batch inserts and updates by configuring
spring.jpa.properties.hibernate.jdbc.batch_size in application.properties. This setting allows
Hibernate to group SQL statements together, reducing database round trips and improving
performance significantly.

For schema migrations in Spring JPA projects, I integrate tools like Liquibase or Flyway. These tools
are configured in Spring Boot applications to automatically apply database schema changes as part of
the deployment process, ensuring the database schema is always in sync with the application's
requirements.

No, Hibernate Session is not thread-safe. The Session object in Hibernate is designed to be used by a single thread at a time and should not be shared across multiple threads. 
Attempting to use a Session concurrently from multiple threads can lead to unpredictable behavior, such as race conditions, data corruption, or application crashes.

The Criteria API is a programmable, object-oriented API in Hibernate used to define complex queries
against database entities. It is used to build up a criteria query object programmatically where you
can apply filtration rules and logical conditions.

The Second Level Cache in Hibernate is an optional cache that can store data across sessions. It is
used to enhance performance by storing entities in cache memory, reducing database access.

The get() method in Hibernate retrieves the object if it exists in the database; otherwise, it returns
null. The load() method also retrieves the object, but if it doesn’t exist, it throws an
ObjectNotFoundException. load() can use a proxy to fetch the data lazily.

Hibernate ensures data integrity by managing database transactions, providing isolation levels, and
supporting concurrency strategies. It also integrates with database constraints and can enforce
application-level integrity using validators.

The N+1 SELECT problem in Hibernate occurs when an application makes one query to retrieve N
parent records and then makes N additional queries to retrieve related child objects. It can be
prevented using strategies like join fetching, batch fetching, or subselect fetching to minimize the
number of queries executed.

Hibernate handles SQL Injection by using prepared statements that automatically escape SQL syntax.
Additionally, using HQL or Criteria API also protects against SQL injection as they translate a query
from HQL into SQL in a way that uses parameterized queries.

Concurrency in Hibernate can be achieved using versioning and locking mechanisms. Hibernate
supports optimistic and pessimistic locking strategies to handle concurrent modifications of data
effectively.

To optimize query performance in Hibernate, I would consider using lazy loading for entity
relationships. This means Hibernate will only fetch related entities when they are explicitly accessed,
not at the time of fetching the parent entity. Additionally, I might use batch fetching and adjust the
fetch sizes in the configuration to reduce the number of database queries.

Hibernate ensures data integrity by using transactions. If an error occurs during the transaction,
hibernate rolls back all operations to the state before the transaction began, using either database
transactions or the Java Transaction API (JTA). This rollback mechanism prevents partial data
modifications that could lead to data inconsistency.

In Hibernate, I handle legacy databases by customizing the ORM mapping. I use the @Table and
@Column annotations to map entity classes to the specific table names and column names defined
in the legacy database. This allows us to map the entities accurately to the database schema without
any changes to the database itself.

VARCHAR: Stores variable-length strings. It uses only the required space based on the string length (plus 1-2
bytes for storing length).

CHAR: Stores fixed-length strings. It always uses the defined length, padding the string with spaces if necessary.

SQL query to find Nth highest salary.
SELECT DISTINCT salary
FROM employees
ORDER BY salary DESC LIMIT 1 OFFSET 2;

SQL to write 2nd highest salary in MYSQL .
SELECT MAX(Salary)
FROM Employee
WHERE Salary < (SELECT MAX(Salary) FROM Employee)

Get Employees Hired in the Last 8 Months.
SELECT *
FROM employees
WHERE hire_date >= CURDATE() - INTERVAL 8 MONTH

Find employees who have worked for more than one department.
SELECT employee_id
FROM employees_history
GROUP BY employee_id HAVING COUNT(DISTINCT department_id) > 1;

Fetch common records between two tables.
SELECT *
FROM table1
intersect SELECT * FROM table2;

The INTERSECT operator compares entire rows based on the values of all columns included in the SELECT clause. For a row to be considered common, every column in the row must match exactly 
between the two tables.

SELECT a.user_id, a.name, COUNT(o.order_id) AS order_count
FROM Accounts a
JOIN Orders o
ON a.user_id = o.user_id
GROUP BY a.user_id, a.name
HAVING COUNT(o.order_id) < 3;

. Database query i want the employee salary > 15000, here I have two different
tables so you have to write a sql query for that.
SELECT e.employee_id, e.name, s.salary
FROM employees e
JOIN salaries s
ON e.employee_id = s.employee_id WHERE s.salary > 15000;

While working with microservices, I face several challenges. One major issue is managing the
communication between numerous small services, which can lead to problems like network
latency and ensuring data consistency. Debugging and troubleshooting become more
difficult because errors can occur in any of the many services. Setting up and maintaining
monitoring and logging for each service is also complicated. Additionally, ensuring security
across all services is crucial but challenging, as each service must be individually secured and
regularly updated.

In microservices, services keep their registration and discovery information up-to-date by
regularly checking in with the Service Registry. When a service starts or changes (like moving
to a new address), it updates its details in the registry. It also sends frequent "heartbeat"
signals to show it's still running. If the registry stops getting these signals, it thinks the
service has stopped working and removes it from the list, so other services don't try to
connect to something that isn't there. This keeps the system's information accurate and
reliable.

To handle a transaction over multiple services, use the Saga pattern. Here's how it works:
Split the main transaction into smaller parts, with each part handled by a different service.
Each service completes its part and tells the others whether it succeeded or failed. If one
part fails, other services undo their work to keep everything consistent. This way, even
though the services are separate, they work together to complete the transaction or back
out if there’s a problem.

Blue-green deployment is a way to update software with minimal downtime. We have two
identical setups: one "Blue" and the other "Green." One setup runs the current version,
while the other prepares the new version. After testing the new version on the inactive
setup, we switch the user traffic from the old to the new. If something goes wrong, we can
quickly switch back to the old version to avoid problems.

Canary releasing slowly introduces a new version to a few users first and, if it works well,
gradually rolls it out to everyone. This method lets we test how the new version performs in
the real world step-by-step. Blue-green deployment switches all users from the old version
to the new one at once after testing. This means all users see the new version at the same
time once it's switched over.

To keep an eye on and manage microservices, we can use tools like Prometheus to monitor
how the services are performing and gather important data. For handling logs from different
services, tools like ELK (Elasticsearch, Logstash, Kibana) are useful for organizing and
analyzing these logs. Grafana is great for visually displaying data. Kubernetes helps manage
these services by automatically adjusting resources, balancing loads, and fixing problems to
keep everything running smoothly.

In a microservices architecture, it's important to keep an eye on how fast services respond,
how often errors occur, and how much CPU, memory, and storage they use. We should also
watch the traffic between services, how they connect with each other, and how quickly data
moves across the network. Monitoring how many requests each service handles helps in
managing workloads and spotting any unusual increases in activity.

Distributed tracing helps monitor microservices by tracking how requests move through
different services. It shows where delays happen, which service might be causing problems,
and how changes in one service affect others. This makes it easier to find and fix issues,
improving how the whole system works.

In a microservices environment, we can use tools like Prometheus for tracking metrics,
Grafana for making charts and graphs, and the ELK Stack (Elasticsearch, Logstash, Kibana) for
managing logs and creating visuals. Jaeger and Zipkin are good for tracing how requests
travel through our services. These tools help we understand how our services are
performing and quickly find and fix any issues.

In microservices, to manage failures, several patterns are used. The Circuit Breaker pattern
stops repeated attempts to a service that's failing, which helps avoid further errors. Fallback
methods give an alternative plan when a service fails. The Retry pattern tries the request
again, using delays to reduce pressure on the system. Bulkhead and Timeout patterns keep
failures in one service from affecting others and prevent long waits for responses.

The Circuit Breaker pattern is like a safety switch for microservices. If a service starts to fail
often, this pattern stops more requests from going to that failing service. This prevents
further problems and gives the service time to fix itself. After a set time, it checks if the
service is working well again before allowing requests to go through, helping to keep the
system stable.

The Bulkhead pattern makes a system more reliable by dividing it into separate sections,
similar to compartments in a ship. If one section has a problem, it doesn't affect the others.
This separation helps ensure that if one part of the system fails or gets too busy, it won't
drag down the entire system. Each section has its own resources, so they don't overwhelm
each other, keeping the system stable.

The Retry pattern means trying a failed operation again, which can help solve temporary
problems like a network glitch. The Backoff pattern adds waiting times between these
retries, increasing the wait after each attempt. This helps avoid overloading the system while
it's still recovering. Using these patterns together helps the system handle failures smoothly
by not rushing to retry, giving everything a better chance to get back to normal.

Maven is a build automation tool used primarily for Java projects. It simplifies and standardizes the
build process, manages dependencies, and provides project structure conventions.

POM (Project Object Model) is an XML file that contains project information and configuration details
required by Maven for building the project. It includes dependencies, plugins, and other settings.

A Maven repository is a directory where all project jars, library jar, plugins, or any other projectspecific artifacts are stored and can be easily used by Maven.

You can exclude dependencies using the <exclusions> element within the <dependency> tag in the
POM file. This allows you to exclude specific transitive dependencies that you don't need.


To run a Maven build, open your command line, navigate to the directory containing your project's
pom.xml file, and type mvn package to build the project

The difference between mvn clean and mvn install is that mvn clean removes files generated
in the previous builds, cleaning the project, while mvn install compiles the project code and
installs the built package into the local repository, making it available for other projects.

In a Maven project, manage dependencies by listing them in the pom.xml file under the
<dependencies> tag. Maven automatically downloads these from repositories and integrates them
into your project.

Maven's life cycle includes phases like compile, test, and deploy, which handle project building in a
sequential manner. Each phase performs specific build tasks, such as compiling code, running tests,
and packaging the compiled code into distributable formats like JARs or WARs

git clone creates a local copy of a remote repository. git pull fetches changes from the remote
repository and merges them into the current branch. git fetch fetches changes from the remote
repository but does not merge them.

Flyway and Liquibase are popular database migration tools used to manage and version database schema changes in a consistent, reliable, and automated way. Both tools are designed to handle
schema evolution for relational databases and help teams track, deploy, and rollback database changes in a controlled manner.

In ci/cd changes are applied to particular instances and are added back to the load balancers. and follow deployment strategies like blue green deployment or canary 
or rolling.

A rolling deployment is a software release strategy where new versions of an application are gradually rolled out to servers or instances, replacing the old version incrementally. 
This approach ensures minimal downtime and maintains availability by keeping some instances of the previous version running during the update process.

if you are connecting to db and don't using jpa then you can use jdbc that provide api to connect to db basically drivers are the implementation class that are used to achieve the
same for ex connecting to MySQL you need MySQL driver.hibernate is the implementation of jpa.

persistence unit in jpa consists the information about db connections , which sql driver the application is using and dialect the application is using.
if your application is connecting to 2 db then it will have 2 persistence unit. on your application bootstrap entityManagerFactory object is created and one persistence unit will have one 
entityManagerFactory from app.properties or persistence.xml entityManagerFactory object is created. one manager can have mutilple entity manager (Session). each entitymanager has one 
persistence context which stores the information about object on which operation are to performed basically it holds the object whatever the work it is going to do on entity.entity manger 
is one for each http request. one persistence context can have many entities associated with it.

Inside plugin tag of pom.xml we can skip or add some of lifecycle methods to project build structure. 

Prometheus is a highly scalable and flexible monitoring solution, while Grafana is a powerful and versatile visualization tool.

Prometheus enables you to collect metrics from your application, and Grafana can be used to display those metrics in a variety of ways. With Spring Boot Actuator, you can access metrics 
like request rate, average response time, and error count, which can be collected by Prometheus.

Combining Prometheus and Grafana creates a comprehensive monitoring solution for Spring Boot applications. This solution enables you to monitor application health, troubleshoot issues, 
and identify trends.

Idempotency is a concept by which we can ensure that each request will have same response.that can be handled by client sending one idempotent key when requesting and generally these key 
are stored in cache and maintained their status and after the completion their status is changed to consumed and if one request is being consumed and its status is changed then no other 
request are entertained by the server.and if due to some reason cache status can't be changed we use ttl to invalidate the user after some time.
On server side we maintain idempotency in db lets say we can use  compute hash and for each request we can check for the existence in db and for any 2 concurrent request idempotency is 
handled by using lock.409 status code for conflict.

31. When you use @Autowired over private static field then ioc will not be able to do di as dependency injection is done on instance variable and not on the static variables as static 
variable
belong to class.
Dependencies should be injected into instance variables, not static ones.
Static fields are not managed by Spring and cannot be autowired.
Non-static fields are managed by Spring, and dependencies are injected properly.

32.# Spring data JPA specification is used when we want to construct queries dynamically where some conditions can be present and some can't be.
  This is usually implemented by extending repository with JpaSpecificationExecutor<Classname>
  Specification interface is at its core and has only one toPredicate() method which converts specification to JpaPredicate.
  it has 3 parameters Root from which your query starts
  CriteriaQuery that is used to construct the query itself.
  CriteriaBuilder is used to define criteria and create predicates.
  Predicate predicate = criteriaBuilder.conjuction() set to by default true that means your query will be executed if you have no parameters.
  Predicate predicate = criteriaBulder.disjunction() set to false.
  if (sourceLocationId != null) {
				predicate = criteriaBuilder.and(predicate,
						criteriaBuilder.equal(root.get("locationId"), sourceLocationId));
			}
  then repo.findAll(Specification);

Enums in Java are special types used to define a set of fixed constants, like days of the week or
directions (NORTH, SOUTH, etc.). They are useful because they make the code more readable and
prevent errors by limiting the possible values for a variable. Instead of using random numbers or
strings, enums ensure only predefined values are used, improving code clarity and safety.

The diamond operator in Java, introduced in Java 7, simplifies the notation of generics by reducing
the need to duplicate generic type parameters. For instance, instead of writing List<String> list = new
ArrayList<String>();, you can use the diamond operator: List<String> list = new ArrayList<>();. The
compiler infers the type parameter String for the ArrayList based on the variable's declared type,
making the code cleaner and easier to read.

Inner classes in Java are classes defined within another class. They are useful for logically grouping
classes that will only be used in one place, increasing encapsulation. Inner classes have access to the
attributes and methods of the outer class, even if they are declared private. There are several types:
non-static nested classes (inner classes), static nested classes, local classes (inside a method), and
anonymous classes (without a class name). Each type serves different purposes based on the specific
need for grouping and scope control.

Anonymous inner classes in Java are useful when you need to implement an interface or extend a
class without creating a separate named class. They are defined and instantiated all at once, typically
at the point of use. This is particularly helpful for handling events or creating runnable objects in GUI
applications with minimal code. By using anonymous inner classes, developers can make their code
more concise and focused on specific tasks.

Java uses pass by value. This means when you pass a variable to a method, Java copies the actual
value of an argument into the formal parameter of the function. For primitive types, Java copies the
actual values, while for objects, Java copies the value of the reference to the object. Therefore,
changes made to the parameter inside the method do not affect the original value outside the
method.

In Java, implementing the Runnable interface and extending the Thread class are two ways to create
a thread, but they serve different purposes. Implementing Runnable is generally preferred as it
allows a class to extend another class while still being able to run in a thread, promoting better
object-oriented design and flexibility. Extending Thread makes a class unable to extend any other
class due to Java's single inheritance limitation, but it can be simpler for straightforward scenarios.

A marker interface in Java is an interface with no methods or fields. It serves to provide runtime
information to objects about what they can do. Essentially, it "marks" a class with a certain property,
allowing the program to use instanceof checks to trigger specific behavior based on the presence of
the marker. Examples include Serializable and Cloneable, which indicate that a class is capable of
serialization or cloning, respectively.

If two packages in Java contain a class with the same name, you can still use both classes in your
program, but you must manage them carefully to avoid naming conflicts. To differentiate between
the two, you should use the fully qualified name of the classes, which includes the package name
followed by the class name, in your code. For example, package1.ClassName and
package2.ClassName. This approach clarifies which class you intend to use from each package.

Using mutable objects as keys in a HashMap can lead to significant issues. If the object’s state
changes after it’s been used as a key, its hashcode can change, making it impossible to locate in the
map even though it's still there. This results in a loss of access to that entry, effectively causing data
loss and potential memory leaks. Therefore, it's best to use immutable objects as keys to maintain
consistent behavior and reliable access.

If you override only the equals() method without overriding hashCode() in a custom key class used in
a HashMap, you'll run into problems. Java requires that equal objects must have the same hash code.
If they don’t, the HashMap might not find the object even though it's there. This inconsistency can
lead to duplicate keys and unpredictable behavior, as the HashMap uses the hash code to locate
keys. Always override both methods to ensure correct behavior.

If you try to sort a list containing null elements using Collections.sort(), it will throw a
NullPointerException. This method requires all elements in the list to be non-null and comparable.
Null elements lack a comparison order, which prevents Collections.sort() from determining their
position relative to other elements. To sort such lists, you must either remove null elements or use a
custom comparator that explicitly handles nulls.

Yes, you can sort a list of custom objects using Collections.sort() without providing a Comparator,
but only if the custom objects implement the Comparable interface. This interface requires defining
a compareTo method, which specifies the natural ordering of the objects. If the objects do not
implement Comparable, or if the compareTo method is not implemented, attempting to sort without
a Comparator will result in a ClassCastException.

To iterate over all values of an enum in Java, you can use the values() method, which returns an array
of all enum constants in the order they're declared. You can then loop through this array using a foreach loop. Here’s how it works: for each constant in the enum, you perform the desired 
operation.
This method is straightforward and efficient for accessing and manipulating each constant in an
enum type.

No, you cannot serialize static fields in Java. Serialization in Java is designed to capture the state of an
object, and static fields are not part of any individual object's state. Instead, static fields belong to
the class itself, shared among all instances. When an object is serialized, only the object's instance
variables are saved, while static fields are ignored. This ensures that the class's shared state remains
consistent and is not duplicated with each object's serialization.

If your Serializable class contains a member that is not serializable, you'll encounter a
NotSerializableException when you try to serialize the class. To fix this, you can either make the nonserializable member transient, which means it won't be included in the serialization 
process, or
ensure that the member class also implements the Serializable interface. Alternatively, you can
customize the serialization process by providing your own writeObject and readObject methods that
handle the non-serializable member appropriately.

Generic type inference in Java is a feature that allows the Java compiler to automatically determine,
or infer, the types of generic arguments that are necessary for method calls and expressions. This
means you don't always have to explicitly specify the generic types when you're coding, which
simplifies your code. For example, when you use the diamond operator (<>) with collections, the
compiler can infer the type of the elements in the collection from the context.

In Java, strings are represented in memory as objects of the String class, which internally uses a
character array to store the string data. Each String object is immutable, meaning once it is created, it
cannot be changed. To optimize memory usage, Java maintains a special area called the "String Pool"
where literals are stored. If you create a string that already exists in the pool, Java reuses the existing
string instead of creating a new one, reducing memory overhead.

To compare the content equality of two custom object instances, override the equals() method in the
class. Inside the method, compare the object's fields (like ID, name, or other properties). This
ensures that two objects with identical values are considered equal, even if their references differ.

No, the keyword this cannot be used in a static method or block in Java. The reason is that this refers
to the current instance of a class, and static methods or blocks do not belong to any instance but to
the class itself. Since static methods can be called without creating an instance of the class, there's
no this context available in static contexts.

Runnable and Callable both functional interface.

In Java 8, Stream.of() is a static method used to create a stream from a set of individual objects. You
can pass one or more objects to this method, and it will return a stream containing the elements you
provided. This is particularly useful for quickly turning a few elements into a stream without needing
to create a collection first. It's a convenient way to work with a fixed number of elements for stream
operations like filtering, mapping, or collecting.

Java 8 introduced default methods in interfaces to enable interfaces to evolve while maintaining
backward compatibility with older versions. Previously, adding a new method to an interface
required all implementing classes to define that method, potentially breaking existing applications.
Default methods allow new functionalities to be added to interfaces without obligating
implementing classes to change. This helps in enhancing interfaces with new methods while ensuring
that existing implementations do not fail.

Yes, lambda expressions in Java can throw exceptions, just like regular methods.

In Java, Optional.of() and Optional.ofNullable() are methods used to create Optional objects, but
they handle null values differently. Optional.of(value) requires a non-null value and throws a
NullPointerException if passed a null. This is suitable when you are certain the value is not null. In
contrast, Optional.ofNullable(value) is safe for use with values that might be null. It returns an empty
Optional if the value is null, thus avoiding any exceptions.

In Java, if you try to modify a local variable inside a lambda expression, you'll encounter a compiletime error. Local variables accessed from within a lambda must be final or effectively 
final—meaning
once they are initialized, they cannot be modified. This restriction ensures that the lambda does not
introduce side effects by altering the local environment, preserving thread safety and functional
programming principles where functions do not modify the state outside their scope.

The volatile keyword in Java concurrency is crucial for ensuring visibility and preventing caching of
variables across threads. When a variable is declared as volatile, it tells the JVM that every read or
write to that variable should go directly to main memory, bypassing any intermediate caches. This
ensures that changes made to a volatile variable by one thread are immediately visible to other
threads, maintaining data consistency across threads without using synchronized blocks.

In Java concurrency, both Runnable and Callable interfaces are used to execute tasks
asynchronously, but they differ in key ways. Runnable has a run() method that does not return a
result and cannot throw checked exceptions. In contrast, Callable includes a call() method that
returns a result and can throw checked exceptions. This makes Callable more versatile for tasks
where you need to handle outcomes and exceptions or require a result upon completion.

The ExecutorService in the Java Executor Framework plays a crucial role in managing and controlling
thread execution. It provides a higher-level replacement for working directly with threads, offering
methods to manage lifecycle operations like starting, running, and stopping threads efficiently. Some
key methods it provides include submit() for executing callable tasks that return a result, execute()
for running runnable tasks, and shutdown() to stop the executor service gracefully once tasks are
completed.

The ConcurrentHashMap in Java is designed for concurrent access without the extensive use of
synchronization. Internally, it divides the data into segments, effectively a hashtable-like structure.
Each segment manages its own lock, reducing contention by allowing multiple threads to
concurrently access different segments of the map. This means that read operations can generally be
performed without locking, and writes require minimal locking, significantly increasing performance
over a Hashtable or synchronized Map under concurrent access scenarios.

When an exception occurs inside a synchronized block in Java, the lock that was acquired when
entering the synchronized block is automatically released. This allows other threads to enter the
synchronized block or method once the current thread has exited due to the exception. Essentially,
the synchronized mechanism ensures that locks are managed cleanly, even in the event of an
exception, preventing deadlocks and allowing program execution to continue in other threads.

you can use tools like
jstack with the process ID to generate a thread dump. This tool is part of the JDK and provides
detailed information about the threads running in your Java application.

In Java, several tools and techniques are used to identify and fix memory leaks. Profiling tools like
VisualVM, JProfiler, or YourKit provide insights into memory usage and help pinpoint leaking objects.
Heap dump analyzers such as Eclipse Memory Analyzer (MAT) are useful for analyzing large amounts
of memory data to identify suspicious consumption patterns. Additionally, code review and ensuring
proper resource management, such as closing streams and sessions, are crucial techniques for
preventing memory leaks.

The Java Memory Model (JMM) defines how threads interact through memory and what behaviors
are allowed in concurrent execution. It specifies the rules for reading and writing to memory
variables and how changes made by one thread become visible to others. The JMM ensures visibility,
atomicity, and ordering of variables to avoid issues like race conditions and data inconsistency. It is
fundamental for developing robust and thread-safe Java applications, ensuring that interactions
between threads are predictable and consistent.

The visibility problem in the Java Memory Model refers to issues where changes to a variable made
by one thread are not immediately or consistently visible to other threads. This can occur because
each thread may cache variables locally instead of reading and writing directly to and from main
memory. Without proper synchronization, there's no guarantee that a thread will see the most
recent write to a variable by another thread, leading to inconsistencies and errors in multithreaded
applications.

In Java, the static keyword affects memory management by allocating memory for static fields and
methods not with individual instances but at the class level. This means that static elements are
stored in the Java method area, a part of the heap memory dedicated to storing class structures and
static content. Static elements are created when the class is loaded by the JVM and remain in
memory as long as the class stays loaded, shared among all instances of that class.

When an exception is thrown in a static initialization block in Java, it prevents the class from being
loaded properly. This results in a java.lang.ExceptionInInitializerError. If an attempt is made to use the
class afterwards, the JVM will throw a NoClassDefFoundError because the class initialization
previously failed. This mechanism ensures that no class is used unless it has been correctly and fully
initialized.

A finally block in Java generally executes reliably, but unexpected behavior can arise if a new
exception is thrown within the finally block itself. For instance, if an exception occurs while closing a
resource in the finally block, it can obscure an exception that was thrown in the try block, leading to
the loss of the original exception's details. This is why it's essential to handle exceptions within the
finally block carefully to prevent such issues.

Using both method overloading and overriding in the same class hierarchy can lead to confusion and
errors in Java. Overloading methods within a class allows multiple methods with the same name but
different parameters. Overriding changes the behavior of a method in a subclass. When these
concepts are combined, it can be unclear whether a method call is invoking an overloaded method
or an overridden one, especially if the signatures are similar. This ambiguity can make the code
harder to read and maintain, and increase the likelihood of bugs.

The Spring Bean lifecycle involves the creation, use, and destruction of beans managed
by the Spring container. Understanding this lifecycle is crucial in large-scale applications
because it helps in optimizing resource management, ensuring beans are created, used,
and disposed of efficiently. This knowledge also aids in troubleshooting issues related to
bean dependencies and execution flow within the application.

BeanFactory is best used in scenarios where minimal resources are available or when
you require only basic bean management functionalities, like in small applications or
embedded systems. On the other hand, ApplicationContext is ideal for enterprise-level
applications that need advanced features such as event propagation, AOP integration,
and declarative services to handle complex business scenarios. It also provides built-in
support for internationalization, web contexts, and various other enterprise-level
services.

A circular dependency issue occurs when two or more beans in a Spring application
depend on each other to be created. For example, Bean A requires Bean B to be created,
and Bean B simultaneously requires Bean A. This situation leads to a deadlock, as neither
bean can be instantiated until the other is, which prevents the application from starting
up properly.

A simple way to break the cycle is by telling Spring to initialize one of the beans lazily. So, instead of fully initializing the bean, it will create a proxy to inject it into the other 
bean. The injected bean will only be fully created when it’s first needed.
use setter injection (or field injection) instead of constructor injection. This way, Spring creates the beans, but the dependencies aren’t injected until they are needed.
using @Primary also resolves circular dependency problem.

In Spring Boot, circular dependencies can be resolved by using setter injection instead of
constructor injection, allowing beans to be instantiated before their dependencies are
set. Another method is using the @Lazy annotation, which defers the initialization of a
bean until it is actually needed, thus breaking the dependency cycle. Additionally, redesigning the application architecture to better separate concerns and reduce coupling between beans 
can also effectively address circular dependencies. 

@Component is a generic stereotype for any Spring-managed component, while
@Service is a specialization of @Component that indicates a bean is performing a service
task or business logic. Technically, they are interchangeable because they both create
Spring beans, but using @Service provides better clarity about the bean's role within the
application. It's best practice to use @Service for service-layer beans and @Component
for beans that don't fit into more specific categories like @Controller or @Repository.

CrudRepository provides basic CRUD (Create, Read, Update, Delete) functionality for
handling entities in a database. In contrast, JpaRepository extends CrudRepository and
adds additional JPA-specific methods like flushing the persistence context and batch
operations. CrudRepository is suitable for applications that require basic database
interactions without the need for the advanced capabilities provided by JpaRepository,
making it ideal for simpler or less demanding data access scenarios.

@Qualifier is used to specify which bean to inject by name, offering precise control when
multiple beans of the same type exist. @Primary marks a bean as the default choice for
autowiring when several options are available, streamlining dependency management.

The @Transactional annotation in Spring is used to define the scope of a single database
transaction. When applied to a method or class, it ensures that the enclosed operations
are executed within a transactional context, meaning they either all succeed or all fail
together. This is particularly useful for maintaining data integrity and handling complex
operations that involve multiple steps or queries to the database.

In Spring Boot, if multiple auto-configuration classes define the same bean, the last one
read by the Spring container usually takes precedence, potentially overriding the beans
defined earlier. This behavior is influenced by the ordering of auto-configuration classes,
which can be controlled using the @AutoConfigureOrder.
@AutoConfigureAfter/@AutoConfigureBefore annotations to specify the load order
explicitly. This setup helps manage dependencies and configurations more effectively in
complex applications.

The @Autowired annotation in Spring primarily uses constructor injection by default,
where dependencies are provided through a class constructor at the time of object
creation, promoting immutability and mandatory dependency declaration. However, it
can also be used for field injection, where Spring directly sets the values of fields on your
beans, and setter injection, where dependencies are injected through setter methods
after the bean is constructed. This flexibility allows for various configurations depending
on the needs of the application.

Constructor injection is recommended over setter-based injection because it ensures
that all necessary dependencies for a class are provided when the class is created. This
makes objects immutable and stable once constructed, as they can't exist without their
required dependencies. Additionally, it prevents the class from being in an incomplete
state, reducing errors related to uninitialized dependencies.

Aspect-Oriented Programming (AOP) is a programming paradigm that allows developers
to modularize cross-cutting concerns, like logging and security, separate from the main
business logic. AOP improves code readability and reduces redundancy by separating
these aspects into distinct sections. However, its biggest disadvantage is that it can make
the flow of execution harder to follow. This complexity arises because the modularized
code executes separately from the main application flow, making it challenging for
developers to trace and debug.

FeignClient can be used for both synchronous and asynchronous communication, depending on the project's needs:
Synchronous by default: FeignClient is synchronous by default, but you can make it asynchronous by wrapping it with an async wrapper that returns a CompletableFuture.
Declarative: FeignClient is declarative and interface-based, which reduces boilerplate code for common operations.
Well-suited for microservices: FeignClient is well-suited for applications with many microservices.

@FeignClient is a Spring Cloud annotation used to declare a Feign client.
Attributes:
name = "address-service": This is the name of the client, typically used for service discovery if you are using tools like Eureka.
url = "http://localhost:8081": This specifies the base URL of the service you want to call. Here, the service is hosted locally on port 8081.
path = "/address-service": This is a common path prefix for all requests made through this client. It helps reduce duplication if all endpoints share the same root path.

@FeignClient(name = "address-service", url = "http://localhost:8081", path = "/address-service")
public interface AddressClient {

    @GetMapping("/address/{id}")
    public ResponseEntity<AddressResponse> getAddressByEmployeeId(@PathVariable("id") int id);

}

Let’s say my Spring Boot application is taking too long to respond to user requests. I could:
• Implement caching for frequently accessed data.
• Optimize database queries to reduce the load on the database.
• Use asynchronous methods for operations like sending emails.
• Load Balancer if traffic is high
• Optimize the time complexity of the code
• Use webFlux to handle a large number of concurrent connections.

Use the getBeanDefinitionNames() method from the
ApplicationContext to get the list of beans.

Cache eviction is when data is removed from the cache to free up space, based on a policy
like "least recently used."
Cache expiration is when data is removed because it's too old, based on a predetermined
time-tolive.
So, eviction manages cache size, while expiration ensures data freshness.

To enable cross-origin requests from a specific domain in Spring Boot, I would use the @CrossOrigin
annotation on my controller or method, like @CrossOrigin(origins = "http://example.com").
For a global approach, I'd configure a WebMvcConfigurer bean, overriding the addCorsMappings
method to apply rules across all controllers, using
registry.addMapping("/**").allowedOrigins("http://example.com").
This setup allows my backend to accept requests from a designated frontend domain and enhancing
security by restricting other cross-origin interactions.

@ConditionalOnProperty(prefix = "mainconfig", name = "enable", havingValue = "true")

Reactive programming is an approach to handling asynchronous and event-based programming that has gained popularity in recent years due to the rise of real-time data-intensive applications. The reactive programming model allows developers to build more efficient, scalable, and resilient systems.

Reactive programming is a design approach that uses asynchronous programming logic to handle real-time adjustments. So the core of reactive programming is a data stream that we can observe
 and react to, even apply back pressure as well. In plain terms, reactive programming is about non-blocking applications that are asynchronous and event-driven and require a small number of
 threads to scale.

Several modern applications today require the ability to handle multiple concurrent or simultaneous requests. Therefore, traditional methods are inadequate to handle these operations.

Spring Web Flux is a reactive programming model introduced by Pivotal in Spring 5.
It provides an asynchronous, non-blocking, and event-driven architecture for building web applications that are resilient and responsive.
It enables developers to build applications that can handle high loads of traffic without compromising on performance.
The framework is based on the Reactive Streams specification and provides support for both client-side and server-side development.
With the help of Spring Web Flux, developers can create applications that are more efficient and scalable than traditional ones.

@RestController
@RequestMapping("/api/user")
public class UserController {

    final UserService userService;

    public UserController(UserService userService) {
        this.userService = userService;
    }

    @PostMapping
    public Mono<UserResponse> saveUser(@RequestBody UserRequest request)
    {
        return userService.saveUser(request);
    }

    @GetMapping(produces = MediaType.TEXT_EVENT_STREAM_VALUE)
    public Flux<UserResponse> retrieveUsers() {
        return userService.retrieveUsers();
    }
    
}

@Repository
public interface UserRepository extends ReactiveCrudRepository<User, String> {
}

@Service
public class UserService {
    final UserMapper userMapper;

    final UserRepository userRepository;

    public UserService(UserMapper userMapper, UserRepository userRepository) {
        this.userMapper = userMapper;
        this.userRepository = userRepository;
    }

    public Mono<UserResponse> saveUser(UserRequest request) {
        User user = userMapper.toUser(request);
        Mono<User> customerMono = userRepository.save(user).log();
        return customerMono.map(userMapper::toUserResponse);
    }

    public Flux<UserResponse> retrieveUsers() {
       return userRepository.findAll().log().map(userMapper::toUserResponse);
    }

}

For example, in a stream processing application, if the producer is generating data at a high rate, the consumer may not be able to keep up with the rate of data consumption. To avoid data
 overload, the consumer can signal the producer to slow down by applying back pressure. The producer will then pause or slow down until the consumer is ready to accept more data.

Each HTTP request is handled by a separate thread from the servlet container's thread pool (e.g., Tomcat, Jetty).
Multiple threads can execute controller methods simultaneously for different requests.
Mono represents a single event/value and Flux represents a stream of multiple values.

for mongo
public interface StudentRepository
    extends ReactiveMongoRepository<Student, String> {
}

for relational db like mysql
@Repository
public interface UserRepository extends ReactiveCrudRepository<User, String> {
}

Reactor (e.g., Mono, Flux)

@GetMapping("/students")
    public Mono<ResponseEntity<Flux<Student>>> getAllStudents() {
        Flux<Student> students = Flux.just(
            new Student("1", "Alice", 22),
            new Student("2", "Bob", 23)
        );

        // Wrap Flux in ResponseEntity and return as Mono
        return Mono.just(ResponseEntity.ok(students));
    }
	
Java's Reflection API can be used to change the value of private fields. Reflection allows you to inspect and manipulate the internal structure of classes, including accessing
private fields and methods.

Access the Field: Use the Class.getDeclaredField(String name) method to get a Field object representing the private field.

Make the Field Accessible: Use Field.setAccessible(true) to bypass the private access modifier.

Change the Field Value: Use the Field.set(Object obj, Object value) method to modify the field's value.

Tools like Hibernate and JPA use reflection to map Java classes to database tables and access fields dynamically.

After the dependencies are injected, the @PostConstruct method is called to perform any initialization logic. You can call methods on ClassA or ClassB to initialize the beans further.

Solution - Using @Lazy
@Component
public class ClassA {

	
	private ClassB b;
	
	public ClassA(@Lazy ClassB b) {
		this.b=b;
	}
	
}

@Component
public class ClassB {
	
	private ClassA a;
	
	public ClassB(ClassA a) {
		this.a=a;
	}
	

}
If you dont use lazy and use setter injection with parameterized constructor then still you will get this circular dependency error for resolving this use below setter injection
and remove parameterized constructor.

@Component
public class ClassB {
	
	private ClassA a;
	
	@Autowired
    public void setClassB(ClassA a) {
        this.a = a;
    }
	

}

@PostConstruct is used after dependencies are injected and you may want to provide some bean initialization logic.
setter injection is not lazy by default as per chatgpt.
don't consider that setter injection is lazy.

"".equals(object) and object.equals("") are used to compare an object with an empty string (""), but there is a subtle difference in how they handle null values and which object is being 
referenced first. Let's break down the key differences:

1. "".equals(object)
This is a safe way to compare a string with an empty string ("").

The equals method is called on the empty string (""), so you are comparing the object (object) with an empty string.

Null safety: If object is null, this will safely return false without throwing a NullPointerException. This is because calling equals on a non-null object (i.e., "") with a null argument 
(object) returns false, instead of throwing an exception.
In this case, you are calling the equals method on the object and passing "" (empty string) as the argument.

Null safety: If object is null, this will throw a NullPointerException because you are trying to call equals on a null reference.

for creating class as key in map you should override the equals and hashcode method to ensure that your behave in consistent manner.

defualt trnasaction isolation level is read_commited.

@PostConstruct is used after application context is initialized and dependencies are injected by ioc container and mostly used related to bean like validating bean and
  defining initilaization logic related to bean.

  ApplicationRunner or CommandLineRunner are are executed after application startup or generally used if you want to write some startup logic or log some message after 
  application starts.
  
  @SpringBootApplication
@EnableAspectJAutoProxy
public class SsoAuthServerApplication implements ApplicationRunner,CommandLineRunner{

	public static void main(String[] args) {
		SpringApplication.run(SsoAuthServerApplication.class, args);
	}

	@Override
	public void run(String... args) throws Exception {
		System.out.println("Command line Runner");
		
	}
	
	@PostConstruct
	public void java8() {
		System.out.println("Post Construct");
	}
	
	

	@Override
	public void run(ApplicationArguments args) throws Exception {
		System.out.println("Application Runner");
		
	}

	
	
}

024-11-30 11:55:11.970  INFO 35368 --- [  restartedMain] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring embedded WebApplicationContext
2024-11-30 11:55:11.970  INFO 35368 --- [  restartedMain] w.s.c.ServletWebServerApplicationContext : Root WebApplicationContext: initialization completed in 368 ms
Post Construct
2024-11-30 11:55:12.159  INFO 35368 --- [  restartedMain] o.s.b.d.a.OptionalLiveReloadServer       : LiveReload server is running on port 35729
2024-11-30 11:55:12.163  INFO 35368 --- [  restartedMain] o.s.b.a.e.web.EndpointLinksResolver      : Exposing 1 endpoint(s) beneath base path '/actuator'
2024-11-30 11:55:12.182  INFO 35368 --- [  restartedMain] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat started on port(s): 8090 (http) with context path ''
2024-11-30 11:55:12.192  INFO 35368 --- [  restartedMain] com.altruist.SsoAuthServerApplication    : Started SsoAuthServerApplication in 0.643 seconds (JVM running for 33.97)
Application Runner
Command line Runner

 A Consumer is an in-build functional interface in the java.util.function package. we use consumers when we need to consume objects, the consumer takes an input value and returns
 nothing.
 
 A Predicate is a functional interface, which accepts an argument and returns a boolean. Usually, it is used to apply in a filter for a collection of objects.
 
 The Supplier Interface is a part of the java.util.function package. It represents a function that does not take in any argument but produces a value of type T.
 
 When you implement both ApplicationRunner and CommandLineRunner in a Spring Boot application, both run methods will be executed, but the execution order is not
 guaranteed by default.
 
 @Component
@Order(1)
public class MyCommandLineRunner implements CommandLineRunner {
    @Override
    public void run(String... args) {
        System.out.println("Executing CommandLineRunner");
    }
}

@Component
@Order(2)
public class MyApplicationRunner implements ApplicationRunner {
    @Override
    public void run(ApplicationArguments args) {
        System.out.println("Executing ApplicationRunner");
    }
}

Optional.of(T value):

Creates an Optional object containing a non-null value.
Throws: NullPointerException if the value provided is null.

String name = "John Doe"; 
Optional<String> optionalName = Optional.of(name); 
System.out.println("Optional Name: " + optionalName);
Optional Name: Optional[John Doe]

Optional.ofNullable(T value):

Creates an Optional object containing the specified value.
The value can be null, in that case, the Optional will be empty.

String city = null; 
Optional<String> optionalCity = Optional.ofNullable(city); 

System.out.println("Optional City: " + optionalCity);
Optional City: Optional.empty

Optional.empty():

Creates an empty Optional object with no value/null value.

Optional<String> optionalEmail = Optional.empty(); 
String email = optionalEmail.orElseGet(() -> "abc@def.com"); 
System.out.println("Email: " + email);
Email: abc@def.com

Optional<String> optionalName = Optional.empty(); 
String name = optionalName
              .orElseThrow(() -> new IllegalArgumentException("Name is absent"));
Exception in thread "main" java.lang.IllegalArgumentException: Name is absent

Sequential Streams are non-parallel streams that use a single thread to process the pipelining. Any stream operation without explicitly
 specified as parallel is treated as a sequential stream. Sequential stream’s objects are pipelined in a single stream on the same processing
 system hence it never takes the advantage of the multi-core system even though the underlying system supports parallel execution. Sequential 
 stream performs operation one by one.
 
 Parallel Stream
 It is a very useful feature of Java to use parallel processing, even if the whole program may not be parallelized. Parallel stream leverage multi-core processors,
 which increases its performance. Using parallel streams, our code gets divide into multiple streams which can be executed parallelly on separate cores of the system
 and the final result is shown as the combination of all the individual core’s outcomes. It is always not necessary that the whole program be parallelized, but at least
 some parts should be parallelized which handles the stream. The order of execution is not under our control and can give us unpredictably unordered results and like any
 other parallel programming, they are complex and error-prone. 
 One of the simple ways to obtain a parallel stream is by invoking the parallelStream() method of Collection interface.
 
 Reference to a static method: A reference to a static method can be created using the syntax ClassName::methodName
 public class MathUtils {
    public static int multiply(int a, int b) {
        return a * b;
    }
}
IntBinaryOperator operator = MathUtils::multiply;

Reference to an instance method of an object: A reference to an instance method of an object can be created using the syntax object::methodName
public class Person {
    private String name;
    
    public Person(String name) {
        this.name = name;
    }
    
    public String getName() {
        return name;
    }
}
Supplier<String> supplier = person::getName;

Reference to a constructor: A reference to a constructor can be created using the syntax ClassName::new
BiFunction<String, Integer, Person> personConstructor = Person::new;


Query to find record where data exist in one table and doesn;t in other column
SELECT t1.*
FROM table1 t1
LEFT JOIN table2 t2 ON t1.common_column = t2.common_column
WHERE t2.common_column IS NULL;

@Override
	public void run(ApplicationArguments args) throws Exception {
		List<Student> studentList = Stream.of(
                new Student(1, "Rohit", 30, "Male", "Mechanical Engineering", "Mumbai", 122, Arrays.asList("+912632632782", "+1673434729929")),
                new Student(2, "Pulkit", 56, "Male", "Computer Engineering", "Delhi", 67, Arrays.asList("+912632632762", "+1673434723929")),
                new Student(3, "Ankit", 25, "Female", "Mechanical Engineering", "Kerala", 164, Arrays.asList("+912632633882", "+1673434709929")),
                new Student(4, "Satish Ray", 30, "Male", "Mechanical Engineering", "Kerala", 26, Arrays.asList("+9126325832782", "+1671434729929")),
                new Student(5, "Roshan", 23, "Male", "Biotech Engineering", "Mumbai", 12, Arrays.asList("+012632632782")),
                new Student(6, "Chetan", 24, "Male", "Mechanical Engineering", "Karnataka", 90, Arrays.asList("+9126254632782", "+16736784729929")),
                new Student(7, "Arun", 26, "Male", "Electronics Engineering", "Karnataka", 324, Arrays.asList("+912632632782", "+1671234729929")),
                new Student(8, "Nam", 31, "Male", "Computer Engineering", "Karnataka", 433, Arrays.asList("+9126326355782", "+1673434729929")),
                new Student(9, "Sonu", 27, "Female", "Computer Engineering", "Karnataka", 7, Arrays.asList("+9126398932782", "+16563434729929", "+5673434729929")),
                new Student(10, "Shubham", 26, "Male", "Instrumentation Engineering", "Mumbai", 98, Arrays.asList("+912632646482", "+16734323229929")))
                .collect(Collectors.toList());
		
		/*
		 * 1. Find the list of students whose rank is in between 50 and 100
		 * List<Student> student=studentList.stream().filter(s -> s.getRank()>50 &&
		 * s.getRank()<100).collect(Collectors.toList());
		 * 
		 * output
		 * [Student{id=2, firstName='Pulkit', age=56, gender='Male', dept='Computer Engineering', city='Delhi', rank=67, contacts=[+912632632762, +1673434723929]},
		 *  Student{id=6, firstName='Chetan', age=24, gender='Male', dept='Mechanical Engineering', city='Karnataka', rank=90, contacts=[+9126254632782, +16736784729929]},
		 *   Student{id=10, firstName='Shubham', age=26, gender='Male', dept='Instrumentation Engineering', city='Mumbai', rank=98, contacts=[+912632646482, +16734323229929]}]
		 */
		
		
		/*
		 * 2. Find the Students who stays in Karnataka and sort them by their names
		 * List<Student> student=studentList.stream().filter(s -> s.getCity().equals("Karnataka")).sorted(Comparator.comparing(Student::getFirstName)).collect(Collectors.toList());
		[Student{id=7, firstName='Arun', age=26, gender='Male', dept='Electronics Engineering', city='Karnataka', rank=324, contacts=[+912632632782, +1671234729929]}, 
		Student{id=6, firstName='Chetan', age=24, gender='Male', dept='Mechanical Engineering', city='Karnataka', rank=90, contacts=[+9126254632782, +16736784729929]},
		 Student{id=8, firstName='Nam', age=31, gender='Male', dept='Computer Engineering', city='Karnataka', rank=433, contacts=[+9126326355782, +1673434729929]},
		 Student{id=9, firstName='Sonu', age=27, gender='Female', dept='Computer Engineering', city='Karnataka', rank=7, contacts=[+9126398932782, +16563434729929, +5673434729929]}]
		 
		 for reverse order 
		 List<Student> student=studentList.stream().filter(s -> s.getCity().equals("Karnataka")).sorted(Comparator.comparing(Student::getFirstName,Comparator.reverseOrder())).collect(Collectors.toList());
		 
		 */
		
		/*
		 * 3. Find all departments names
		 * studentList.stream().map(s -> s.getDept()).distinct().forEach(System.out::print);
		 * Mechanical EngineeringComputer EngineeringBiotech EngineeringElectronics EngineeringInstrumentation Engineering
		 */
		
		/*
		 * 4. Find all the contact numbers
		 * studentList.stream().flatMap(s -> s.getContacts().stream()).forEach(System.out::println);
		 * List<String> collect = studentList.stream().flatMap(s -> s.getContacts().stream()).distinct().collect(Collectors.toList());
		 * +912632632782
           +1673434729929
           +912632632762
           +1673434723929
		 */
		
		/*
		 * 5. Group The Student By Department Names
		 * Map<String, List<Student>> studentMap = studentList.stream().collect(Collectors.groupingBy(Student::getDept));
		
		for(Map.Entry<String,List<Student>> map:studentMap.entrySet()) {
			System.out.println(map);
		}
		
		Map<String, Long> studentMap = studentList.stream().collect(Collectors.groupingBy(Student::getDept,Collectors.counting()));
		
		for(Map.Entry<String,Long> map:studentMap.entrySet()) {
			System.out.println(map);
		}
		
		 */
		
		
		/*
		 * 6. Find the average age of male and female students
		 * Map<String, Double> studentMap = studentList.stream().collect(Collectors.groupingBy(Student::getGender,Collectors.averagingInt(Student::getAge)));
	
		for(Map.Entry<String,Double> map:studentMap.entrySet()) {
			System.out.println(map);
		}
		
		Female=26.0
		Male=30.75
		
		 */		
		
		/*
		 * 7. Find the highest rank in each department
		 * Map<String, Optional<Student>> studentMap = studentList.stream().collect(Collectors.groupingBy(Student::getDept,Collectors.minBy(Comparator.comparing(Student::getRank))));
		
		for(Map.Entry<String,Optional<Student>> map:studentMap.entrySet()) {
			System.out.println(map.getValue().get().getRank());
		}
		 */
		
		/*
		 * 8. List<Integer> numbers = Arrays.asList(3, 7, 8, 1, 5, 9); 
		 * int sum = numbers.stream().mapToInt(i -> i).sum(); 
		 * 
		 * Integer sum = numbers.stream().reduce(0, (a,b) -> a+b);
		 * Integer sum = numbers.stream().reduce(0, Integer::sum);
		 * System.out.println(sum); - > 33
		 * 
		 * List<Integer> numbers = Arrays.asList(3, 7, 8, 1, 5, 9);
		OptionalDouble sum = numbers.stream().mapToInt(i -> i).average();
		System.out.println(sum);
		 * 
		 */
		
		/*
		 * 9.List<String> projects = employees.stream()
                .flatMap(e -> e.getProjects().stream())
                .map(p -> p.getName()).distinct()
                .collect(Collectors.toList());
                Here project is also a class so map is called on it. 
		 */
		
		/*
		 * 10. Finding max rank
		 * Optional<Student> student = studentList.stream().max(Comparator.comparing(Student::getRank));
		 * 
		 * Optional<Employee> highestPaidEmployees = employees.stream()
                .max(Comparator.comparingDouble(Employee::getSalary));

       // System.out.println("Highest paid employee : "+highestPaidEmployees);

        Optional<Employee> lowestPaidEmployees = employees.stream()
                .min(Comparator.comparingDouble(Employee::getSalary));
		 * 
		 * 
		 */
		
		/*
		 * 11. Find first and Find any
		 * Employee findFirstElement = employees.stream()
                .filter(e -> e.getDept().equals("Development"))
                .findFirst()
                .orElseThrow(()->new IllegalArgumentException("Employee not found "));
		 */
		
		/*
		 * 12. Any match and All match
		 * 
		 * //anyMatch(Predicate) , allMatch(Predicate) , noneMatch(Predicate) Predicate accepts argument and return boolean
		 * 
		 * boolean developmentEmpAnyMatch = employees.stream()
                .anyMatch(e -> e.getDept().equals("Development"));
                 boolean developmentEmpAllMatch = employees.stream()
                .allMatch(e -> e.getSalary()>50000);//55000
                
                boolean isNoneMatch = employees.stream()
                .noneMatch(e -> e.getDept().equals("abc"));
		 */
	}
	
	Shallow copy copies changes the actual object as all its associated referenced variable share same reference.
	In case of deep copy all the objects are independent and changes made in the copied object does not reflect in other object.
 
Problem: Find the maximum value in a list of integers.
    Solution:
    Optional<Integer> max = numbers.stream()
    .max(Integer::compare);
	
	List of Names to Uppercase
	List<String> upperNames = names.stream()
	.map(String::toUpperCase)
	.collect(Collectors.toList());
	
	Problem: Sort a list of integers in ascending order.
	Solution:
	List<Integer> sortedNumbers = numbers.stream()
    .sorted()
    .collect(Collectors.toList());
	
	Count the number of elements in a list that are greater than 5.
	Solution:
	long count = numbers.stream()
	.filter(n -> n > 5)
	.count();
	
	int total = numbers.stream()
   .reduce(0, Integer::sum);
   
   The Stream. findFirst() method in Java returns an Optional containing the first element of the stream.
   
   /*
		 * 13 . List<Integer> numbers = Arrays.asList(3, 7, 8, 1, 5, 9);
		 * numbers.stream().limit(3).forEach(System.out::println); 3 7 8
		 */     
		
		/*14. 
		List<Integer> numbers = Arrays.asList(3, 7, 8, 1, 5, 9);
		numbers.stream()
				 .skip(2)
				 .collect(Collectors.toList()).forEach(i -> System.out.print(" " +i)); 8 1 5 9
		*/	
		
		/*
		 * 15. List<Integer> numbers = Arrays.asList(3, 7, 8, 1, 5, 9);
		 * numbers.stream().map(String::valueOf).filter(s ->
		 * s.startsWith("1")).forEach(System.out::println); 1
		 */
		
		/*
		 * 16. List<Integer> myList = Arrays.asList(10,15,8,49,25,98,98,32,15); int max
		 * = myList.stream() .max(Integer::compare) .get();
		 */

The Builder Design Pattern is a creational design pattern used to construct complex objects step by step. It allows you to create different types and representations of an object using 
the same construction code. The pattern is especially useful when an object contains many fields or optional parameters.

public class Car {
    private final String engine;
    private final int wheels;
    private final boolean airbags;

    private Car(CarBuilder builder) {
        this.engine = builder.engine;
        this.wheels = builder.wheels;
        this.airbags = builder.airbags;
    }

    // Getters
    public String getEngine() {
        return engine;
    }

    public int getWheels() {
        return wheels;
    }

    public boolean hasAirbags() {
        return airbags;
    }

    // Inner Builder Class
    public static class CarBuilder {
        private String engine;
        private int wheels;
        private boolean airbags;

        public CarBuilder setEngine(String engine) {
            this.engine = engine;
            return this;
        }

        public CarBuilder setWheels(int wheels) {
            this.wheels = wheels;
            return this;
        }

        public CarBuilder setAirbags(boolean airbags) {
            this.airbags = airbags;
            return this;
        }

        public Car build() {
            return new Car(this);
        }
    }
}


public class BuilderPatternDemo {
    public static void main(String[] args) {
        // Using the builder to create a Car object
        Car car = new Car.CarBuilder()
                .setEngine("V8")
                .setWheels(4)
                .setAirbags(true)
                .build();

        System.out.println("Car Engine: " + car.getEngine());
        System.out.println("Number of Wheels: " + car.getWheels());
        System.out.println("Has Airbags: " + car.hasAirbags());
    }
}

Use StringBuilder:

When your application runs in a single-threaded environment.
When performance is critical and synchronization is not needed.

Use StringBuffer:

When your application runs in a multi-threaded environment.
When you need synchronized access to the object.

The Prototype Design Pattern is a creational design pattern that allows objects to be copied or cloned. Instead of creating new instances from scratch, the pattern creates new objects by 
copying or cloning existing objects, which can be more efficient, especially for expensive object creation.

public interface Prototype {
    Prototype clone();
}

public class Employee implements Prototype {
    private String name;
    private String designation;
    private double salary;

    public Employee(String name, String designation, double salary) {
        this.name = name;
        this.designation = designation;
        this.salary = salary;
    }

    // Getters and setters
    public String getName() {
        return name;
    }

    public void setName(String name) {
        this.name = name;
    }

    public String getDesignation() {
        return designation;
    }

    public void setDesignation(String designation) {
        this.designation = designation;
    }

    public double getSalary() {
        return salary;
    }

    public void setSalary(double salary) {
        this.salary = salary;
    }

    // Clone method
    @Override
    public Employee clone() {
        return new Employee(this.name, this.designation, this.salary);
    }

    @Override
    public String toString() {
        return "Employee{name='" + name + "', designation='" + designation + "', salary=" + salary + '}';
    }
}


The Adapter Design Pattern is a structural design pattern used to bridge the gap between two incompatible interfaces. It acts as a wrapper that allows classes with different interfaces to 
work together without modifying their existing code.

public interface MediaPlayer {
    void play(String audioType, String fileName);
}

public class AdvancedMediaPlayer {
    public void playVlc(String fileName) {
        System.out.println("Playing VLC file. Name: " + fileName);
    }

    public void playMp4(String fileName) {
        System.out.println("Playing MP4 file. Name: " + fileName);
    }
}

public class MediaAdapter implements MediaPlayer {
    private AdvancedMediaPlayer advancedMediaPlayer;

    public MediaAdapter(String audioType) {
        if (audioType.equalsIgnoreCase("vlc")) {
            advancedMediaPlayer = new AdvancedMediaPlayer();

        } else if (audioType.equalsIgnoreCase("mp4")) {
            advancedMediaPlayer = new AdvancedMediaPlayer();
        }
    }

    @Override
    public void play(String audioType, String fileName) {
        if (audioType.equalsIgnoreCase("vlc")) {
            advancedMediaPlayer.playVlc(fileName);
        } else if (audioType.equalsIgnoreCase("mp4")) {
            advancedMediaPlayer.playMp4(fileName);
        }
    }
}


public class AudioPlayer implements MediaPlayer {
    private MediaAdapter mediaAdapter;

    @Override
    public void play(String audioType, String fileName) {
        if (audioType.equalsIgnoreCase("mp3")) {
            System.out.println("Playing MP3 file. Name: " + fileName);
        } else if (audioType.equalsIgnoreCase("vlc") || audioType.equalsIgnoreCase("mp4")) {
            mediaAdapter = new MediaAdapter(audioType);
            mediaAdapter.play(audioType, fileName);
        } else {
            System.out.println("Invalid media type. " + audioType + " format not supported.");
        }
    }
}


public class AdapterPatternDemo {
    public static void main(String[] args) {
        MediaPlayer audioPlayer = new AudioPlayer();

        audioPlayer.play("mp3", "song.mp3");
        audioPlayer.play("vlc", "movie.vlc");
        audioPlayer.play("mp4", "video.mp4");
        audioPlayer.play("avi", "unsupported.avi");
    }
}


Playing MP3 file. Name: song.mp3
Playing VLC file. Name: movie.vlc
Playing MP4 file. Name: video.mp4
Invalid media type. avi format not supported.


The Composite Design Pattern is a structural design pattern that allows you to compose objects into tree-like structures to represent part-whole hierarchies. It treats individual objects 
and compositions of objects uniformly, making it easier to work with complex hierarchies.

ex like in a folder we can have directory and file both.

The Proxy Design Pattern is a structural design pattern used to provide a surrogate or placeholder object to control access to another object. It acts as an intermediary, adding a level of
 abstraction to perform additional actions like security checks, lazy initialization, logging, or caching.

The Flyweight Design Pattern is a structural design pattern used to minimize memory usage and improve performance by sharing as much data as possible with similar objects. It is 
particularly useful when working with a large number of objects that share common properties.ex String constant pool.

The Facade Design Pattern is a structural design pattern that provides a simplified interface to a larger and more complex system. It hides the complexity of the subsystem and provides a 
unified, high-level interface for the client to interact with the subsystem.

public class DVDPlayer {
    public void on() {
        System.out.println("DVD Player is ON");
    }

    public void play(String movie) {
        System.out.println("Playing movie: " + movie);
    }

    public void off() {
        System.out.println("DVD Player is OFF");
    }
}

public class Projector {
    public void on() {
        System.out.println("Projector is ON");
    }

    public void setInput(String source) {
        System.out.println("Projector input set to " + source);
    }

    public void off() {
        System.out.println("Projector is OFF");
    }
}

public class SoundSystem {
    public void on() {
        System.out.println("Sound System is ON");
    }

    public void setVolume(int level) {
        System.out.println("Volume set to " + level);
    }

    public void off() {
        System.out.println("Sound System is OFF");
    }
}

public class HomeTheaterFacade {
    private DVDPlayer dvdPlayer;
    private Projector projector;
    private SoundSystem soundSystem;

    public HomeTheaterFacade(DVDPlayer dvdPlayer, Projector projector, SoundSystem soundSystem) {
        this.dvdPlayer = dvdPlayer;
        this.projector = projector;
        this.soundSystem = soundSystem;
    }

    public void watchMovie(String movie) {
        System.out.println("Get ready to watch a movie...");
        projector.on();
        projector.setInput("DVD");
        soundSystem.on();
        soundSystem.setVolume(10);
        dvdPlayer.on();
        dvdPlayer.play(movie);
    }

    public void endMovie() {
        System.out.println("Shutting down the home theater...");
        dvdPlayer.off();
        projector.off();
        soundSystem.off();
    }
}

public class FacadePatternDemo {
    public static void main(String[] args) {
        DVDPlayer dvdPlayer = new DVDPlayer();
        Projector projector = new Projector();
        SoundSystem soundSystem = new SoundSystem();

        HomeTheaterFacade homeTheater = new HomeTheaterFacade(dvdPlayer, projector, soundSystem);

        // Start the movie
        homeTheater.watchMovie("Inception");

        System.out.println();

        // End the movie
        homeTheater.endMovie();
    }
}

Get ready to watch a movie...
Projector is ON
Projector input set to DVD
Sound System is ON
Volume set to 10
DVD Player is ON
Playing movie: Inception

Shutting down the home theater...
DVD Player is OFF
Projector is OFF
Sound System is OFF


The Decorator Design Pattern is a structural design pattern that allows behavior to be added to an object dynamically without modifying its structure. It is often used to extend the 
functionalities of objects in a flexible and reusable way.

public interface Coffee {
    String getDescription();
    double getCost();
}

public class SimpleCoffee implements Coffee {
    @Override
    public String getDescription() {
        return "Simple Coffee";
    }

    @Override
    public double getCost() {
        return 50.0;
    }
}

public abstract class CoffeeDecorator implements Coffee {
    protected Coffee coffee;

    public CoffeeDecorator(Coffee coffee) {
        this.coffee = coffee;
    }

    @Override
    public String getDescription() {
        return coffee.getDescription();
    }

    @Override
    public double getCost() {
        return coffee.getCost();
    }
}

public abstract class CoffeeDecorator implements Coffee {
    protected Coffee coffee;

    public CoffeeDecorator(Coffee coffee) {
        this.coffee = coffee;
    }

    @Override
    public String getDescription() {
        return coffee.getDescription();
    }

    @Override
    public double getCost() {
        return coffee.getCost();
    }
}

public class MilkDecorator extends CoffeeDecorator {
    public MilkDecorator(Coffee coffee) {
        super(coffee);
    }

    @Override
    public String getDescription() {
        return coffee.getDescription() + ", Milk";
    }

    @Override
    public double getCost() {
        return coffee.getCost() + 10.0;
    }
}

public class SugarDecorator extends CoffeeDecorator {
    public SugarDecorator(Coffee coffee) {
        super(coffee);
    }

    @Override
    public String getDescription() {
        return coffee.getDescription() + ", Sugar";
    }

    @Override
    public double getCost() {
        return coffee.getCost() + 5.0;
    }
}

public class MilkDecorator extends CoffeeDecorator {
    public MilkDecorator(Coffee coffee) {
        super(coffee);
    }

    @Override
    public String getDescription() {
        return coffee.getDescription() + ", Milk";
    }

    @Override
    public double getCost() {
        return coffee.getCost() + 10.0;
    }
}

public class SugarDecorator extends CoffeeDecorator {
    public SugarDecorator(Coffee coffee) {
        super(coffee);
    }

    @Override
    public String getDescription() {
        return coffee.getDescription() + ", Sugar";
    }

    @Override
    public double getCost() {
        return coffee.getCost() + 5.0;
    }
}


public class DecoratorPatternDemo {
    public static void main(String[] args) {
        Coffee simpleCoffee = new SimpleCoffee();
        System.out.println(simpleCoffee.getDescription() + " Cost: " + simpleCoffee.getCost());

        Coffee milkCoffee = new MilkDecorator(simpleCoffee);
        System.out.println(milkCoffee.getDescription() + " Cost: " + milkCoffee.getCost());

        Coffee sugarMilkCoffee = new SugarDecorator(milkCoffee);
        System.out.println(sugarMilkCoffee.getDescription() + " Cost: " + sugarMilkCoffee.getCost());
    }
}

Simple Coffee Cost: 50.0
Simple Coffee, Milk Cost: 60.0
Simple Coffee, Milk, Sugar Cost: 65.0

public interface PaymentStrategy {
    void pay(int amount);
}

public class CreditCardPayment implements PaymentStrategy {
    private String cardNumber;

    public CreditCardPayment(String cardNumber) {
        this.cardNumber = cardNumber;
    }

    @Override
    public void pay(int amount) {
        System.out.println("Paid " + amount + " using Credit Card: " + cardNumber);
    }
}

public class PayPalPayment implements PaymentStrategy {
    private String email;

    public PayPalPayment(String email) {
        this.email = email;
    }

    @Override
    public void pay(int amount) {
        System.out.println("Paid " + amount + " using PayPal: " + email);
    }
}

public class ShoppingCart {
    private PaymentStrategy paymentStrategy;

    public void setPaymentStrategy(PaymentStrategy paymentStrategy) {
        this.paymentStrategy = paymentStrategy;
    }

    public void checkout(int amount) {
        if (paymentStrategy == null) {
            System.out.println("No payment method selected.");
        } else {
            paymentStrategy.pay(amount);
        }
    }
}

public class StrategyPatternDemo {
    public static void main(String[] args) {
        ShoppingCart cart = new ShoppingCart();

        // Pay using Credit Card
        cart.setPaymentStrategy(new CreditCardPayment("1234-5678-9876-5432"));
        cart.checkout(100);

        System.out.println();

        // Pay using PayPal
        cart.setPaymentStrategy(new PayPalPayment("user@example.com"));
        cart.checkout(200);
    }
}

Paid 100 using Credit Card: 1234-5678-9876-5432

Paid 200 using PayPal: user@example.com

The Memento Design Pattern is a behavioral design pattern that allows you to capture and externalize an object's internal state so that it can be restored later, without violating 
encapsulation. It is used to implement undo/redo functionality or to allow objects to be restored to a previous state.

Shallow cloning creates a copy of an object, but the references to nested objects are shared between
the original and the clone, meaning changes in one affect the other. Deep cloning, however, creates a
complete copy of the object and all its nested objects, so both are independent. The Cloneable
interface in Java marks a class as capable of being cloned using the clone() method, but you must
override clone() to implement deep or shallow cloning behavior.

Generics in Java help maintain type safety by allowing you to define classes, methods, or collections
with a placeholder for types, ensuring that only the specified type can be used. This prevents
runtime errors by catching type mismatches at compile time. Generics also reduce code duplication
because you can create flexible, reusable code that works with different types, rather than writing
separate versions of methods or classes for each type

To ensure atomicity without using the synchronized keyword, you can use Java's Atomic classes from
the java.util.concurrent.atomic package, such as AtomicInteger or AtomicReference. These classes
provide lock-free thread-safe operations like incrementing or updating values atomically. By using
these atomic classes, you avoid the need for explicit locking, ensuring safe concurrent access to
shared resources while improving performance in multi-threaded environments.

The @Target annotation in Java specifies where an annotation can be applied. It restricts the usage
of an annotation to specific program elements like classes, methods, fields, or constructors. For
example, using @Target(ElementType.METHOD) ensures the annotation can only be used on
methods. This helps prevent accidental misuse of annotations and improves code clarity by clearly
defining where they are applicable in your code.

Class.forName() loads a class and also initializes it by executing any static blocks or static variable
initializations. In contrast, ClassLoader.loadClass() only loads the class without initializing it until it's
needed later. Use Class.forName() when you need the class to be loaded and initialized immediately,
while ClassLoader.loadClass() is useful when you want to defer initialization for performance reasons
or when initializing the class isn't immediately required.

To resolve the diamond problem when a class implements two interfaces with the same default
method signature but different bodies, you must explicitly override the conflicting method in the
implementing class. Within the overridden method, you can decide which interface's default method
to call by using InterfaceName.super.methodName(). This approach ensures that the implementing
class resolves the conflict by specifying the desired behavior.

If a final field is changed using reflection in Java, the change can bypass compile-time restrictions,
allowing the field to be modified. However, this breaks the immutability contract, and the behavior
may not be predictable. For example, some compilers or JVM optimizations might still assume the
field is immutable, leading to inconsistent behavior. To modify a final field using reflection, you must
disable access checks with setAccessible(true), but this should be avoided in practice due to potential
risks.

A service mesh is a way to manage communication between different parts of an application,
especially when the application is broken into many small services (microservices). It helps to control
how parts of an application share data with each other, provides security, and monitors
performance. Essentially, it acts like a middleman that helps all the different services in an
application talk to each other smoothly and securely.

EhCache is the cache provider when you store cache results then jvm look into this except into session factory. 

	The HibernateJpaVendorAdapter is a class provided by Spring Framework that acts as a bridge between JPA (Java Persistence API) and Hibernate, which is a popular JPA implementation.
	It configures Hibernate-specific settings in a way that's compatible with Spring's JPA infrastructure.
	
	@Configuration
@EnableJpaRepositories(basePackages = {
		"com.altruist.model.firebird.repo"}, entityManagerFactoryRef = "firebirdEntityManager", transactionManagerRef = "firebirdTransactionManager")
public class FirebirdDataSource {
	
	@Autowired
	private Environment env;

	@Bean
	@Primary
	public LocalContainerEntityManagerFactoryBean firebirdEntityManager() {
		LocalContainerEntityManagerFactoryBean em = new LocalContainerEntityManagerFactoryBean();
		em.setDataSource(firebirdDatasource());
		em.setPackagesToScan(new String[] { "com.altruist.firebird.model" });
		em.setPersistenceUnitName("firebirdEntityManager");
		HibernateJpaVendorAdapter vendorAdapter = new HibernateJpaVendorAdapter();
		em.setJpaVendorAdapter(vendorAdapter);
		HashMap<String, Object> properties = new HashMap<String, Object>();
		properties.put("hibernate.dialect", env.getProperty("hibernate.dialect"));
		em.setJpaPropertyMap(properties);
		return em;
	}

	@Bean
	@Primary
	public DataSource firebirdDatasource() {

		DriverManagerDataSource dataSource = new DriverManagerDataSource();  // it is implementation class of data source
		dataSource.setUrl(env.getProperty("firebird.spring.datasource.url"));
		dataSource.setUsername(env.getProperty("firebird.spring.datasource.username"));
		dataSource.setPassword(env.getProperty("firebird.spring.datasource.password"));

		return dataSource;
	}

	@Bean
	public PlatformTransactionManager firebirdTransactionManager() {

		JpaTransactionManager transactionManager = new JpaTransactionManager();
		transactionManager.setEntityManagerFactory(firebirdEntityManager().getObject());
		return transactionManager;
	}
	
}

 Creating console application in springboot - 
	
	@SpringBootApplication
@EnableAspectJAutoProxy
public class SsoAuthServerApplication implements CommandLineRunner{

	public static void main(String[] args) {
		SpringApplication.run(SsoAuthServerApplication.class, args);
	}

	@Override
	public void run(String... args) throws Exception {
		Scanner scanner = new Scanner(System.in);
        System.out.println("Enter your name: ");
        String name = scanner.nextLine();
        System.out.println(name);
        System.exit(0);
        
		
	}
	}
	
	in properties u can add - spring.main.web-application-type=none
	#spring.main.web-application-type=none it stops the server after performing the operations.
	
	The behavior you’re observing happens because a Spring Boot application by default starts a non-daemon thread for its lifecycle management, 
	which keeps the application running indefinitely until it is explicitly terminated. This is intentional for web applications, where the application must remain active
	to handle requests.

	For a normal Java application, the program exits once the main thread (and all non-daemon threads) complete execution.
	
	The factory pattern allows a interface to create the type of object based on the subclass provided at runtime.
	
	The abstract factory provides the interface to creating families or dependent object without specifying their concrete class.
	
	The Bridge pattern decouples the abstraction and its implementation so that two can vary independently , promoting more scalability and flexibility.
	
	The Composite Pattern allows you to compose objects into tree-like structures and treat individual
	objects and compositions uniformly.
	
	The Decorator Pattern allows behavior to be added to individual objects dynamically, without
	affecting the behavior of other objects from the same class.
	
	The Facade Pattern provides a simplified interface to a complex subsystem, making it easier to
	interact with by hiding the internal complexity.
	
	The Chain of Responsibility Pattern allows a request to be passed along a chain of handlers until it is
	handled, providing flexibility in assigning responsibilities to objects.
	
	The Observer Pattern defines a one-to-many dependency between objects, where if one object
	changes state, all its dependents are notified and updated automatically.
	
	The Strategy Pattern defines a family of algorithms, encapsulates each one, and makes them
	interchangeable, allowing the algorithm to vary independently from clients that use it.
	
	The Template Method Pattern defines the skeleton of an algorithm in a method, allowing subclasses
	to redefine certain steps without changing the algorithm's structure.
	
	The Bridge and Adapter patterns both facilitate working with different interfaces, but they serve
	different purposes and are applied in different contexts. The Bridge pattern is used to separate an
	abstraction from its implementation, allowing them to vary independently—ideal for system design
	flexibility. The Adapter pattern, however, is used to make existing classes work together without
	modifying their source code by reconciling incompatible interfaces—useful for integrating external
	systems or libraries. Essentially, Bridge is for planned design flexibility, while Adapter is for
	integration fixes.
	
	Composite pattern is most useful when you want to treat individual objects and compositions of objects uniformly.
	
	To implement the Facade pattern in Java, first create a Facade class that provides a simplified
	interface to multiple subsystems. For example, in a home theater system, you have classes like
	DVDPlayer, Amplifier, and Projector. The Facade class, HomeTheaterFacade, would provide high-level
	methods like watchMovie() that internally calls methods of the subsystem classes. This simplifies the
	client interaction by hiding the complex subsystem details behind simple methods in the Facade
	class.
	
	The Facade pattern offers key advantages in large applications by reducing complexity and simplifying
	interactions. It provides a clear, high-level interface for clients, hiding the complexities of underlying
	subsystems. This leads to cleaner, more maintainable code and reduces dependencies between
	components, making it easier to manage and extend the system. Additionally, it promotes loose
	coupling, allowing subsystems to change without impacting the client code, which improves
	scalability and flexibility.
	
	Why 1 == 1 is true, but 128 == 128 is false(When data type is Integer otherwise it will give true)
	Java optimizes the memory usage of Integer objects by maintaining a cache of frequently used values. The Integer class caches objects for values between -128 	and 127 by default. This means that:

	For integers in this range, the same object reference is returned for each occurrence of a value.
	For integers outside this range, a new object is created each time.

	localhost refer to domain hat is reolved by dns and internally refered to 127.0.0.1 and 127.0.0.1 refers to ip .

	1. API Gateway Pattern
Purpose:
The API Gateway pattern serves as a single entry point for all client requests to a microservices-based
application. It manages the routing of requests to the appropriate microservices, handles crosscutting concerns like authentication, rate limiting, and load balancing, and can aggregate 
responses from multiple microservices.
Common Use Cases:
• Centralized entry point for microservices in large-scale applications.
• Handling requests from different clients (web, mobile, etc.) and providing customized
responses.
• Managing concerns like authorization, logging, caching, and throttling at a centralized
location.
Example Scenario:
In an e-commerce application, a client needs product details, customer reviews, and
recommendations. The API Gateway aggregates these responses from separate microservices
(Product, Reviews, and Recommendation services) and returns a unified response to the client.

2. Circuit Breaker Pattern
Purpose:
The Circuit Breaker pattern prevents a service from continuously trying to communicate with a failing
or unresponsive service. It stops calls for a while when a failure threshold is reached, improving the
stability of the system by allowing failing services to recover.
Common Use Cases:
• Preventing cascading failures in a distributed system when a downstream service is
unavailable.
• Ensuring stability and fault tolerance when external services or microservices fail
intermittently.
Example Scenario:
In a payment processing system, if the Payment Gateway service is down, the Circuit Breaker
prevents repeated failed attempts to contact the service and allows fallback mechanisms (e.g.,
queuing payments for later processing).

4. Database per Microservice Pattern
Purpose:
Each microservice in this pattern has its own database, ensuring loose coupling between
microservices. This allows services to evolve independently, choose their own database technology,
and scale independently.
Common Use Cases:
• Large-scale applications where different services need to use different database technologies
(SQL, NoSQL, etc.).
• Scenarios where strong service autonomy is required, and different services need different
data models.
Example Scenario:
In a ride-sharing application, the Booking service uses a relational database for transactional
consistency, while the Driver Tracking service uses a NoSQL database to handle high-frequency
updates on driver locations.

5. Saga Pattern
Purpose:
The Saga pattern manages distributed transactions across multiple microservices. Instead of a single
global transaction, Sagas break transactions into a series of smaller, local transactions, coordinated
through either choreography (event-driven) or orchestration.
Common Use Cases:
• Distributed systems where ACID transactions are not feasible.
• Ensuring consistency across multiple microservices in a long-running business transaction.
Example Scenario:
In an e-commerce application, when placing an order, the Order service communicates with the
Inventory, Payment, and Shipping services. If the Payment service fails, the Saga triggers
compensating actions to roll back the order and notify the customer

6. Bulkhead Pattern
Purpose:
The Bulkhead pattern isolates different parts of a system (typically by allocating separate resources
such as thread pools) so that failure in one service doesn't impact others. This enhances the
resilience of the system by containing failures.
Common Use Cases:
• Isolating critical services from non-critical ones to ensure that failure in non-essential
services doesn’t affect the overall system.
• Protecting different microservices from resource starvation caused by overloaded services.
Example Scenario:
In a microservices-based airline booking system, if the Loyalty Points service fails, it doesn't affect the
core Booking service. Separate thread pools for each ensure that one service's failure doesn’t
overwhelm the others.

7. Choreography Pattern (Event-Driven)
Purpose:
In the Choreography pattern, microservices communicate by publishing and consuming events rather
than direct calls. This allows for loose coupling between services, where services react to events and
make their decisions independently.
Common Use Cases:
• Systems where microservices need to communicate asynchronously.
• Scenarios where services need to respond to domain events across distributed services.
Example Scenario:
In a social media application, when a user posts a status update, multiple microservices (such as
Notification, Feed, and Analytics) listen for the "post created" event and perform their respective
tasks independently.
8. Orchestration Pattern
Purpose:
The Orchestration pattern involves a central orchestrator service controlling the interactions
between microservices. The orchestrator dictates the workflow and manages the sequence in which
microservices are called to complete a business process.
Common Use Cases:
• Complex workflows that require multiple microservices to interact in a specific sequence.
• Scenarios where a single entity is responsible for coordinating transactions or business logic.
Example Scenario:
In an order fulfillment process, the Orchestrator coordinates between services like Order, Payment,
Inventory, and Shipping, ensuring the entire workflow is completed correctly and compensates for
failures.

9. Strangler Pattern
Purpose:
The Strangler pattern gradually replaces parts of a monolithic application with microservices. New
microservices are built alongside the monolith, and over time, the monolithic parts are "strangled"
and replaced.
Common Use Cases:
• Migrating a legacy monolithic application to microservices in incremental steps.
• Reducing risk and complexity during modernization efforts by avoiding a "big bang"
migration.
Example Scenario:
A retail company with a monolithic e-commerce platform starts replacing individual modules (e.g.,
Cart, Payment) with microservices, while the rest of the monolith continues to run until it is
completely replaced.

10. Retry Pattern
Purpose:
The Retry pattern automatically retries failed requests, typically for transient failures, to increase the
reliability of interactions between services. It helps in handling temporary issues like network failures
or timeouts.
Common Use Cases:
• Scenarios where microservices communicate over unreliable networks or experience
intermittent service outages.
• Services that call external APIs, where failures might occur due to temporary conditions.
Example Scenario:
In an inventory management system, if the Inventory service fails to respond when checking stock
levels, the Retry pattern allows multiple attempts before considering the request as failed, increasing
fault tolerance.

1) What is the Circuit Breaker pattern, and how does it improve the resilience of a system?
The Circuit Breaker pattern is like a safety switch that prevents system overload. In a software
system, when a part (like a microservice) starts to fail frequently, the circuit breaker "trips" and
temporarily stops more requests from reaching that failing part. This allows the system to avoid
further errors and gives the troubled part time to recover. It improves resilience by preventing
failures from cascading and affecting the entire system.
2) Can you explain the different states of a Circuit Breaker (closed, open, half-open)?
Sure! The Circuit Breaker pattern has three states: closed, open, and half-open. When it's closed,
everything is normal, and requests flow through freely. If errors start occurring too often, the
circuit breaker opens, stopping any more requests to prevent overload and let the system
recover. After some time, it moves to half-open, where it allows a few requests to check if the
problem is fixed before fully reopening or closing again based on the outcome.

3) How does the Circuit Breaker pattern differ from the Retry pattern?
The Circuit Breaker pattern and the Retry pattern handle service failures differently. The Circuit
Breaker stops further requests to a failing service to prevent system overload and gives the
service time to recover. In contrast, the Retry pattern automatically attempts to resend a request
when it fails, hoping for a successful response after one or more tries. While Retry actively seeks
immediate resolution, Circuit Breaker aims to protect the system's stability over time.

The Service Discovery pattern in microservices is like an address book for services. As
microservices frequently change locations and scales (due to updates or scaling operations),
keeping track of where each service is located becomes challenging. Service Discovery helps by
automatically tracking and listing the network locations of all services. This enables services to
find and communicate with each other easily, ensuring that the entire system functions smoothly
and efficiently.

It's recommended to have a separate database for each microservice to ensure that each service
operates independently. This setup prevents services from affecting each other's data and
performance, enhances security by isolating databases, and makes scaling easier. When each
microservice controls its own database, it can manage its data schema and transactions without
dependencies, leading to a more robust and flexible application architecture.

Handling data consistency across microservices with separate databases involves using strategies
like distributed transactions or event-driven approaches. In distributed transactions, you ensure
that changes in different services either succeed or fail together. Alternatively, an event-driven
approach uses events to trigger updates across services, maintaining consistency by reacting to
changes rather than coordinating them upfront. This method helps keep data aligned across
services while allowing each to remain independent and resilient.

The Saga pattern manages distributed transactions across microservices by breaking a
transaction into smaller, local transactions, each handled by different services. Instead of a single
service coordinating a big transaction, each service performs its part and communicates success
or failure to the next service. If something goes wrong, the Saga initiates compensating
transactions to undo the changes, maintaining data integrity. This method allows for flexible and
reliable coordination across microservices without relying on a central transaction manager.

In the Saga pattern, choreography and orchestration are two ways to manage transactions across
microservices. Choreography involves each service independently deciding when and how to
interact with other services based on events, like a dance where each participant knows their
moves. Orchestration, on the other hand, uses a central coordinator (like a conductor) that
explicitly directs each service on what to do and when, guiding the entire process step-by-step.

Compensating transactions are used in the Saga pattern to undo changes if a part of a multi-step
process fails. Think of them as "rollback" actions for each step that has already succeeded when
a subsequent step fails. For instance, if a booking process involves reserving a flight and a hotel,
and the hotel reservation fails, a compensating transaction would cancel the already booked
flight, ensuring the system returns to its initial state. This mechanism helps maintain data
consistency across distributed services.

The Bulkhead pattern, inspired by the compartments in ships, involves dividing a system into
separate sections that operate independently. If one section becomes overloaded or fails, the
bulkheads prevent the issue from spreading to other parts of the system. This approach
enhances system resilience by isolating failures, ensuring that a problem in one area doesn't
cause a complete system breakdown. It's especially useful in distributed systems like
microservices to maintain overall stability.

To implement Bulkheads in a microservices architecture, you can isolate services by assigning
them dedicated resources like CPUs, memory, and network connections. You can also limit the
number of concurrent requests a service can handle and use separate thread pools or queues for
different service operations. This setup prevents one service's issues from affecting others and
helps maintain stable performance across the system. It's like giving each microservice its own
safety zone to operate within.

Consider an online banking system with separate services for account management, transaction
processing, and customer support. By using the Bulkhead pattern, each service is allocated its
own resources (like CPU and memory). If the transaction processing service experiences a surge
in demand and becomes overloaded, it won't affect account management or customer support.
This isolation improves system reliability by ensuring that critical services remain operational,
even if one part of the system is under stress.

The Bulkhead pattern directly supports resource isolation in microservices by ensuring that each
microservice operates with its own set of resources, such as CPU, memory, and network
bandwidth. This separation prevents one microservice from consuming all the resources, which
could lead to system failures or poor performance across other services. By isolating resources,
the Bulkhead pattern helps maintain a stable and predictable environment where services can
perform reliably without interfering with each other.

The Choreography pattern in microservices is a method of coordinating interactions between
services without a central controller. Instead, each service knows when and how to act based on
the events it observes from other services. When one service completes a task, it publishes an
event, which other services can listen to and react accordingly, initiating their part of the process.
This decentralized approach allows services to operate independently, simplifying
communication and workflow management.

The Choreography pattern promotes loose coupling between microservices by allowing each
service to operate independently without direct knowledge of others. Services communicate
through events rather than direct requests. When a service completes an action, it broadcasts an
event, and any interested service can respond based on its own logic and requirements. This
setup minimizes dependencies, as no service needs to know the workflow or internal details of
others, enhancing flexibility and scalability.

In the Choreography model, consider an e-commerce application where a customer places an
order. The order service processes the order and publishes an "Order Created" event. The
payment service listens for this event and initiates payment processing. Upon successful
payment, it publishes a "Payment Processed" event, which the shipping service then listens to
and starts the shipping process. Each service acts independently based on the events it receives,
coordinating the workflow without direct dependencies.

The Orchestration pattern in microservices involves a central coordinator, often called an
orchestrator, which manages the interaction between services. This contrasts with the
Choreography pattern, where services independently decide their actions based on events. In
Orchestration, the orchestrator directs each service on what to do and when, much like a
conductor with an orchestra. This centralized approach provides clearer control and easier
management of workflows, but can increase dependency and reduce flexibility compared to
Choreography.

An orchestrator in a microservices architecture controls interactions by explicitly dictating the
sequence and logic of service interactions in a workflow. It sends commands to each
microservice, telling them when to perform a specific action based on the workflow's
requirements. This approach ensures that each step is executed in the correct order and that the
overall process is managed centrally, providing a clear and structured execution path for complex
operations across multiple services.

The Retry pattern improves the fault tolerance of microservices by allowing them to
automatically attempt failed operations again, thus handling temporary problems without
crashing or requiring human intervention. By retrying, services can overcome transient issues like
network timeouts or resource unavailability. This pattern helps ensure that the system remains
operational and responsive even when minor disruptions occur, leading to a more robust and
resilient service architecture.

Cross-Origin Resource Sharing allows a website to safely access resources from another website. In
Spring Boot, we can set up CORS by adding @CrossOrigin to controllers or by configuring it globally.
This tells our application which other websites can use its resources, what type of requests they can
make, and what headers they can use.
This way, We control who can interact with our application, keeping it secure while letting it
communicate across different web domains.

In Spring Security, the SecurityContext is where details about the currently authenticated user are
stored, like user details and granted authorities.
The SecurityContextHolder is a helper class that holds the SecurityContext. It's like a container or
storage space that keeps track of the authentication information of the current user throughout the
application.
This makes it easy to access the user's details anywhere in the application, ensuring that security
decisions can be made based on the user's authentication status and roles.

Spring Security protects against CSRF attacks by generating unique tokens for each session and
requiring that each request from the client includes this token.
This ensures the request is from the authenticated user, not a malicious site. However, CSRF
protection might be disabled for APIs meant to be accessed by non-browser clients, like mobile apps
or other back-end services, where the risk of CSRF is low and tokens can't be easily managed. 
If you are using jwt tokens for maintaining session and authentication then you can disable csrf token.

I can use Spring Expression Language (SpEL) for fine-grained access control by embedding it in
security annotations like @PreAuthorize. For example, I can write expressions that check if a user has
specific roles, and permissions, or even match against method parameters to decide access.

Salting in Spring Security means adding a random piece of data to a password before turning it into a
hash, a kind of scrambled version.
This makes every user's password hash unique, even if the actual passwords are the same. It helps
stop attackers from guessing passwords using known hash lists.
When a password needs to be checked, it's combined with its salt again, hashed, and then compared
to the stored hash to see if the password is correct. This way, the security of user passwords is
greatly increased.

JWT (JSON Web Token) is a compact, URL-safe means of representing claims between two parties. It is widely used for authentication and information exchange. 
A JWT comprises three parts: Header, Payload, and Signature, separated by dots (.).
A JWT (JSON Web Token) contains three main parts: the header, the payload, and the
signature. The header describes the token's type and the algorithm used for signing. The
payload includes claims, which are statements about the user like their ID and
permissions, along with metadata such as token issuance and expiration times. The
signature ensures the token hasn’t been altered, providing security and authenticity. This
format makes JWTs a secure way to transmit user information.

When a class implements two interfaces that have the same default method, the compiler will throw an error. This is because there is ambiguity about 
which default method the implementing class should inherit, and Java does not automatically resolve such conflicts.

interface InterfaceA {
    default void display() {
        System.out.println("InterfaceA display");
    }
}

interface InterfaceB {
    default void display() {
        System.out.println("InterfaceB display");
    }
}

public class MyClass implements InterfaceA, InterfaceB {
    public static void main(String[] args) {
        MyClass obj = new MyClass();
        obj.display();
    }
}

The above code will not compile and throw the following error:
class MyClass inherits unrelated defaults for display() from types InterfaceA and InterfaceB.

To resolve this conflict, the implementing class must override the default method and explicitly specify which interface's method to use (if required).
public class MyClass implements InterfaceA, InterfaceB {
    @Override
    public void display() {
        // Explicitly call one of the interface's default methods
        InterfaceA.super.display();
        // or
        // InterfaceB.super.display();
    }

    public static void main(String[] args) {
        MyClass obj = new MyClass();
        obj.display();
    }
}

Output (if InterfaceA.super.display() is called):
InterfaceA display

By overriding the display method in the implementing class, you eliminate ambiguity and make the behavior explicit.

No, the @PreAuthorize annotation in Spring Security will not work on private methods.

Reason:
The @PreAuthorize annotation works through Spring AOP (Aspect-Oriented Programming), which requires proxy-based method interception. For AOP proxies to work, the method must be:

Public: Only public methods can be intercepted by Spring proxies.
Non-final: Final methods cannot be proxied, as dynamic proxies cannot override them.
Since private methods are not visible outside the class and are not part of the proxied interface or subclass, Spring Security cannot intercept and apply the @PreAuthorize logic.

What Happens If You Use @PreAuthorize on a Private Method?
No Access Control: The private method will execute without any authorization checks.
No Error or Warning: Spring does not throw an explicit error, but the annotation is effectively ignored.

Keep Transactional Methods Public: Always apply @Transactional to public methods to ensure proxy-based interception works.

Proxy-based interception is a technique used in Aspect-Oriented Programming (AOP) where a proxy object is created to intercept method calls on a target object. The proxy wraps the target 
object and adds additional behavior (such as logging, security checks, or transaction management) before or after the actual method execution.

This is a core mechanism used by Spring AOP to implement cross-cutting concerns like logging, transaction management, and security without modifying the target object directly.

How Proxy-Based Interception Works:
Proxy Creation: A proxy object is created to act as a substitute for the actual target object.

For interfaces: JDK Dynamic Proxies are used.
For concrete classes without interfaces: CGLIB (Code Generation Library) proxies are used.
Method Interception: The proxy intercepts method calls on the target object.

Before invoking the actual method on the target object, additional logic (advice) is executed.
After the method execution, more logic (if needed) can be applied.
Invocation of the Target Method: After performing pre-processing logic, the proxy delegates the method call to the target object.

Post-Processing: Additional behavior can be executed after the method call (e.g., closing a transaction or logging the result).

Advantages of Proxy-Based Interception:
Separation of Concerns: Cross-cutting concerns like logging and transactions can be implemented separately without modifying the target code.
Reusability: The same proxy logic can be applied to multiple methods or classes.
Dynamic Behavior: Proxies can dynamically decide which advice to apply based on runtime conditions.

Private or Final Methods Are Not Intercepted: Proxies cannot override private or final methods, as they are not part of the proxy or are unmodifiable.

Key Components
a. AuthenticationManager
The central interface for authenticating users. Delegates authentication to AuthenticationProviders.

b. AuthenticationProvider
Responsible for performing authentication logic. Examples:

DaoAuthenticationProvider: Authenticates using data loaded via UserDetailsService.
Custom providers for LDAP, OAuth2, JWT, etc.
c. UserDetailsService
Loads user-specific data from a database or other storage.
Returns a UserDetails object containing user information (username, password, roles).
d. PasswordEncoder
Encodes and verifies passwords. Common implementations:

BCryptPasswordEncoder
Pbkdf2PasswordEncoder
e. SecurityContextHolder
Stores the SecurityContext.
Provides access to the authenticated user's details across the application.

you cannot access a non-static variable directly in a static context in Java.
The best candidates for keys in a HashMap in Java should have the following characteristics:

1. Immutability
Keys should be immutable so that their hash code remains constant once they are inserted into the HashMap.
If the key's value changes after insertion, the hash code will change, and the key cannot be located correctly in the map.
Examples of immutable keys:

String (recommended as it is immutable and commonly used)
Wrapper classes like Integer, Double, Character, etc.
2. Properly Implemented hashCode() and equals()
The key class must override the hashCode() and equals() methods to ensure correct behavior in the HashMap.
hashCode() is used to determine the bucket where the entry will be stored.
equals() is used to compare keys for equality in the same bucket.
The two methods must comply with the hashCode-equals contract:
If two keys are equal, they must have the same hash code.
If two keys have the same hash code, equals() will determine if they are equal.
3. Efficiency of hashCode()
The hashCode() method should distribute keys uniformly across the HashMap buckets to minimize collisions.
Avoid poorly implemented hashCode() methods that result in a high number of collisions, as this degrades performance.

If two threads have the same priority in Java, the thread execution order is not guaranteed and depends on the thread scheduler. The thread scheduler, 
which is part of the Java Virtual Machine (JVM), determines the order of thread execution. The behavior can vary across operating systems and JVM implementations.

In Java, static variables cannot be serialized. Serialization in Java is the process of converting an object into a byte stream, which includes the object's instance data.
Since static variables are associated with the class rather than any specific instance, they are not serialized as part of the object state.

The purpose of serialization is to capture the state of an object. Since static variables do not represent the state of any particular object, they are excluded.

Update the @ComponentScan annotation in your main application class or configuration class to include both com.altruist and com.my packages.

@SpringBootApplication
@ComponentScan(basePackages = {"com.altruist", "com.my"})
public class MyApplication {
    public static void main(String[] args) {
        SpringApplication.run(MyApplication.class, args);
    }
}

1. What is static in Java?

Ans: In Java, static is a keyword that is used for memory management mainly. Static means single copy storage for variables or methods.

The members that are marked with the static keyword inside a class are called static members.

2. Can we access static members if no instance of the class is constructed?

Ans: Yes, we can access the static members if no instance of class exists because they are not tied to a specific instance. They are shared across all instances of the class.

3. Can we apply static keyword with a top-level class?

Ans: No, static keyword cannot be applied with outer or top-level class but an inner class can be static.

4. Will the following code snippet compile successfully? If yes, what is the output of the following program?

public class Myclass 
{
   private int x = 10;
   static int m1() {
       int y = x;
       return y;
    }
public static void main(String[] args) {
     m1();
   }
}
Ans: No, the above code will not be compiled because x is an instance variable and instance member cannot be accessed from static region.

5. Identify the error in the following code snippet. If there is no error then what will be the output of the program?

public class Myclass 
{
   private int x = 10;
   static int m1()
  {
      Myclass obj = new Myclass();
       int y = obj.x;
       return y;
   }
public static void main(String[] args) {
     System.out.println(m1());
   }
}
Ans: There is no error in the above code snippet. Output: 10.


6. What is the main use of static keyword in java?
Ans: The main use of java static keyword is as follows:

The static keyword can be used when we want to access the data, method, or block of the class without any object creation.
It can be used to make the programs more memory efficient.
7. Can we mark a local variable as static?

Ans: No, we cannot mark a local variable with a static keyword.

8. When does a static variable get memory?

Ans: When a class is loaded into the memory at runtime, the static variable is created and initialized into the common memory location only once.

9. What will be the output of the following program?

public class Myclass 
{
   static int a = 20;
   static int b = 30;
   static int c = 40;
   Myclass() 
  {
      a = 200;
   }
static void m1() {
      b = 300;
}
static {
      c = 400;
}
public static void main(String[] args) {
     System.out.println(a);
     System.out.println(b);
     System.out.println(c);
   }
}
Ans: Output: 20, 30, 400.

10. What will be the output of the following code?

public class Myclass {
     static int a = 20;
Myclass() {
     a = 200;
}
public static void main(String[] args) {
    new Myclass();
    System.out.println(a);
  }
}
Ans: Output: 200.

11. In which part of memory static variables are stored?

Ans: All static variables are stored in PermGen space of the heap memory.

12. How static variable is different from the instance variable?

Ans: The difference between static variable and instance variable is as follows:

a) A static variable is also called class variable whereas, an instance variable is also called non-static variable.

b) Class variable can be accessed inside a static block, instance block, static method, instance method, and method of the inner class whereas, instance variable can be accessed only 
inside the instance members, and method of the inner class.



c) Class variable is always resolved during compile time whereas, instance variable is resolved during the runtime.
4) Static variable cannot be serialized in Java whereas, instance variable can be serialized.

13. Will the following code snippet compile fine? If yes, what will be the output of the following program?

public class Myclass {
  static int a = 20;
 Myclass() {
       a++;
  }
 void m1()  {
       a++;
       System.out.println(a);
  }
public static void main(String[] args) 
{
    Myclass obj = new Myclass();
    Myclass obj2 = new Myclass();
    Myclass obj3 = new Myclass();
       obj3.m1();
   }
}
Ans: Output: 24.

14. What is a static method in Java?

Ans: When a method is declared with the keyword ‘static’, it is called static method in java.

15. Why is a static method also called a class method?

Ans: A static method is also called a class method because it ties to a class rather than an individual instance of a class. Therefore, we need not to create an object of the class to call
 and execute static method.

16. Can we access static members (such as static variables and static methods) from an instance method?

Ans: Yes, we can access static members from an instance method in java.

17. Is it possible to access instance members from a static method?

Ans: No, it is not possible to access instance members like instance variable and instance method from a static method.

18. Identify the error in the following code.

public class Test
{
   Test() {
       m2();
   }
  void m1() {
        System.out.println("Instance method");
   }
  static void m2() {
        System.out.println("Static method");
        m1();
   }
public static void main(String[] args)
{
      new Test();
  }
}
Ans: There is an error inside the static method because we cannot make a static reference to the non-static method m1() from the type Test.

19. Will the below code compile successfully? If yes, what will be the output of the following program?

public class Test
{
  Test(Test t) {
    m1();
    System.out.println("Constructor");
  }
  void m1() {
    m2();
    System.out.println("Instance method");
  }
  static void m2() {
     System.out.println("Static method");
  }
public static void main(String[] args)
{
     new Test(null);
  }
}
Ans: Yes, the above code will be compiled successfully. There is no problem. Output: Static method, Instance method, Constructor.

20. Is there any error in the below code snippet? If yes, identify the error and give the reason behind it.

public class Test
{
 void m1(Test test) {
       System.out.println("Instance method");
  }
 static void m1(Test t) {
         System.out.println("Static method");
  }
}
Ans: Yes, Duplicate method error. This is because we cannot declare a static method and instance method with the same signature in the same class.

21. What is the difference between static method and instance method?

Ans: Go to this tutorial: Static method in Java

22. Can we have a static method in an interface?

Ans: Yes, from Java 8 and onwards, the interface allows to define a static method with body.

23. Can we use this or super keyword in static method in Java?

Ans: No, In the entire core java, this and super keywords are not allowed inside the static region.

24. Is it possible to overload static methods in a class?

Ans: Yes, we can overload static methods but override them. This is because they are bound with class, not instance.

25. Is it possible to override static methods of a class?

Ans: No, we cannot override static methods because static methods belong to a class, not individual objects, and are resolved at compile time by java compiler.

For more detail, go to this tutorial: Can we override static methods in Java?

26. Can we override an instance method as static?

Ans: No.

27. What is the output of the following program code?

public class Myclass {
  static int a = 20;
  static void m2() {
        a++;
   }
public static void main(String[] args) {
    System.out.println(a);
  }
}
Ans: Output: 20.

28. Why static block is executed before the main method in java?

Ans: When the dot class file is loaded into memory, static block is executed. After executing the static block, JVM calls the main method to start execution.
 Therefore, static block is executed before the main method.

29. What is the output of the following program below?

public class Myclass {
  static int a = 20;
  static {
        a++;
   }
{
    a++;
    System.out.println(a);
}
public static void main(String[] args) 
{
   Myclass obj = new Myclass();
   Myclass obj2 = new Myclass();
   Myclass obj3 = new Myclass();
  }
}
Ans: Output: 22, 23, 24.

30. What is the use of static block in java?

Ans: A static block can be used when

we want to write that logic inside static block that is executed during the class loading.
we want to change the default value of static variables.
we want to initialize static variable of the class.
31. What is the output of the following code snippet?

public class Myclass {
Myclass() {
      System.out.println("constructor");
}
static void m1() {
      System.out.println("static method");
}
void m2(){
      System.out.println("instance method");
}
static {
      System.out.println("static block");
}
{
      System.out.println("instance block");
}
public static void main(String[] args) 
{
    Myclass obj = new Myclass();
    m1();
    obj.m2();
   }
}
Ans: Output: static block, instance block, constructor, static method, instance method.

32. How static block is different from an instance block in java?

Ans: Static block is different from an instance block by the following key points:

a) Static block is also called a static initialization block whereas instance block is also called instance initialization block or non-static block.

b) Static block gets executed before the instance block whereas, instance block executes after the static block.

c) Only static variables can be accessed inside the static block whereas, both static and non-static variables can be accessed inside the instance block.

d) Static block executes when the class is loaded into the memory whereas instance block executes only when an instance of the class is created.

e) We cannot use this keyword inside the static block whereas this keyword can be used in the instance block.

33. Can we declare a static block inside a method?

Ans: No, we cannot declare a static block inside a method.

34. What will be the output of the following code snippet after execution?

public class Myclass {
static {
    System.out.println("static block");
}
{
    System.out.println("instance block");
}
public static void main(String[] args) {
   Myclass obj = new Myclass();
   Myclass obj2 = new Myclass();
   Myclass obj3 = new Myclass();
  }
}
Ans: Output: static block, instance block, instance block, instance block.

35. Will the following code snippet compile fine? If yes, what will be output after execution?

public class Myclass {
private static int x = 10;
static {
      x++;
}
static {
      ++x;
}
{
      x--;
}
public static void main(String[] args) {
    Myclass obj = new Myclass();
    Myclass obj2 = new Myclass();
    Myclass obj3 = new Myclass();
    System.out.println(x);
  }
}
Ans: Yes, the code will be compiled fine. The output is 9.

conditionalOnProperty(prefix="mysqlconnection",value="enabled",havingValue=true,matchIfMssing=false)
mysqconnectio.enbled=true

What happens when the no-args constructor is absent in the
Entity bean?
Hibernate framework internally uses Reflection API for creating entity bean instances
when get() or load() methods are called. The method Class.newInstance() is used
which requires a no-args constructor to be present. When we don't have this
constructor in the entity beans, then hibernate fails to instantiate the bean and
hence it throws HibernateException.
30. Can we declare the Entity class final?
No, we should not define the entity class final because hibernate uses proxy classes
and objects for lazy loading of data and hits the database only when it is absolutely
needed. This is achieved by extending the entity bean. If the entity class (or bean) is
made final, then it cant be extended and hence lazy loading can not be supported.

A persistent entity can exist in any of the following states:
Transient:
This state is the initial state of any entity object.
Once the instance of the entity class is created, then the object is said to have
entered a transient state. These objects exist in heap memory.
In this state, the object is not linked to any session. Hence, it is not related to any
database due to which any changes in the data object don't affect the data in
the database.
Persistent:
This state is entered whenever the object is linked or associated with the
session.
An object is said to be in a persistence state whenever we save or persist an
object in the database. Each object corresponds to the row in the database
table. Any modifications to the data in this state cause changes in the record in
the database.
Following methods can be used upon the persistence object:
session.save(record);
session.persist(record);
session.update(record);
session.saveOrUpdate(record);
session.lock(record);
session.merge(record);
Detached:
InterviewBitEmployee employee=new InterviewBitEmployee(); //The object is in the transi
employee.setId(101);
employee.setFullName("Hibernate");
employee.setEmail("hibernate@interviewbit.com");
Hibernate Interview Questions
The object enters this state whenever the session is closed or the cache is
cleared.
Due to the object being no longer part of the session, any changes in the object
will not reflect in the corresponding row of the database. However, it would still
have its representation in the database.
In case the developer wants to persist changes of this object, it has to be
reattached to the hibernate session.
In order to achieve the reattachment, we can use the methods load(), merge(),
refresh(), update(), or save() methods on a new session by using the reference of
the detached object.
The object enters this state whenever any of the following methods are called:
session.close();
session.clear();
session.detach(record);
session.evict(record);

What are the concurrency strategies available in hibernate?
Concurrency strategies are the mediators responsible for storing and retrieving items
from the cache. While enabling second-level cache, it is the responsibility of the
developer to provide what strategy is to be implemented to decide for each
persistent class and collection.
Following are the concurrency strategies that are used:
Transactional: This is used in cases of updating data that most likely causes
stale data and this prevention is most critical to the application.
Read-Only: This is used when we don't want the data to be modified and can be
used for reference data only.
Read-Write: Here, data is mostly read and is used when the prevention of stale
data is of critical importance.
Non-strict-Read-Write: Using this strategy will ensure that there wouldn't be
any consistency between the database and cache. This strategy can be used
when the data can be modified and stale data is not of critical concern.

In Java, Lambda expressions basically express instances of functional interfaces (An interface with a single abstract method is called a functional interface). Lambda Expressions in Java 
are the same as lambda functions which are the short block of code that accepts input as parameters and returns a resultant value.

Functionalities of Lambda Expression in Java-
Enable to treat functionality as a method argument, or code as data.
A function that can be created without belonging to any class.
A lambda expression can be passed around as if it was an object and executed on demand.

interface FuncInterface
{
    // An abstract function
    void abstractFun(int x);
 
    // A non-abstract (or default) function
    default void normalFun()
    {
       System.out.println("Hello");
    }
}
 
class Test
{
    public static void main(String args[])
    {
        // lambda expression to implement above
        // functional interface. This interface
        // by default implements abstractFun()
        FuncInterface fobj = (int x)->System.out.println(2*x);
 
        // This calls above lambda expression and prints 10.
        fobj.abstractFun(5);
    }
}

The body of a lambda expression can contain zero, one, or more statements.
When there is a single statement curly brackets are not mandatory and the return type of the anonymous function is the same as that of the body expression.
When there is more than one statement, then these must be enclosed in curly brackets (a code block) and the return type of the anonymous function is the same as the type of the value 
returned within the code block, or void if nothing is returned.

Lambda Expressions are anonymous functions. These functions do not need a name or a class to be used. Lambda expressions are added in Java 8. Lambda expressions basically express instances of functional interfaces An interface with a single abstract method is called a functional interface. One example is java.lang.Runnable.

in case of using comparable and comparator.
While defining our own sorting, JVM is always going to call Comparator to compare() method.

returns negative value(-1), if and only if obj1 has to come before obj2.
returns positive value(+1), if and only if obj1 has to come after obj2.
returns zero(0), if and only if obj1 and obj2 are equal.

Functional Interfaces
An interface that contains only one abstract method is known as a functional interface, but there is no restriction, you can have n number of default and static methods inside a functional interface.

The Consumer Interface is a part of the java.util.function package which has been introduced since Java 8, to implement functional programming in Java. It represents a function which 
takes in one argument and produces a result. However these kind of functions don’t return any value.

public class Main {
    public static void main(String args[])
    {
        // Consumer to display a number
        Consumer<Integer> display = a -> System.out.println(a);
 
        // Implement display using accept()
        display.accept(10);
 
        // Consumer to multiply 2 to every integer of a list
        Consumer<List<Integer> > modify = list ->
        {
            for (int i = 0; i < list.size(); i++)
                list.set(i, 2 * list.get(i));
        };
 
        // Consumer to display a list of numbers
        Consumer<List<Integer> >
            dispList = list -> list.stream().forEach(a -> System.out.print(a + " "));
 
        List<Integer> list = new ArrayList<Integer>();
        list.add(2);
        list.add(1);
        list.add(3);
 
        // Implement modify using accept()
        modify.accept(list);
 
        // Implement dispList using accept()
        dispList.accept(list);
    }
}

it has one method accept().

The BiConsumer Interface is a part of the java.util.function package which has been introduced since Java 8, to implement functional programming in Java. It represents a function that 
takes in two arguments and produces a result. However, these kinds of functions doesn’t return any value.

This functional interface takes in two generics, namely:- 

T: denotes the type of the first input argument to the operation
U: denotes the type of the second input argument to the operation
void accept(T t, U u)

The Predicate is a functional interface defined in java. util. Function package, which accepts an argument and returns a boolean. This is mainly used to filter data from a Java Stream. 
The filter method of a stream accepts a predicate to filter the data and returns a new stream satisfying the predicate.

The Consumer is a functional interface defined in java. util. Function package, which accepts a single input argument and returns no result. Unlike most other functional interfaces, the Consumer is expected to operate via side effects.

The Supplier is a functional interface defined in java. util. Function package. The Supplier interface takes no argument and returns a result. As this is a functional interface and can therefore be used as the assignment target for a lambda expression or method reference. The Supplier has only one get() method and has no default method. In the below example, we created the Supplier object which will be used to supply a new User object.

There are four type method references that are as follows:

Static Method Reference. GFG::compareByName
Instance Method Reference of a particular object. comparator::compareByName(object::methodName)
Instance Method Reference of an arbitrary object of a particular type. String::compareToIgnoreCase
Constructor Reference. StringBuilder::new

Java provides a new feature called method reference in Java 8. Method reference is used to refer method of functional interface. It is compact and easy form of lambda expression. Each time when you are using lambda expression to just referring a method, you can replace your lambda expression with method reference.

A Stream is a sequence of components that can be processed sequentially. These packages include classes, interfaces, and enum to allow functional-style operations on the elements. 
The stream can be used by importing java.util.stream package. Stream API is used to process collections of objects. Streams are designed to be efficient and can support improving your 
program’s performance by allowing you to avoid unnecessary loops and iterations.Streams can be used for filtering, collecting, printing, and converting from one data structure to another, 
etc.

Features of Java Stream
A stream is not a data structure instead it takes input from the Collections, Arrays, or I/O channels.
Streams don’t change the original data structure, they only provide the result as per the pipelined methods.
Each intermediate operation is lazily executed and returns a stream as a result, hence various intermediate operations can be pipelined. Terminal operations mark the end of the stream and 
return the result.

Parallel Streams are the type of streams that can perform operations concurrently on multiple threads. These Streams are meant to make use of multiple processors or cores available to 
speed up the processing speed. There are two methods to create parallel streams are mentioned below:

Using the parallel() method on a stream
Using parallelStream() on a Collection

Intermediate functions return a stream back.
On any stream you can execute any number of intermediate operations, but the terminal operation should be single and written at last. So following are the intermediate methods provided by 
the Stream
Predicate is a non-interfering, stateless predicate to apply to each element to determine if it should be included or not.

intList.stream().filter(
          element -> (element%2==0)
        )
        .forEach(
          element -> System.out.print(element+ " ")
        );

 intList.stream()
            .map(element -> element * element * element)
            .forEach(
                element -> System.out.print(element + " "));
 
        // Display message only
        System.out.println(
            "\n\nOutput after distinct() implementation : ");
 
        // Applying distinct() on this
        intList.stream()
            .distinct()
            .map(element -> element * element * element)
            .forEach(
                element -> System.out.print(element + " "));
 
        // Display message only
        System.out.println(
            "\n\nOutput after sorted() implementation : ");
 
        // Now applying sorted() on this
        intList.stream()
            .distinct()
            .sorted()
            .map(element -> element * element * element)
            .forEach(
                element -> System.out.print(element + " "));
 
        // Display message only
        System.out.println(
            "\n\nOutput after filter() implementation : ");
 
        // Applying Filter() that values
        // only below 10000 will be printed
        intList.stream()
            .distinct()
            .sorted()
            .map(element -> element * element * element)
            .filter(element -> element < 10000)
            .forEach(
                element -> System.out.print(element + " "));

Optional in java.util package. It can help in writing a neat code without using too many null checks. By using Optional, we can specify alternate values to return or alternate code to run.

SecurityContextHolder, to provide access to the SecurityContext.
SecurityContext, to hold the Authentication and possibly request-specific security information.
Authentication, to represent the principal in a Spring Security-specific manner.
GrantedAuthority, to reflect the application-wide permissions granted to a principal.
UserDetails, to provide the necessary information to build an Authentication object from your application’s DAOs or other source of security data.
UserDetailsService, to create a UserDetails when passed in a String-based username (or certificate ID or the like).

The AuthenticationManager is just an interface, so the implementation can be anything we choose, but how does it work in practice? What if we need to check multiple authentication 
databases or a combination of different authentication services such as a database and an LDAP server?

The default implementation in Spring Security is called ProviderManager and rather than handling the authentication request itself, it delegates to a list of configured 
AuthenticationProvider s, each of which is queried in turn to see if it can perform the authentication. Each provider will either throw an exception or return a fully populated 
Authentication object. Remember our good friends, UserDetails and UserDetailsService? If not, head back to the previous chapter and refresh your memory. The most common approach to 
verifying an authentication request is to load the corresponding UserDetails and check the loaded password against the one that has been entered by the user. This is the approach used 
by the DaoAuthenticationProvider (see below). The loaded UserDetails object - and particularly the GrantedAuthority s it contains - will be used when building the fully populated 
Authentication object which is returned from a successful authentication and stored in the SecurityContext.

userDetailsService ia an interface which provides the single method loadUserByUsername(String name) and returns the object of userDetails which inturn is an interface and which provide only the getter methods to get grantedAuthorities,username,token etc.
In loadUserByUsername method after fetching the user object we return userDetails object by its implementing class build method;

The AuthenticationEntryPoint in Spring Security is invoked when an unauthenticated request is received by a secured endpoint. It is responsible for handling authentication failures and 
initiating the authentication process.
AuthenticationEntryPoint is an interface which posseses the single method commence()
Overall, the commence() method plays a crucial role in handling authentication failures and initiating the authentication process, ensuring that unauthenticated requests are handled 
appropriately according to the application's security policies.

If you don't provide an AuthenticationEntryPoint implementation while using UsernamePasswordAuthenticationFilter, and an unauthenticated request is made to a secured resource, 
Spring Security will use its default behavior to handle the authentication failure.

Without a custom AuthenticationEntryPoint, Spring Security typically sends a 401 Unauthorized response to the client when an unauthenticated request is made to a secured resource.
 This response indicates that authentication is required to access the requested resource. However, the response may lack additional details or customization that you might want to provide
 to the client.

In the context of Spring Security, the AuthenticationManager is a core component responsible for authenticating users based on their credentials. It is typically used to validate 
authentication requests, such as login attempts, against a configured set of authentication providers.

Here's an overview of how the AuthenticationManager works within Spring Security:

Authentication Request: When a user attempts to authenticate, usually by submitting login credentials through a form or an API request, Spring Security intercepts the authentication 
request.

Authentication Object: Spring Security creates an Authentication object encapsulating the user's credentials (e.g., username and password).

AuthenticationManager: The AuthenticationManager is responsible for processing the Authentication object and determining whether the provided credentials are valid. It delegates the 
authentication process to one or more configured AuthenticationProvider instances.

AuthenticationProvider: Each AuthenticationProvider in the authentication chain is responsible for authenticating users using a specific method or strategy. For example, 
Spring Security provides various AuthenticationProvider implementations for different authentication mechanisms, such as username/password authentication, LDAP authentication, 
or OAuth authentication.

Authentication Process: The AuthenticationManager iterates over the configured AuthenticationProviders until one of them successfully authenticates the user or until all providers have 
been tried without success.

Authentication Result: If authentication is successful, the AuthenticationManager returns a fully authenticated Authentication object containing details about the authenticated user, 
including their granted authorities (roles). If authentication fails, an exception is thrown or an unsuccessful authentication response is generated.

Authentication Flow Control: Depending on the outcome of the authentication process, Spring Security may proceed with the requested operation if authentication is successful or deny access and handle authentication failures accordingly.

In summary, the AuthenticationManager is a crucial component in Spring Security responsible for orchestrating the authentication process, validating user credentials, and determining 
whether access should be granted or denied based on the authentication result. It acts as the central authority for authenticating users within a Spring Security-enabled application.
AuthenticationManager has a single method authenticate() which authenticate the user.

AuthenticationProvider interface has 2 methods authenticate() and boolean supports. authenticate method returns the fully authenticated object if authenticated.and supports return true 
if AuthenticationProvider supports the indicated authentication object.and if false then request is delegated to another AuthenticationProvider.

In Spring Security, the AuthenticationManagerBuilder is a class that is used to build and configure an AuthenticationManager. The AuthenticationManager is a central interface in Spring 
Security responsible for authenticating users based on their credentials.

Here's an overview of how AuthenticationManagerBuilder works:

Configuration: In a Spring Security configuration class (typically annotated with @Configuration), you can use the AuthenticationManagerBuilder to define how authentication should be 
performed.

Method Chaining: The AuthenticationManagerBuilder provides methods for configuring authentication, such as inMemoryAuthentication(), jdbcAuthentication(), ldapAuthentication(), 
and userDetailsService(). These methods allow you to specify different authentication mechanisms and providers.

User Details: When configuring authentication, you typically provide details about users, such as their username, password, and authorities (roles). For example, with 
inMemoryAuthentication(), you can specify users and their roles directly in the configuration.

Delegation: Behind the scenes, the AuthenticationManagerBuilder delegates the configuration to appropriate AuthenticationProvider implementations based on the authentication mechanism 
being used. For example, inMemoryAuthentication() configures an InMemoryUserDetailsManager, while jdbcAuthentication() configures a DaoAuthenticationProvider backed by a JDBC datasource.

Customization: You can also customize the authentication process by providing custom implementations of UserDetailsService, PasswordEncoder, AuthenticationProvider, etc., 
and registering them with the AuthenticationManagerBuilder.

Building: Once the authentication configuration is complete, you call the AuthenticationManagerBuilder's build() method to build the AuthenticationManager instance.

Usage: The configured AuthenticationManager is then typically injected into other components, such as UsernamePasswordAuthenticationFilter or AuthenticationManager beans, to handle 
authentication requests within your application.

Overall, the AuthenticationManagerBuilder provides a convenient and flexible way to configure authentication in Spring Security, allowing you to tailor the authentication mechanism to 
your application's specific requirements.

ProviderManager: The ProviderManager class is responsible for managing a list of AuthenticationProvider instances and using them to authenticate Authentication requests. 
It iterates over the configured providers and delegates the authentication request to the first provider that supports the given Authentication token.

By default, Spring Boot does not enable CORS support, so if your frontend code tries to make requests to a Spring Boot backend hosted on a different domain, those requests will be blocked 
by the browser's CORS policy.

To allow such cross-origin requests, you can use the @CrossOrigin annotation in your Spring Boot controller. When you apply @CrossOrigin(origins = "*") to a controller or controller method,
 it tells Spring Boot to include the appropriate CORS headers in the response, allowing requests from any origin (*) to access the controller's endpoints.

what happens if you select spring application port to 0.
if you want to run multiple instances then in that case available port will be 
assigned to your application.

The Domain Name System (DNS) is the phonebook of the Internet. Humans access information online through domain names, like nytimes.com or espn.com. Web browsers interact through 
Internet Protocol (IP) addresses. DNS translates domain names to IP addresses so browsers can load Internet resources.
Each device connected to the Internet has a unique IP address which other machines use to find the device.

The process of DNS resolution involves converting a hostname (such as www.example.com) into a computer-friendly IP address (such as 192.168.1.1). An IP address is given to each device on the Internet, and that address is necessary to find the appropriate Internet device - like a street address is used to find a particular home. When a user wants to load a webpage, a translation must occur between what a user types into their web browser (example.com) and the machine-friendly address necessary to locate the example.com webpage.

Unlike traditional HTTP, which follows a request-response model, WebSockets allow bi-directional communication. This means that the client and the server can send data to each other anytime without continuous polling.

WebSockets are used for real-time, event-driven communication between clients and servers. They are particularly useful for building applications requiring instant updates, such as real-time chat, messaging, and multiplayer games.

In traditional HTTP, the client sends a request to the server, and the server responds with the requested data. This request-response model requires continuous polling from the client to the server, which can result in increased latency and decreased efficiency.

On the other hand, WebSockets establish a persistent connection between the client and the server. This means that once the connection is established, the client and the server can send data to each other at any time without continuous polling. This allows realtime communication, where updates can be sent and received instantly.

An API gateway accepts API requests from a client, processes them based on defined policies, directs them to the appropriate services, and combines the responses for a simplified user experience. Typically, it handles a request by invoking multiple microservices and aggregating the results. It can also translate between protocols in legacy deployments.

A checksum is a value calculated from a data set to verify its integrity or authenticity. It is a fixed-size, unique value derived from the data using an algorithm, and it serves as a digital fingerprint for the data. Checksums are commonly used in computer networks, storage systems, and data transmission protocols to detect errors or tampering in transmitted or stored data.



Tips for Managing Large Datasets in Java Spring
1.Optimize Loops
The result can be influenced heavily by the iterative processes hence it is paramount to ensure they are optimized and, in most cases, the use of optimized code and
 enhanced for loops will suffice.

Example:

// Inefficient loop
for (int i = 0; i < list.size(); i++) {
    process(list.get(i));
}// Optimized loop
int size = list.size();
for (int i = 0; i < size; i++) {
    process(list.get(i));
}
Explanation: In the first example, list.size() has been invoked with every cycle of the loop which can be expensive. The second example solves this problem by making
 sure that the size of the object is placed outside of the loop.

2.Use Optional Carefully
Using Optional to handle nullable values is useful but avoid using it in collections or as fields in data classes, as it adds overhead.
// Avoid
private Optional<String> name;  // Creates unnecessary wrapping objects.// Optimize
// Use null checks instead or initialize with default values.
private String name;
3.Use Batch Processing
If you’re dealing with large datasets (e.g., importing/exporting data), process them in batches to reduce memory consumption you can add springBatch our make it in this
 way very easily.
@Transactional
public void importEmployees(List<Employee> employees) {
int size = employees.size();
    for (int i = 0; i < size ; i += 100) {
//this is great make very simple batch
        List<Employee> batch = employees.subList(i, Math.min(i + 100, employees.size()));
        employeeRepository.saveAll(batch);
        employeeRepository.flush();  // Clear persistence context to free memory.
    }
}
4.Stream API Best Practices
When using Java Streams, avoid creating large, intermediate collections. Use lazy evaluation and terminal operations wisely.
// Avoid
List<String> names = employees.stream()
                              .filter(e -> e.getAge() > 30)
                              .map(Employee::getName)	
                              .collect(Collectors.toList()); 	
 // Collects intermediate results to a new List// Optimize
employees.stream()
         .filter(e -> e.getAge() > 30)
         .map(Employee::getName)
         .forEach(System.out::println);  // Directly use forEach to avoid extra memory allocation.
5. Use Efficient Data Structures
Choose the right data structure for your needs. For example:
Use ArrayList over LinkedList if you require random access, as ArrayList uses less memory per element.
Use HashMap with proper initial size to avoid resizing during runtime.
Use primitive data types instead of their wrapper classes when possible (e.g., int instead of Integer).
Example:

// Avoid
List<Integer> list = new ArrayList<>();
for (int i = 0; i < 10000; i++) {
    list.add(i);  // Autoboxing adds unnecessary overhead.
}
// Use primitive arrays
int[] arr = new int[10000];  // Avoids autoboxing
for (int i = 0; i < arr.length; i++) {
    arr[i] = i;
}
// Estimate the initial capacity based on expected number of entries
 int expectedEntries = 10000;
 float loadFactor = 0.75f;
 int initialCapacity = (int) (expectedEntries / loadFactor + 1);
// Create a HashMap with the calculated initial capacity
  HashMap<String, Integer> accountBalances = new HashMap<>(initialCapacity);
6.Avoid Unnecessary Object Creation
Reuse objects wherever possible instead of creating new instances repeatedly. For example, avoid using new inside loops.
// Avoid
for (int i = 0; i < 1000; i++) {
    String result = new String("Result");  // Creates a new String object in each iteration.
}
// Optimize
String result = "Result";  // Reuse the same object (string literals are interned in Java).
for (int i = 0; i < 1000; i++) {
    // Use 'result' without creating a new instance.
}
7. Use Parallel Streams and Fork/Join Framework
Implementing Fork/Join and parallel streams in Java for CPU-bound execution can accelerate the process by making use of the multiple cores available.

Example:

// Sequential stream
list.stream().forEach(this::process);
// Parallel stream
list.parallelStream().forEach(this::process);
Explanation: The parallel stream makes use of multiple threads in the data processing which leads to faster execution on multi-core processors. The caution
 here is that parallel streams should not be overused, especially concerning overhead due to thread management.

8.Tune the JVM Garbage Collector
Garbage collection in Java is good but to a limit, and elements need to be tuned well, otherwise, performance is hampered II: An appropriate set of JVM options
 such as –xms and -xmx can be employed to define the initial and maximum heaps, and also for deploying a garbage collector which suits an application such as G1GC, or ZGC.

Example:

# JVM options for GC tuning
java -Xms1g -Xmx2g -XX:+UseG1GC -XX:MaxGCPauseMillis=200 -jar myapp.jar
Explanation: It has been found that poor performance of controlled allocation followed by garbage collection is a result of tolerating uncontrolled allocation.
 As is now evident, if the allocator fits the application well, then over a usage period the expected average will be close. Reducing GC pauses and optimizing memory
 usage for the application at hand can be beneficial.

9.Enable Garbage Collection Logs
Enable GC logs in production to monitor memory usage and detect if excessive memory is being consumed or freed frequently.
-XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps
10. Avoid Memory Leaks in Long-Lived Objects
Be cautious with static fields or long-lived objects holding large references. These objects may not be garbage collected, leading to memory leaks.
// Avoid
public class Cache {
    private static List<Employee> employeeCache = new ArrayList<>();
}
// Optimize
public class Cache {
    private static WeakHashMap<String, Employee> employeeCache = new WeakHashMap<>();  // Using Weak References
}
11.Avoid Full Object Serialization
When serializing objects (e.g., for session persistence or caching), avoid serializing unnecessary fields by marking them as transient.
public class Employee implements Serializable {
    private String name;
    private transient int salary;  // 'salary' will not be serialized
}
Conclusion
Handling large datasets efficiently is crucial for building robust and scalable applications. By leveraging Java Spring’s capabilities, such as optimized queries,
 caching, and batch processing, you can significantly improve performance and manageability. Additionally, using techniques like setting an appropriate initial size
 for data structures, such as HashMap, helps avoid unnecessary overhead during runtime. Implementing these best practices ensures your applications are well-equipped
 to handle large volumes of data efficiently .
 
 Autowired annotation is not supported on static fields due to binding at compile time.
 
 GET - Retreives data from server. it should only retreive data from server and doesn't do any changes in server.
 POST - Used to send data to server to create a new resource . The data is included in the body.
 PUT - similar to post and used when we want to update an existing resource . The complete resource is being updated.
 PATCH - also used to update resource but only the included fields are updated.
 DELETE - is iused to delete the resource from the server.
 
 Concurrency means an application is making progress on more than one task at the same time.

In a computer, the tasks are executed using Central Processing Unit (CPU).

While a single CPU can work on only one task at a time, it achieves concurrency by rapidly switching between tasks.

For example, consider playing music while writing code. The CPU alternates between these tasks so quickly that, to the user, it feels like both are happening at the same time.

Parallelism means multiple tasks are executed simultaneously.

To achieve parallelism, an application divides its tasks into smaller, independent subtasks. These subtasks are distributed across multiple CPUs, CPU cores, GPU cores, or similar 
processing units, allowing them to be processed in parallel.

How does Parallelism Works?
Modern CPUs consist of multiple cores. Each core can independently execute a task. Parallelism divides a problem into smaller parts and assigns each part to a separate core for 
simultaneous processing.

Best API Response Structure - 

Best Example of generic class
public class ApiResponse<T> {
    private String status;
    private String message;
    private T data;
    private Object metadata;

    public ApiResponse(String status, String message, T data, Object metadata) {
        this.status = status;
        this.message = message;
        this.data = data;
        this.metadata = metadata;
    }

    // Getters and setters omitted for brevity
}

@RestController
@RequestMapping("/api/users")
public class UserController {

    @GetMapping("/{id}")
    public ResponseEntity<ApiResponse<User>> getUserById(@PathVariable Long id) {
        User user = userService.findById(id);
        ApiResponse<User> response = new ApiResponse<>(
                "success",
                "User retrieved successfully",
                user,
                null
        );
        return ResponseEntity.ok(response);
    }
}

@RestControllerAdvice
public class GlobalExceptionHandler {

    @ExceptionHandler(ResourceNotFoundException.class)
    public ResponseEntity<ApiResponse<Object>> handleResourceNotFound(ResourceNotFoundException ex) {
        ApiResponse<Object> response = new ApiResponse<>(
                "error",
                ex.getMessage(),
                null,
                null
        );
        return ResponseEntity.status(HttpStatus.NOT_FOUND).body(response);
    }
}

public class ResponseUtil {

    public static <T> ApiResponse<T> success(String message, T data, Object metadata) {
        return new ApiResponse<>("success", message, data, metadata);
    }

    public static <T> ApiResponse<T> error(String message, T data) {
        return new ApiResponse<>("error", message, data, null);
    }
}
 
 ------------------------------------------------------------------------------------------------------------------------
 
@Component
public class DateUtils {
    public static LocalDate parse(String date) {
        DateTimeFormatter formatter = DateTimeFormatter.ofPattern("yyyy-MM-dd"); // Adjust the pattern as needed
        try {
            return LocalDate.parse(date, formatter);
        } catch (DateTimeParseException e) {
            e.printStackTrace();
            return null; // Or handle the exception as needed
        }
    }
}

Utility classes generally contain static methods and do not need Spring’s dependency injection or lifecycle management.

Using @Component on a utility class is unnecessary because it does not need to be instantiated or managed by Spring.

Impact of Overusing @Component:

Unnecessary Complexity: Introducing @Component without any actual need adds unnecessary complexity to the application.
Resource Management: Spring will create and manage a bean for the utility class, which is an unnecessary use of resources.
Misleading Semantics: It might mislead other developers into thinking that the class has dependencies or state that needs to be managed by Spring.

For utility classes like DateUtils that only contain static methods and do not require any Spring features, it’s best to avoid using @Component. 
Simply defining the class without any Spring annotations is the most appropriate approach.

Improper `@Autowired` Injection
-Mistake: Using field injection with `@Autowired`.
-Tip: Use constructor-based injection for better testability and immutability.


@Service
public class EmployeeService {

    @Autowired
    private EmployeeRepository employeeRepository;

    public List<Employee> getAllEmployees() {
        return employeeRepository.findAll();
    }
}
In your EmployeeService class, using constructor injection instead of field injection (@Autowired on the field) is generally considered a better practice. Constructor injection 
promotes better testability, readability, and helps in managing dependencies more effectively.

Here’s how you can refactor your EmployeeService class using constructor injection:

public class EmployeeService {

    private final EmployeeRepository employeeRepository;

    // Constructor injection
    public EmployeeService(EmployeeRepository employeeRepository) {
        this.employeeRepository = employeeRepository;
    }

    public List<Employee> getAllEmployees() {
        return employeeRepository.findAll();
    }
}
Constructor Injection Benefits:

Testability: Constructor injection allows you to easily mock dependencies when writing unit tests for EmployeeService.
Explicit Dependencies: It makes dependencies explicit, improving readability and reducing the chance of null pointer exceptions.
Immutable Dependencies: Once initialized, dependencies (like employeeRepository in this case) cannot be changed, promoting immutability.

When using the @Async annotation, by default, the SimpleAsyncTaskExecutor thread pool is used. This thread pool is not a true thread pool.

Using this thread pool cannot achieve thread reuse. A new thread will be created every time it is called. If threads are continuously created in the system,
it will eventually lead to excessive memory usage by the system and cause an OutOfMemoryError.

SimpleAsyncTaskExecutor: Not a real thread pool. This class does not reuse threads and creates a new thread every time it is called.
SyncTaskExecutor: This class does not implement asynchronous calls and is only a synchronous operation. Only applicable to scenarios where multithreading is not needed.
ConcurrentTaskExecutor: An adaptation class for Executor. Not recommended. Consider using this class only when ThreadPoolTaskExecutor does not meet the requirements.
ThreadPoolTaskScheduler: Can use cron expressions.
ThreadPoolTaskExecutor: Most commonly used and recommended. In essence, it is a wrapper for java.util.concurrent.ThreadPoolExecutor.

Let’s directly look at the code implementation. Here, a thread pool named asyncPoolTaskExecutor is implemented:

@Configuration
@EnableAsync
public class SyncConfiguration {
    @Bean(name = "myAsyncPoolTaskExecutor")
    public ThreadPoolTaskExecutor executor() {
        ThreadPoolTaskExecutor taskExecutor = new ThreadPoolTaskExecutor();
        // Core thread count.
        taskExecutor.setCorePoolSize(10);
        // The maximum number of threads maintained in the thread pool. Only when the buffer queue is full will threads exceeding the core thread count be requested.
        taskExecutor.setMaxPoolSize(100);
        // Cache queue.
        taskExecutor.setQueueCapacity(50);
        // Allowed idle time. Threads other than core threads will be destroyed after the idle time arrives.
        taskExecutor.setKeepAliveSeconds(200);
        // Thread name prefix for asynchronous methods.
        taskExecutor.setThreadNamePrefix("async-");
        /**
         * When the task cache queue of the thread pool is full and the number of threads in the thread pool reaches maximumPoolSize, if there are still tasks coming, a
		 task rejection policy will be adopted.
         * There are usually four policies:
         * ThreadPoolExecutor.AbortPolicy: Discard the task and throw RejectedExecutionException.
         * ThreadPoolExecutor.DiscardPolicy: Also discard the task, but do not throw an exception.
         * ThreadPoolExecutor.DiscardOldestPolicy: Discard the task at the front of the queue and then try to execute the task again (repeat this process).
         * ThreadPoolExecutor.CallerRunsPolicy: Retry adding the current task and automatically call the execute() method repeatedly until it succeeds.
         */
        taskExecutor.setRejectedExecutionHandler(new ThreadPoolExecutor.CallerRunsPolicy());
        taskExecutor.initialize();
        return taskExecutor;
    }
}

1. What is Type Erasure in Java?
Answer:

Type erasure is a process by which the Java compiler removes all generic type information during compilation. This means that generic types are enforced only at compile-time, 
and the compiled bytecode contains raw types. It’s how Java ensures backward compatibility with older versions that didn’t use generics.

2. Can You Override a Private or Static Method in Java?
Answer:

No, you cannot override a private or static method. Private methods are not visible to subclasses, and static methods belong to the class, not instances. However, you can hide a
static method by declaring a new static method in the subclass with the same signature.

3. What Happens If a Static Modifier is Not Used with Main Method?
Answer:

If you omit the static keyword from the main method, the JVM won't be able to call it as an entry point of the program. This will result in a runtime error: 
"NoSuchMethodError: main".

4. Explain the Difference Between == and equals() Method.
Answer:

== checks for reference equality, i.e., whether two references point to the same object in memory.
equals() checks for value equality, i.e., whether two objects are logically equal.

5. What is the Use of a Volatile Keyword in Java?
Answer:

The volatile keyword ensures that a variable's value is always read from the main memory, not from a thread's local cache. It's used to prevent memory consistency errors 
in multithreaded environments.

6. What is the Difference Between StringBuilder and StringBuffer?
Answer:

Both are mutable sequence of characters, but:

StringBuilder is non-synchronized and faster.
StringBuffer is synchronized (thread-safe) but slower due to overhead of synchronization.

7. Can You Access Non-Static Variables in the Static Context?
Answer:

No, you cannot access non-static (instance) variables from a static context (like a static method) because static context doesn’t have access to instance variables.

8. What is the Purpose of the transient Keyword?
Answer:

The transient keyword indicates that a variable should not be serialized. During serialization, the value of a transient variable is not included in the serialized representation 
of the object.

10. What is a Memory Leak in Java?
Answer:

A memory leak in Java occurs when objects are no longer needed but are still referenced, preventing the garbage collector from reclaiming their memory. Over time, this can lead to 
OutOfMemoryError.

11. How Does the HashMap Work Internally?
Answer:

HashMap uses an array of linked lists (buckets). It computes the hash code of the key and places the key-value pair in the corresponding bucket. In Java 8 and above, when a bucket 
becomes 
too large, it transforms into a tree to optimize performance.

12. What is the Difference Between throw and throws?
Answer:

throw is used to explicitly throw an exception.
throws declares that a method might throw certain exceptions, alerting callers to handle them.

13. Can You Serialize a Static Variable?
Answer:

No, static variables belong to the class, not instances. Serialization works on instance variables, so static variables are not serialized.

14. What is the finalize() Method?
Answer:

finalize() is a method called by the garbage collector before it reclaims an object's memory. It's used to perform cleanup, but its execution is not guaranteed, 
so it's better to use try-with-resources or finally blocks.

15. Explain the Producer-Consumer Problem and How You’d Solve It in Java.
Answer:

The Producer-Consumer problem involves two threads: one producing data and another consuming it. In Java, you can solve this using synchronization, wait/notify mechanisms, or 
higher-level constructs like BlockingQueue.

16. What is a ClassLoader in Java?
Answer:

A ClassLoader is a part of the Java Runtime Environment that loads classes into memory. It abstracts the process of locating and importing class files.

17. Explain the Difference Between sleep() and wait().
Answer:

sleep() pauses the current thread for a specified duration but doesn't release any locks.
wait() causes the current thread to wait until another thread invokes notify() or notifyAll() on the same object, and it releases the object's lock.

18. What is the Use of the assert Keyword?
Answer:

The assert keyword is used for debugging purposes to make assertions about the program's state. If the assertion fails, it throws an AssertionError.

19. How Do You Prevent a Class from Being Subclassed?
Answer:

By declaring the class as final, you prevent it from being subclassed.

public final class MyClass {
    // class body
}

20. What is the Difference Between Callable and Runnable Interfaces?
Answer:

Runnable doesn't return a result and cannot throw checked exceptions.
Callable returns a result and can throw checked exceptions.

21. Explain the Term “Daemon Thread”.
Answer:

A daemon thread is a background thread that doesn’t prevent the JVM from exiting when all user threads are finished. It’s typically used for tasks like garbage collection.

23. How Does Autoboxing and Unboxing Work?
Answer:

Autoboxing automatically converts primitive types to their corresponding wrapper classes, while unboxing converts wrapper classes back to primitive types.

24. What is a WeakReference in Java?
Answer:

A WeakReference allows you to hold a reference to an object without preventing it from being garbage collected. Useful for caches where you want to allow GC to reclaim memory if 
needed.

25. Explain the Fork/Join Framework.
Answer:

The Fork/Join framework is used for parallel execution of tasks. It splits a task into smaller subtasks (forks), processes them in parallel, and then joins the results.

26. What is the Difference Between String, StringBuilder, and StringBuffer?
Answer:

String is immutable.
StringBuilder is mutable and not thread-safe.
StringBuffer is mutable and thread-safe due to synchronization.

28. What is the Use of the default Keyword in Interfaces?
Answer:

The default keyword allows you to add new methods to interfaces without breaking existing implementations by providing a default implementation.

29. Explain Double-Checked Locking in Singleton Pattern.
Answer:

Double-checked locking reduces the overhead of acquiring a lock by first checking the locking criterion without synchronization. Only if the check indicates that locking is required 
does the thread synchronize.

Intermediate Operations: filter(), map(), flatMap(), distinct(), sorted(), peek(), limit(), skip();

Terminal Operations: forEach(), collect(), reduce(), count(), min(), max(), anyMatch(), allMatch(), noneMatch(), findFirst(), findAny().

Parallel Streams :
Java Streams provide a powerful way to process sequences of elements, and parallel streams allow you to perform operations on those elements concurrently, taking advantage of 
multiple CPU cores

A parallel stream divides the source data into multiple parts and processes each part in parallel using multiple threads. This can improve performance for large datasets, especially 
for CPU-intensive operations.

Sequential stream :
In Java, a sequential stream is the default type of stream processing, where elements are processed one at a time in the order they appear in the source. Sequential streams are 
simple to use and are suitable for many scenarios, especially when the dataset is small or when operations are not CPU-intensive.

What is difference between ClassNotFoundException and NoClassDefFoundError?
ClassNotFoundException occurs when you try to load a class at runtime using Class.forName() or loadClass() methods and requested classes not found in classpath. This error occurs 
when you do not have corresponding Jar on classpath. It is a checked exception.

NoClassDefFoundError occurs when the class was present during compile time and the program was compiled successfully but the class was not present during runtime. It is an Error. 
May be compile your project and then deleted one of the .class file. It will give NoClassDefFoundError.

Explain different types of Encryption?
There are basically two types of Encryption. Symmetric Encryption — Single key is used for encryption and decryption. It is used when you can securely share key between sender and 
recipient. Symmetric Encryption is used when you want to encrypt large amount of data, encrypt data at rest. Common Symmetric Encryption method — AES, DES

Asymmetric Encryption: Two different keys are used — Private Key and Public Key. Data encrypted with public key and it can only be decrypted with corresponding private key. A 
website’s SSL/TLS certificate which is available publicly contains the Public Key. Common Asymmetric Encryption method — RCA

How ConcurrentHashMap is different from HashMap?
ConcurrentHashMap divides the map into segments and only locks particular segment when writing. So this leads to better concurrency by permitting multiple threads to write 
different segments simultaneously.

Reads in ConcurrentHashMap are non-blocking and do not require locks. This means multiple threads can safely read from the map simultaneously without waiting on locks, which 
significantly improves performance in read-heavy scenarios. The volatile mechanism ensures memory visibility, meaning when a value is written by one thread, it becomes visible to 
all other threads immediately. This allows read operations to always see the most recent value.

When you open an URL/website in browser, what all steps the request go through? Please explain step by step?
When you open a URL or website in a browser, the request goes through several steps:

DNS Resolution:

The browser checks its cache for the DNS record of the domain.
If not found, it queries the local DNS server to resolve the domain to an IP address.
TCP Connection:

The browser establishes a TCP connection with the server using the resolved IP address on port 80 (HTTP) or 443 (HTTPS).
TLS Handshake (if HTTPS):

If the connection is HTTPS, a TLS handshake occurs to establish a secure connection.
HTTP Request:

The browser sends an HTTP request to the server, including headers and possibly a payload (for POST requests).
Server Processing:

The server processes the request, which may involve querying databases, running server-side scripts, etc.
HTTP Response:

The server sends back an HTTP response, including headers and the requested resource (HTML, CSS, JavaScript, images, etc.).
Rendering:

The browser parses the HTML and builds the DOM (Document Object Model).
It fetches and processes CSS to apply styles.
It fetches and executes JavaScript.
It renders the content on the screen.
Additional Requests:

The browser may make additional requests for resources like images, CSS files, JavaScript files, etc., repeating the HTTP request/response cycle for each.
User Interaction:

The browser handles user interactions, which may trigger additional requests or updates to the DOM.

What is difference between 2PC and SAGA design pattern?
The main differences between 2PC (Two-Phase Commit) and the SAGA design pattern are:

Two-Phase Commit (2PC)

Nature: Immediate transactions.
Commit/Rollback: Entire transaction is committed or rolled back seamlessly.
Agreement: All parties must agree to commit; otherwise, the transaction is rolled back.
Use Case: Suitable for scenarios with a limited number of services.
Fault Tolerance: Less fault-tolerant due to a single point of failure.
Locking: Requires global locks, which can lead to blocking and reduced scalability.
SAGA Pattern

Nature: Long-running transactions.
Compensating Actions: Each action has a corresponding compensating action (e.g., Create Order / Revert Order).
Scalability: More scalable and fault-tolerant as it does not require global locks.
Use Case: Suitable for scenarios involving many different services.
Failure Handling: If a step fails (e.g., Payment request fails), compensating actions are executed to revert previous steps (e.g., Revert Inventory, Revert Order).
Transaction Splitting: Splits the transaction into multiple steps, each with its own commit and compensating action.
Example Scenario

2PC:
All services (Order, Inventory, Payment) must agree to commit the transaction. If any service disagrees, the entire transaction is rolled back.
SAGA:
If the Payment request fails, the SAGA pattern will execute compensating actions to revert the Inventory and Order steps, ensuring eventual consistency.

What is CQRS design pattern?
CQRS (Command Query Responsibility Segregation) is a design pattern used to separate the read and write operations in a system. It splits the responsibilities of handling commands 
(operations that change the state of an application) from queries (operations that retrieve data without modifying it).

It is very subjective how to implement it depends based on requirement. For example — On Command Service side there will be event like Create Product, Update Product Quantity, 
Update Product Price, Delete Product. It will publish this event to Kafka. The Query service will consume these events and will have its own store (May be Elastic Search) and store 
data in required form that be queries based on business requirement. Since its event base, It is possible to time travel and see what changes are done over the time. It also helps 
to revert changes easily.

What are different design patterns used in Distributed System?
Service Discovery and Registration, Load Balancing, Client Side Load Balancing, Circuit Breaker, API Gateway, Synchronous Communication (REST), Asynchronous Communication 
(Messaging — Example Kafka, ActiveMQ), Rate Limiting.

I have Employee Class with 200+ different fields inside it. I want to serialize just 10 fields out of this 200 fields when serializing this object. What is best way to achieve this?
Use Externalizable interface. It gives more control where we can write logic for Serialization. Here we just need to serialize 10 fields that is needed.

import java.io.Externalizable;
import java.io.IOException;
import java.io.ObjectInput;
import java.io.ObjectOutput;
public class Employee implements Externalizable {
    // 50 fields (for simplicity, we show only 6)
    private String name;
    private int age;
    private String department;
    private String address;
    private double salary;
    private String nonSerializedField; // Field not to be serialized
    // Constructors
    public Employee() {
        // No-arg constructor required by Externalizable
    }
    public Employee(String name, int age, String department, String address, double salary, String nonSerializedField) {
        this.name = name;
        this.age = age;
        this.department = department;
        this.address = address;
        this.salary = salary;
        this.nonSerializedField = nonSerializedField;
    }
    // Getter and Setter Methods
    public String getName() {
        return name;
    }
    public int getAge() {
        return age;
    }
    public String getDepartment() {
        return department;
    }
    public String getAddress() {
        return address;
    }
    public double getSalary() {
        return salary;
    }
    public String getNonSerializedField() {
        return nonSerializedField;
    }
    // Externalizable methods
    @Override
    public void writeExternal(ObjectOutput out) throws IOException {
        // Manually write only the 5 selected fields
        out.writeObject(name);
        out.writeInt(age);
        out.writeObject(department);
        out.writeObject(address);
        out.writeDouble(salary);
    }
    @Override
    public void readExternal(ObjectInput in) throws IOException, ClassNotFoundException {
        // Manually read only the 5 selected fields
        name = (String) in.readObject();
        age = in.readInt();
        department = (String) in.readObject();
        address = (String) in.readObject();
        salary = in.readDouble();
    }
    @Override
    public String toString() {
        return "Employee{" +
                "name='" + name + '\'' +
                ", age=" + age +
                ", department='" + department + '\'' +
                ", address='" + address + '\'' +
                ", salary=" + salary +
                ", nonSerializedField='" + nonSerializedField + '\'' +
                '}';
    }
}

TCP (Transmission Control Protocol) is a connection-oriented protocol that provides reliable data transmission by ensuring packets are delivered in order, with error-checking and 
retransmission if needed. This makes TCP slower but suitable for applications where accuracy is critical, like file transfers (FTP), email (SMTP) and web browsing (HTTP).

UDP (User Datagram Protocol), on the other hand, is a connectionless protocol that sends packets without establishing a connection, making it faster but less reliable since it 
doesn’t guarantee delivery or order. UDP is ideal for applications where speed is essential and minor data loss is acceptable, such as video streaming, VoIP, and online gaming.

Enums in Java provide a powerful way to represent a fixed set of constants. When used effectively in a Spring Boot API, enums can simplify code and incorporate design patterns like 
the Strategy Pattern, reducing the need for complex if-else or switch statements. Here's how using enums can enhance your API development:

Step 1: Define a DTO with Enums
@Getter
@Setter
public class CarSubmitRequestDto {
    private String name ;
    private CarColor color;
    private CarGrade grade;
}
Step 2: Define Enums with Behavior
CarColorEnum
@AllArgsConstructor
public enum CarColor {
    BLACK {
        @Override
        public String selectedColor() {
            return this + "--has 4% discount--";
        }
    },
    RED {
        @Override
        public String selectedColor() {
            return this + "--has 10% discount--";
        }
    },
    WHITE {
        @Override
        public String selectedColor() {
            return this + "--has 6% discount--";
        }
    };

    public abstract String selectedColor();
}
CarGradeEnum
@AllArgsConstructor
public enum CarGrade {
    SPORT {
        @Override
        public String carSelectedGrade() {
            return this + "-- grade has 17% extra charge premium--";
        }
    },
    NORMAL {
        @Override
        public String carSelectedGrade() {
            return this + "-- grade has 0% extra charge premium--";
        }
    },
    FULL_OPTION {
        @Override
        public String carSelectedGrade() {
            return this + "-- grade has 24% extra charge premium--";
        }
    };

    public abstract String carSelectedGrade();
}
Step 3: Use DTO in Controller
Make sure to use @RequestBody in your controller method. When you send a JSON request, the string value should match exactly one of the enum constants.

   @PostMapping("/")                                                                                            
   public  ResponseEntity<CarSubmitResponseDto> submit(@RequestBody CarSubmitRequestDto carSubmitRequestDto){   
       CarSubmitResponseDto responseDto = new CarSubmitResponseDto();                                           
       String result = service.carRequestSubmit(carSubmitRequestDto);                                           
       responseDto.setSubmitForm(result);                                                                       
       return ResponseEntity.ok(responseDto);                                                                   
   }                                                                                                            
Step 4: Simplify Logic in Service Layer
@Service
public class CarService {
    public String carRequestSubmit(CarSubmitRequestDto carDto) {
        return "Dear " + carDto.getName() + ": you are submitting: "
                + " model " + carDto.getGrade().carSelectedGrade()
                + " Color " + carDto.getColor().selectedColor();
    }
}
when use this you dont need to add alot of Boilerplate code as condition such this :

if (carSubmitRequestDto.getCarGrade().equals("SPORT")) {
    // logic for SPORT status
} else if (carSubmitRequestDto.getCarGrade().equals("NORMAL")) {
    // logic for NORMAL status
} else if (carSubmitRequestDto.getCarGrade().equals("FULL_OPTION")) {
    // logic for FULL_OPTION status
}
or something like this :



switch (carSubmitRequestDto.getCarGrade()) {
    case CarGrade.SPORT:
        // logic for active status
        break;
    case CarGrade.NORMAL:
        // logic for NORMAL status
        break;
    case CarGrade.FULL_OPTION:
        // logic for FULL_OPTION status
        break;
}
just do like this on your service layer method :

@Service
public class CarService {
    public String carRequestSubmit(CarSubmitRequestDto carDto){
        return "Dear "+ carDto.getName() + " : you are submitting : "
                +" model " + carDto.getGrade().carSelectedGrad()
                + "Color "+carDto.getColor().selectedColor();
    }
}

SELECT dt.id 
FROM dashboard_type_d dt 
LEFT JOIN tbl_dashboard_menu_d dm 
ON dm.dashboard_id = dt.id 
WHERE dm.dashboard_id IS NULL to find records in one table and not in another table

Underlying Data Structure
HashMap:

Implements a hash table.
Keys are hashed, and the hash code determines where the key-value pair is stored.
Does not maintain any order.
TreeMap:

Implements a red-black tree.
Keys are sorted in natural order (if the key implements Comparable) or by a custom comparator (if provided).
2. Order of Elements
HashMap:

No guaranteed order of elements. Iteration order may change as elements are added or removed.
TreeMap:

Maintains the keys in sorted order.
Useful when order matters (e.g., when performing range queries or displaying elements in ascending order).
3. Performance
HashMap:

Time Complexity:
O(1) for get, put, and remove (on average, assuming a good hash function).
Can degrade to O(n) in the worst case (e.g., when many keys hash to the same bucket).
Better performance for frequent insertions, lookups, and deletions.
TreeMap:

Time Complexity:
O(log n) for get, put, and remove.
Slower compared to HashMap due to the overhead of maintaining order in the red-black tree.
4. Null Keys and Values
HashMap:

Allows one null key.
Allows multiple null values.
TreeMap:

Does not allow null keys (throws NullPointerException).
Allows multiple null values.
5. Use Cases
HashMap:

Use when you need fast access to data without worrying about the order of elements.
Examples: Caching, storing configurations, and quick lookups.
TreeMap:

Use when you need to maintain the keys in a sorted order or need range-based operations.
Examples: Storing sorted data, implementing a dictionary, or handling priority queues.
6. Memory Usage
HashMap:

Requires more memory because of the internal hashing mechanism and potential bucket collisions.
TreeMap:

Requires less memory per entry compared to HashMap, but the red-black tree structure introduces additional overhead for balancing.

 
Back when applications ran on a single server, life was simple.

Today’s modern applications are far more complex, consisting of dozens or even hundreds of services, each with multiple instances that scale up and down dynamically.

This makes it harder for services to efficiently find and communicate with each other across networks.

That’s where Service Discovery comes into play.


1. What is Service Discovery?
Service discovery is a mechanism that allows services in a distributed system to find and communicate with each other dynamically.

It hides the complex details of where services are located, so they can interact without knowing each other's exact network spots.

Service discovery registers and maintains a record of all your services in a service registry. This service registry acts as a single source of truth that allows your services to 
query and communicate with each other.

Client-Side Discovery
In this model, the responsibility for discovering and connecting to a service lies entirely with the client.

How it works:

Visualized using Multiplayer
Service Registration: Services (e.g., UserService, PaymentService) register themselves with a centralized service registry.

They provide their network details (IP address and port) along with metadata like service health or version.

Client Queries the Registry: The client (a microservice or API gateway) sends a request to the service registry to find the instances of a target service (e.g., PaymentService).

The registry responds with a list of available instances, including their IP addresses and ports.

Client Routes the Request: Based on the information retrieved, the client selects one of the service instances (often using a load balancing algorithm) and connects directly to it.

The client maintains control over how requests are routed, such as distributing traffic evenly across instances or prioritizing the closest instance.

A Payment Service has three instances running on different servers.

When the Order Service needs to process a payment, it queries the service registry for the location of the Payment Service.

The service registry responds with a list of available instances (e.g., IP1:Port1, IP2:Port2, IP3:Port3).

The Order Service chooses an instance (e.g., IP1:Port1) and sends the payment request directly to it.

In this model, the client delegates the responsibility of discovering and routing requests to a specific service instance to a centralized server or load balancer.

Unlike client-side discovery, the client does not need to query the service registry directly or perform any load balancing itself.

Instead, the client simply sends a request to a central server (load balancer or api gateway), which handles the rest.

How it works:

Service Registration: Services register themselves with a centralized service registry, similar to client-side discovery.

The service registry keeps track of all service instances, their IP addresses, ports, and metadata.

Client Sends Request: The client sends a request to a load balancer or API gateway, specifying the service it wants to communicate with (e.g., payment-service).

The client does not query the service registry or know the specific location of the service instances.

Server Queries the Service Registry: The load balancer or gateway queries the service registry to find available instances of the requested service.

Routing: The load balancer selects a suitable service instance (based on factors like load, proximity, or health) and routes the client’s request to that instance.

Response: The service instance processes the request and sends the response back to the client via the load balancer or gateway.

Example Workflow
Let’s take an example of an e-commerce platform with microservices for "Order Management" and "Payment Processing."

Registration: The PaymentService registers two instances with the service registry:

Instance 1: IP1:8080

Instance 2: IP2:8081

Client Request: The OrderService sends a request to the load balancer or API gateway, specifying the PaymentService.

Discovery and Routing: The load balancer queries the service registry and retrieves the list of available PaymentService instances.

It selects one instance (e.g., IP1:8080) and routes the request to it.

Processing and Response: The selected instance of PaymentService processes the request and sends the response back to the OrderService via the load balancer.

Generics means parameterized types. Generics allow you to create classes, interfaces, and methods that operate on types specified as parameters. Using generics, you can write code 
that works with different types while enforcing compile-time type safety.

Example Without Generics:

List list = new ArrayList();
list.add("Hello");
String s = (String) list.get(0); // Must cast to String
With Generics:

List<String> list = new ArrayList<>();
list.add("Hello");
String s = list.get(0); // No casting needed, type safety is ensured

Why Use Generics?

Type Safety: Generics allow you to catch type errors at compile time, reducing the chance of ClassCastException.
// Without Generics, we can store any type of objects.
List list = new ArrayList();    
list.add(10);  
list.add("10");  

// With Generics, it is required to specify the type of object we need to store.  
List<Integer> list = new ArrayList<Integer>();    
list.add(10);  
list.add("10");// compile-time error  
2. Elimination of Casting: No need to cast when retrieving elements, as the compiler already knows the type.

// Before Generics, we need to type cast.
List list = new ArrayList();    
list.add("hello");    
String s = (String) list.get(0); //typecasting    

// After Generics, we don't need to typecast the object.  
List<String> list = new ArrayList<String>();    
list.add("hello");    
String s = list.get(0);  

Type erasure is a process where generic types are removed at runtime, replaced by their bounds or Object. This allows backward compatibility with older Java versions but means you 
can’t use type parameters at runtime.

2. How does hashCode() relate to equals()?
Interviewers love to ask this. If two objects are equal according to equals(), they must have the same hashCode(). If you break this rule, you’re in for some strange behavior when 
working with collections like HashMap or HashSet.

12. Can you explain what happens when an exception is thrown inside a finally block?
The finally block always executes, even if an exception is thrown. But if an exception occurs within finally, it will override any previous exceptions. Watch out for this—it can 
swallow important exceptions if not handled properly.

Both are used in multithreading but serve different purposes. The volatile keyword ensures that the value of a variable is always read from the main memory, ensuring visibility 
across threads. On the other hand, synchronized ensures that only one thread can execute a block of code at a time.

17. What is a ThreadLocal variable, and when would you use it?
A ThreadLocal variable provides each thread with its own independently initialized variable. This is useful when you want to avoid sharing variables between threads, like in cases 
of user sessions or database connections.

What is the difference between Callable and Runnable?
Both are used for executing code in separate threads, but Callable can return a result and throw checked exceptions, while Runnable does neither. If you need to fetch a result from 
a background task, Callable is the way to go.

A shallow copy copies the reference to an object, meaning any changes to the copied object will affect the original. A deep copy, on the other hand, creates a new instance of the 
object, fully independent of the original.

Can you explain how the try-with-resources statement works?
The try-with-resources statement was introduced in Java 7 to automatically close resources (like files or database connections) after use. Any class that implements the 
AutoCloseable interface can be used here, reducing the risk of resource leaks.

How does Java’s memory model handle instruction reordering?
Java’s memory model allows the JVM to reorder instructions for optimization purposes, but it ensures happens-before relationships are maintained. This means changes in one thread 
are visible to another when synchronization mechanisms are used properly.

What is the Optional class in Java, and why is it useful?
The Optional class helps prevent NullPointerException by providing a more elegant way to handle null values. Instead of checking for null directly, you can use Optional to safely 
return an empty value or handle null scenarios more gracefully.

The volatile keyword ensures that the value of a variable is always read from and written to main memory, rather than being cached by threads. It’s mainly used for flags and 
variables that are frequently updated across multiple threads.

The double-checked locking pattern is used to minimize the overhead of acquiring a lock by checking the condition both before and after acquiring the lock. It’s commonly used in 
the implementation of singleton classes to ensure thread safety while avoiding unnecessary locking.

Explain the concept of Auto-Configuration in Spring Boot.
Answer:

Auto-Configuration in Spring Boot automatically configures beans based on the dependencies in the classpath and defined properties. It eliminates the need for explicit bean 
declarations. Spring Boot scans the classpath and:

Detects available libraries (e.g., H2 database, Thymeleaf).
Configures beans relevant to those libraries.
Applies sensible defaults, which can be overridden by custom configurations.

Starter Dependencies are a set of convenient dependency descriptors that you can include in your application. They provide a ready-to-use set of dependencies for building specific 
types of applications.

@Configuration
@ConfigurationProperties(prefix = "mail")
public class ConfigProperties {
    
    private String hostName;
    private int port;
    private String from;

    // standard getters and setters
}
Copy
We use @Configuration so that Spring creates a Spring bean in the application context.

@ConfigurationProperties works best with hierarchical properties that all have the same prefix; therefore, we add a prefix of mail.

If we don’t use @Configuration in the POJO, then we need to add @EnableConfigurationProperties(ConfigProperties.class) in the main Spring application class to bind the properties 
into the POJO:

@SpringBootApplication
@EnableConfigurationProperties(ConfigProperties.class)
public class EnableConfigurationDemoApplication {
    public static void main(String[] args) {
        SpringApplication.run(EnableConfigurationDemoApplication.class, args);
    }
}
Copy
That’s it! Spring will automatically bind any property defined in our property file that has the prefix mail and the same name as one of the fields in the ConfigProperties class.

Spring Boot applications come with embedded servers like Tomcat, Jetty, or Undertow. This means:

Self-Contained Applications: No need to deploy WAR files to external servers.
Simplified Deployment: Run applications as standalone JARs.
Customizable: Configure server settings via properties.

@Id: Specifies the primary key of the entity.
@GeneratedValue: Defines the strategy for primary key generation (e.g., AUTO, IDENTITY).

Ways to check null pointer exception - 
 Simple If-Else Check
This is the most basic error prevention mechanism. By checking if the variable is null, we perform operations only if it is not.

TestObj obj = null;

if (obj != null) { //do the job if not null
  System.out.println(obj .getTestField());
} else { 
  //exception handling or the job if null
}
2. Correct Usage of Equals Method
When checking the value of a variable that can be null, we compare it to a constant value using the equals method. The key point here is to call the equals method on an object 
that is guaranteed not to be null. Otherwise, if we call it on an object that could potentially be null, we still risk encountering an error.

String str = null;

if ("test".equals(str)) {
  System.out.println("String is equal to 'test'.");
} else {
  System.out.println("String is null or not equal to 'test'.");
}
In this example, since the “test” String is not a variable, it is considered safe to call our equals method on this object. If we had called the equals method on the str variable 
to check if it is equal to “test”, we would risk encountering a NullPointerException.

3. Java 8 and Beyond: Usage of Optional
The Optional class introduced in Java 8 wraps a potentially nullable object, enabling safer and more readable code writing.

String str = null;

int length = Optional.ofNullable(str)  // Wrap the string (could be null)
        .filter(s -> !s.isEmpty())  // Proceed only if the string is not empty
        .map(String::length)  // Transform the string to its length
        .orElse(0);  // Provide default value if null or empty

System.out.println("String length: " + length);

Additionally we can use bean validation in request dto.

If there is no explicit transaction (e.g., using @Transactional), Spring creates a default transaction at the repository level only for the specific save() operation. However, 
this is a limited transaction scope and might not encompass other related operations or fail during complex use cases like cascading operations.

Consequences:

Changes made via repo.save() could still be persisted if the underlying JPA provider (like Hibernate) and database allow implicit transactions.
However, without a managed transaction, problems like partial updates, lack of rollback in case of failure, and potential data inconsistencies can occur.
Best Practice:

Always ensure that database operations occur within a transactional boundary, typically at the service layer, to maintain data integrity and consistency.
Annotate your service or repository methods with @Transactional where necessary to explicitly define a transaction.

The behavior you're describing—where setting values in your response class inadvertently affects the database—is likely due to how Hibernate (or your JPA provider) manages entities 
and their lifecycle. Here's an explanation of why this happens:

Key Concepts:
Persistent Context (First-Level Cache):

When you fetch an entity using a SELECT query (e.g., repository.findById() or a custom query), Hibernate manages that entity in the persistent context (or first-level cache).
Any changes made to this entity will be tracked and automatically persisted (flushed) to the database when the transaction is committed.
Dirty Checking:

Hibernate uses a mechanism called dirty checking to detect changes in managed entities. If you modify a managed entity's fields, Hibernate automatically issues an UPDATE statement 
during the session flush.
Detached Entities:

If you manipulate entities after detaching them from the persistence context, changes won't be synchronized with the database unless they are explicitly merged back into the 
context.

Direct Manipulation of Entity Objects: If your response class (e.g., ResponseDTO) is directly referencing the managed entity fetched from the database, any changes made to the 
fields of the entity object will be tracked and persisted by Hibernate.

Example:

java
Copy code
@Entity
public class MyEntity {
    private int count; // Assume this is a column in the database
    // Getters and setters
}

public ResponseDTO myServiceMethod() {
    MyEntity entity = repository.findById(1).orElseThrow();
    entity.setCount(100); // Hibernate tracks this change
    return new ResponseDTO(entity); // The change is unintentionally saved to the DB
}
In this case, setting count directly on the entity causes Hibernate to update the database.

No Explicit Transaction Management: If you're modifying the entity within a transactional context (e.g., @Transactional), Hibernate automatically flushes these changes to the 
database at the end of the transaction.

How to Prevent It:
Use DTOs or Projections: Avoid exposing or manipulating the entity object directly. Instead, map the entity data to a separate Data Transfer Object (DTO) or use a projection for 
the response.

java
Copy code
public ResponseDTO myServiceMethod() {
    MyEntity entity = repository.findById(1).orElseThrow();
    ResponseDTO response = new ResponseDTO(entity.getCount());
    return response;
}
Detach the Entity: If you must manipulate the entity but don't want Hibernate to track changes, detach it from the persistence context:

java
Copy code
@Transactional
public ResponseDTO myServiceMethod() {
    MyEntity entity = repository.findById(1).orElseThrow();
    entityManager.detach(entity); // Detach the entity
    entity.setCount(100); // This change won't be tracked
    return new ResponseDTO(entity.getCount());
}
Read-Only Transactions: If the method is purely for reading data, annotate it with @Transactional(readOnly = true) to prevent updates.

java
Copy code
@Transactional(readOnly = true)
public ResponseDTO myServiceMethod() {
    MyEntity entity = repository.findById(1).orElseThrow();
    ResponseDTO response = new ResponseDTO(entity.getCount());
    return response;
}
Custom Queries with DTO Results: Instead of fetching the entity, use a custom query to fetch only the required data into a DTO:

java
Copy code
@Query("SELECT new com.example.ResponseDTO(e.count) FROM MyEntity e WHERE e.id = :id")
ResponseDTO findResponseById(@Param("id") Long id);
Conclusion:
The issue arises because Hibernate tracks and persists changes to managed entities. By ensuring your response logic doesn't directly manipulate managed entities, you can avoid 
unintentional database updates. Use DTOs, projections, or detached entities for safe and clean handling of data.

securing application measures - 

1. Implement Security Measures
Authentication and Authorization
Use Spring Security: Implement role-based access control (RBAC) and secure endpoints using @PreAuthorize, @Secured, or @RolesAllowed.
JWT/OAuth2: Use JSON Web Tokens (JWT) or OAuth2 for secure authentication and stateless sessions.
Password Security: Hash passwords using bcrypt or PBKDF2.
Secure Communication
HTTPS: Use HTTPS to encrypt communication between clients and the server.
HSTS: Enforce HTTP Strict Transport Security (HSTS).
Protect Against Common Vulnerabilities
CSRF Protection: Enable CSRF tokens in Spring Security.
CORS Configuration: Configure CORS policies to restrict cross-origin requests.
SQL Injection Prevention: Use parameterized queries or Hibernate’s ORM.
XSS Protection: Sanitize input and enable HTTP headers like Content-Security-Policy.
Sensitive Data Management
Encrypt Sensitive Data: Encrypt sensitive fields in the database (e.g., account numbers, PINs).
Environment Variables: Store credentials securely using tools like AWS Secrets Manager, HashiCorp Vault, or Spring Boot’s @ConfigurationProperties.
Logging and Monitoring
Audit Trails: Maintain logs for authentication attempts, access patterns, and transactions.
Centralized Logging: Use ELK (Elasticsearch, Logstash, Kibana) or similar tools.

An API Gateway acts as a central server that sits between clients (e.g., browsers, mobile apps) and backend services.

Instead of clients interacting with multiple microservices directly, they send their requests to the API Gateway. The gateway processes these requests, enforces security, 
and forwards them to the appropriate microservices.

Core Features of an API Gateway - 
1. Authentication and Authorization
API Gateway secures the backend systems by ensuring only authorized users and clients can access backend services.

It handles tasks like:

Authentication: Verifying the identity of the client using tokens (e.g., OAuth, JWT), API keys, or certificates.

Authorization: Checking the client’s permissions to access specific services or resources.

By centralizing these tasks, the API gateway eliminates the need for individual services to handle authentication, reducing redundancy and ensuring consistent access control 
across the system.

2. Rate Limiting
To prevent abuse and ensure fair usage of resources, most API Gateways implement rate limiting.

This feature:

Controls the frequency of requests a client can make within a given timeframe.

Protects backend services from being overwhelmed by excessive traffic or potential denial-of-service (DoS) attacks.

For example, a public API might allow a maximum of 100 requests per minute per user. If a client exceeds this limit, the API Gateway will block additional requests until the rate 
resets.

3. Load Balancing
High-traffic applications rely on load balancing to distribute incoming requests evenly across multiple instances of a service.

The API Gateway can:

Redirect requests to healthy service instances while avoiding ones that are down or overloaded.

Use algorithms like round-robin, least connections, or weighted distribution to manage traffic intelligently.

4. Caching
To improve response times and reduce the strain on backend services, most API Gateways provide caching.

They temporarily store frequently requested data, such as:

Responses to commonly accessed endpoints (e.g., product catalogs or weather data).

Static resources like images or metadata.

Caching helps in reducing latency and enhancing user experience while lowering the operational cost of backend services.

5. Request Transformation
In systems with diverse clients and backend services, request transformation is essential for compatibility.

An API Gateway can:

Modify the structure or format of incoming requests to match the backend service requirements.

Transform responses before sending them back to the client, ensuring they meet the client’s expectations.

For instance, it might convert XML responses from a legacy service into JSON for modern frontend applications.

6. Service Discovery
Modern systems often involve microservices that scale dynamically.

The service discovery feature of an API Gateway dynamically identifies the appropriate backend service instance to handle each request.

This ensures seamless request routing even in environments where services frequently scale up or down.

7. Circuit Breaking
Circuit breaking is a mechanism that temporarily stops sending requests to a backend service when it detects persistent failures, such as:

Slow responses or timeouts.

Server errors (e.g., HTTP 500 status codes).

High latency or unavailability of a service.

The API Gateway continuously monitors the health and performance of backend services and uses circuit breaking to block requests to a failing service.

8. Logging and Monitoring
API Gateways provide robust monitoring and logging capabilities to track and analyze system behavior.

These capabilities include:

Logging detailed information about each request, such as source, destination, and response time.

Collecting metrics like request rates, error rates, and latency.

This data helps system administrators detect anomalies, troubleshoot issues, and optimize the system’s performance. Many API Gateways also integrate with monitoring tools like 
Prometheus, Grafana, or AWS CloudWatch.

spring:
  cloud:
    gateway:
      routes:
        - id: service-route
          uri: http://downstream-service
          predicates:
            - Path=/service/**
          filters:
            - name: CircuitBreaker
              args:
                name: serviceCB
                fallbackUri: forward:/fallback
resilience4j:
  circuitbreaker:
    instances:
      serviceCB:
        slidingWindowSize: 10
        failureRateThreshold: 50
        waitDurationInOpenState: 5000ms
        permittedNumberOfCallsInHalfOpenState: 3

@RestController
public class FallbackController {
    @GetMapping("/fallback")
    public ResponseEntity<String> fallback() {
        return ResponseEntity.ok("Fallback response: Service is currently unavailable.");
    }
}

4. How the Circuit Breaker Works in the API Gateway
Requests to the http://downstream-service route are monitored.
If failures exceed the defined threshold, the circuit breaker trips (moves to Open state).
While in the Open state, requests are short-circuited and routed to the fallback endpoint.
After the defined wait duration, it transitions to Half-Open to test if the downstream service is healthy.
If the service is healthy, the circuit closes; otherwise, it remains Open.

eliminating if - else from code can make code more cleaner,modular and easy to understand.

1 using startegy pattern.
2 Enum Usage
Enums can be used to represent a set of predefined constants and their associated behaviors.

Example: Order Status Management
Define an OrderStatus enum with different behaviors:

public enum OrderStatus {
    NEW {
        @Override
        public void handle() {
            System.out.println("Processing new order.");
        }
    },
    SHIPPED {
        @Override
        public void handle() {
            System.out.println("Order shipped.");
        }
    },
    DELIVERED {
        @Override
        public void handle() {
            System.out.println("Order delivered.");
        }
    };

    public abstract void handle();
}
Use this enum in a service:

@Service
public class OrderService {
    public void processOrder(OrderStatus status) {
        status.handle();
    }
}
3. Polymorphism
4. Lambda Expressions and Functional Interfaces

Query hints are optional parameters you can pass to the JPA provider to modify query execution. Examples include:

Enabling or disabling caching.
Optimizing query execution.
Specifying read-only results.

@QueryHints Overview
The @QueryHints annotation is part of the org.springframework.data.jpa.repository package and can be used with:

Repository methods in Spring Data JPA.
Native or JPQL queries.
Attributes
value: An array of @QueryHint annotations.
Each @QueryHint requires:
name: Name of the hint (e.g., org.hibernate.readOnly).
value: Value for the hint (e.g., true or false).

Example Use Cases
3.1 Caching Results
You can use @QueryHints to enable or disable second-level caching for a query.

3.2 Read-Only Queries
Set queries to read-only mode for optimization, especially for read-intensive operations.

3.3 Custom Behavior
Pass provider-specific hints (e.g., Hibernate-specific hints) to fine-tune performance.

import org.springframework.data.jpa.repository.QueryHints;
import org.springframework.data.jpa.repository.JpaRepository;

import javax.persistence.QueryHint;

public interface UserRepository extends JpaRepository<User, Long> {

    // Example 1: Read-Only Query
    @QueryHints({@QueryHint(name = "org.hibernate.readOnly", value = "true")})
    List<User> findByName(String name);

    // Example 2: Disable Second-Level Cache for Query
    @QueryHints({@QueryHint(name = "org.hibernate.cacheable", value = "false")})
    @Query("SELECT u FROM User u WHERE u.email = ?1")
    User findByEmail(String email);

    // Example 3: Native Query with Hints
    @QueryHints({@QueryHint(name = "javax.persistence.query.timeout", value = "1000")})
    @Query(value = "SELECT * FROM user WHERE name = ?1", nativeQuery = true)
    List<User> findByNameNative(String name);
}

Common Query Hints
Hibernate-Specific Hints
org.hibernate.readOnly: Marks the query result as read-only (e.g., "true").
org.hibernate.cacheable: Enables/disables second-level caching for the query (e.g., "true" or "false").
org.hibernate.fetchSize: Specifies the number of rows fetched at a time.
JPA Standard Hints
javax.persistence.query.timeout: Sets a timeout for the query in milliseconds.
javax.persistence.lock.timeout: Sets a lock timeout.
javax.persistence.fetchgraph or javax.persistence.loadgraph: Optimizes entity loading with Entity Graphs.

When working with objects in Java, it’s often necessary to create copies of objects. However, simply copying the reference of an object can lead to unintended side effects. 
This is where the concepts of cloning, shallow copy, and deep copy become crucial. 

The Cloneable Interface

The Cloneable interface in Java is a marker interface. This means it doesn’t contain any methods but serves as a signal to the Java runtime that the class allows for 
field-for-field copying of instances. When a class implements Cloneable, it indicates that it supports the creation of a copy through the clone() method.

Implementing Cloning

To implement cloning:
1. Implement the Cloneable interface in your class.
2. Override the clone() method from the Object class.
3. Call super.clone() inside the clone() method.

Shallow Copy
A shallow copy of an object copies all the member fields, but it does not copy objects that the fields refer to. Instead, the references to these objects are copied, meaning both 
the original and the copied object refer to the same objects.

In Java, the clone() method creates a shallow copy by default.
Example of Shallow Copy

Consider a Person class that has a reference to an Address class:

class Address {
    String city;
    Address(String city) {
        this.city = city;
    }
}
class Person implements Cloneable {
    String name;
    int age;
    Address address;
    Person(String name, int age, Address address) {
        this.name = name;
        this.age = age;
        this.address = address;
    }
    @Override
    protected Object clone() throws CloneNotSupportedException {
        return super.clone();
    }
    @Override
    public String toString() {
        return "Person{name='" + name + "', age=" + age + ", address=" + address.city + "}";
    }
    public static void main(String[] args) {
        try {
            Address address = new Address("New York");
            Person original = new Person("Alice", 30, address);
            Person cloned = (Person) original.clone();
            System.out.println("Original: " + original);
            System.out.println("Cloned: " + cloned);
            // Modify the address in the cloned object
            cloned.address.city = "Los Angeles";
            
            System.out.println("After modifying cloned object:");
            System.out.println("Original: " + original);
            System.out.println("Cloned: " + cloned);
        } catch (CloneNotSupportedException e) {
            e.printStackTrace();
        }
    }
}
Output:

Original: Person{name='Alice', age=30, address=New York}
Cloned: Person{name='Alice', age=30, address=New York}
After modifying cloned object:
Original: Person{name='Alice', age=30, address=Los Angeles}
Cloned: Person{name='Alice', age=30, address=Los Angeles}
As seen in the output, modifying the address field in the cloned object also affects the original object because both the original and the clone share the same Address object.

In Java, all classes implicitly extend the Object class unless explicitly specified otherwise. This is because Object is the root class of the Java class hierarchy.

Deep Copy
A deep copy, on the other hand, duplicates everything directly or indirectly referenced by the fields in the original object. This means the cloned object does not share 
references with the original object; it creates a completely independent copy.

Example of Deep Copy

To implement a deep copy, you need to clone each field that is an object:

class Address implements Cloneable {
    String city;
    Address(String city) {
        this.city = city;
    }
    @Override
    protected Object clone() throws CloneNotSupportedException {
        return super.clone();
    }
}
class Person implements Cloneable {
    String name;
    int age;
    Address address;
    Person(String name, int age, Address address) {
        this.name = name;
        this.age = age;
        this.address = address;
    }
    @Override
    protected Object clone() throws CloneNotSupportedException {
        Person cloned = (Person) super.clone();
        cloned.address = (Address) address.clone(); // Deep copy of address
        return cloned;
    }
    @Override
    public String toString() {
        return "Person{name='" + name + "', age=" + age + ", address=" + address.city + "}";
    }
    public static void main(String[] args) {
        try {
            Address address = new Address("New York");
            Person original = new Person("Alice", 30, address);
            Person cloned = (Person) original.clone();
            System.out.println("Original: " + original);
            System.out.println("Cloned: " + cloned);
            // Modify the address in the cloned object
            cloned.address.city = "Los Angeles";
            
            System.out.println("After modifying cloned object:");
            System.out.println("Original: " + original);
            System.out.println("Cloned: " + cloned);
        } catch (CloneNotSupportedException e) {
            e.printStackTrace();
        }
    }
}
Output:

Original: Person{name='Alice', age=30, address=New York}
Cloned: Person{name='Alice', age=30, address=New York}
After modifying cloned object:
Original: Person{name='Alice', age=30, address=New York}
Cloned: Person{name='Alice', age=30, address=Los Angeles}
In this case, modifying the address field in the cloned object does not affect the original object because both the original and the clone have their own separate Address objects.

What is a Spring Bean?
A Spring bean is an object that is managed by the Spring IoC container. Beans are the building blocks of a Spring application, and they are instantiated, configured, and wired 
together by the container.For example, VideoController is a Spring bean because it is managed by Spring.

@RestController
public class VideoController {



    @Autowired
    private VideoRepository videoRepository; // we are injecting field injection
    
    public String next(){
        return "next video";
    }
}

What is applicationContext?
applicationContext is the central interface to the Spring IoC container. It is responsible for instantiating, configuring, and assembling the beans in the application. 
It acts as a container that holds all the beans and provides various functionalities such as dependency injection, event propagation, and resource loading.

What is Dependency Injection?
Dependency Injection is a technique where an object receives its dependencies from an external source rather than creating them itself.

What is IoC in Spring?
Inversion of Control (IoC) is a principle where the control of creating and managing objects is transferred from the application code to the framework. In Spring, IoC is 
achieved through the Spring IoC container, which manages the beans and their dependencies.

Constructor-based or setter-based DI?

Since you can mix constructor-based and setter-based DI, it is a good rule of thumb to use constructors for mandatory dependencies and setter methods or configuration methods 
for optional dependencies. Note that use of the @Autowired annotation on a setter method can be used to make the property be a required dependency; however, constructor injection 
with programmatic validation of arguments is preferable.

The Spring team generally advocates constructor injection, as it lets you implement application components as immutable objects and ensures that required dependencies are not null. 
Furthermore, constructor-injected components are always returned to the client (calling) code in a fully initialized state. As a side note, a large number of constructor arguments 
is a bad code smell, implying that the class likely has too many responsibilities and should be refactored to better address proper separation of concerns.

Constructor based DI - 
You can use Lombok’s @RequiredArgsConstructor Annotation for reducing more boilerplate Code.

package com.springTalk.Spring.talk.Controller;

import com.springTalk.Spring.talk.Repository.VideoRepository;
import lombok.RequiredArgsConstructor;
import org.springframework.web.bind.annotation.RestController;

@RestController
@RequiredArgsConstructor
public class VideoController {

    private final VideoRepository videoRepository; 

    public String next(){
        return "next video";
    }
}

The Java Program Execution Process
When you write and run a Java program, several steps occur:

Write the Code: Create a .java file using any text editor or IDE.
Compile the Code: Use the javac compiler to convert the source code into bytecode, which is saved as a .class file.
Run the Program: The JVM interprets the bytecode and executes the program.
# Command to compile
javac HelloWorld.java

# Command to run
java HelloWorld
Understanding classpath and path in Java Development
A. classpath: Locating Class Files
The classpath environment variable tells the Java compiler and JVM where to find the required .class files (compiled bytecode). It’s a directory path or a set of directories 
where Java looks for classes and packages during compilation and runtime.

Key Points:

Purpose: Specifies the location of user-defined and third-party class files.
Usage: Both the Java Compiler (javac) and the Java Runtime (java) use classpath to locate necessary classes.
Importance: If classpath is not set correctly, your program might fail to compile or throw a ClassNotFoundException at runtime.

Default Behavior:

If classpath is not explicitly set, Java defaults to the current directory (.).
Example:

To set the classpath:

set CLASSPATH=C:\myproject\classes;C:\libraries\mylib.jar

B. path: Locating Java Tools
The path environment variable points to directories containing executable files, such as javac (Java Compiler) and java (Java Runtime).

Key Points:

Purpose: Ensures that command-line tools like javac and java are accessible from any directory in the terminal or command prompt.
Usage: Without setting path, you would need to specify the full path of the executable every time you run a Java command.
Importance: If the path is not configured, commands like javac and java will not be recognized, resulting in errors such as "javac is not recognized as an internal or external 
command".

Example:

To set the path variable:

set PATH=C:\Program Files\Java\jdk-17\bin;

JDK vs. JRE vs. JVM
When working with Java, it’s important to understand the roles of the JDK (Java Development Kit), JRE (Java Runtime Environment), and JVM (Java Virtual Machine). These components 
form the foundation of the Java ecosystem, each serving a specific purpose in the development and execution of Java applications.

1. Java Development Kit (JDK)
The JDK is a comprehensive package that provides the necessary tools for both developing and running Java applications.

Purpose:
It includes tools such as the compiler (javac), debugger, and other utilities for building Java applications.
Also contains the JRE to run Java programs.
Components:
JRE: To provide the runtime environment.
Development Tools: Tools like the compiler, debugger, and other utilities necessary for development.
Key Point:
JDK = JRE + Development Tools

2. Java Runtime Environment (JRE)
The JRE provides the environment required to execute Java applications.

Purpose:
It includes the libraries, JVM, and other components necessary to run Java programs.
Components:
JVM: To execute Java bytecode.
Library Classes: Core libraries like java.lang, java.util, etc., required for program execution.
Key Point:
JRE = JVM + Library Classes

3. Java Virtual Machine (JVM)
The JVM is a virtual machine that interprets and executes Java bytecode.

Purpose:
It acts as an interpreter, executing Java programs line by line.
It abstracts the underlying hardware and operating system, providing Java’s platform independence.
Key Responsibilities:
Class Loading: Loads .class files.
Bytecode Verification: Ensures code integrity and security.
Execution: Executes bytecode using the interpreter and JIT compiler.

JAR vs. WAR vs. EAR
Java provides different types of archive files for packaging applications: JAR, WAR, and EAR. Each serves a specific purpose in organizing and deploying Java applications, 
from simple utilities to complex enterprise systems.

1. JAR (Java Archive)
Purpose:
A JAR file bundles multiple .class files and associated resources (such as images, text files, or configuration files) into a single archive.
Typically used for packaging Java libraries or standalone applications.
Key Features:
Compressed for efficient storage and distribution.
Can include a MANIFEST.MF file to specify metadata like the main class for execution.
Example:

jar cf MyApp.jar com/myapp/*.class
Use Case: Distributing reusable Java libraries or running standalone applications.

2. WAR (Web Archive)
Purpose:
A WAR file packages an entire web application, including:Servlets,JSP (Java Server Pages),HTML/CSS/JS files,Configuration files like web.xml
Key Features:
Simplifies the deployment of web applications to a Java EE-compliant web server (e.g., Apache Tomcat, Jetty).
Improves project management by bundling all necessary files into a single deployable unit.
Example Structure:

MyApp.war
├── WEB-INF/
│   ├── web.xml
│   ├── classes/ (compiled servlets)
│   └── lib/ (JAR files for dependencies)
├── index.html
├── style.css
└── script.js
Use Case: Deploying dynamic web applications on a server.

3. EAR (Enterprise Archive)
Purpose:
An EAR file packages an entire enterprise application and is used in large-scale, distributed systems.
Contains multiple WAR and JAR files along with EJB (Enterprise Java Beans), JMS (Java Message Service), and other resources.
Key Features:
Designed for deployment on Java EE application servers (e.g., JBoss, WebLogic, GlassFish).
Supports complex enterprise applications with multiple tiers and components.
Example Structure:

MyEnterpriseApp.ear
├── MyWebApp.war
├── MyLibrary.jar
├── MyEJBModule.jar
└── META-INF/
    └── application.xml
Use Case: Deploying large, multi-module enterprise applications.

In Java applications, mapping between objects (DTOs, entities, and domain models) is a common task, particularly in layered architectures. While manual mapping can be tedious 
and error-prone, libraries like MapStruct simplify the process by automating object mapping at compile time.

What is MapStruct?
MapStruct is a Java library for object mapping that generates code during the compilation phase. It provides a seamless and efficient way to map objects without runtime overhead.

Define a Mapper Interface
Create a mapper interface and annotate it with @Mapper

import org.mapstruct.Mapper;
import org.mapstruct.Mapping;

@Mapper
public interface UserMapper {
    
    @Mapping(source = "email", target = "email")
    @Mapping(source = "firstName", target = "fullName")
    UserDTO toDTO(User user);
    
    @Mapping(source = "email", target = "email")
    @Mapping(source = "fullName", target = "firstName")
    User toEntity(UserDTO userDTO);
}

Use the Mapper
MapStruct generates the implementation during compilation. Use the mapper as follows:

public class Main {
    public static void main(String[] args) {
        User user = new User();
        user.setFirstName("John");
        user.setLastName("Doe");
        user.setEmail("john.doe@example.com");

        UserMapper mapper = Mappers.getMapper(UserMapper.class);
        UserDTO userDTO = mapper.toDTO(user);

        System.out.println("Mapped DTO: " + userDTO.getFullName());
    }
}

 s.split(" "); or s.split("\\s+");
 
 java.util.Date: Represents both date and time information.
 java.sql.Date: Represents only the date (year, month, and day) without time information.

 1. Definition
Synchronized Collections:

Collections from the java.util package that are made thread-safe by wrapping them using the Collections.synchronizedXXX methods.
Example: Collections.synchronizedList, Collections.synchronizedMap.
Concurrent Collections:

Collections from the java.util.concurrent package designed specifically for concurrent access.
They provide better scalability and performance than synchronized collections in multi-threaded scenarios.
Example: ConcurrentHashMap, CopyOnWriteArrayList.
2. Thread Safety Mechanism
Synchronized Collections:

Use synchronization (intrinsic locks) to make methods thread-safe.
Entire methods are synchronized, which can lead to contention and reduced performance.
Example:
java
Copy code
List<String> list = Collections.synchronizedList(new ArrayList<>());
synchronized (list) { 
    for (String item : list) { 
        System.out.println(item); 
    }
}
Concurrent Collections:

Use advanced techniques like lock-free algorithms, segment-level locking, or copy-on-write mechanisms to ensure thread safety.
They allow concurrent reads and writes with minimal contention.
Example:
java
Copy code
ConcurrentHashMap<String, String> map = new ConcurrentHashMap<>();
map.put("key", "value");
3. Performance
Synchronized Collections:

Slower because they block the entire collection or critical sections during operations.
Increased contention in high-concurrency environments.
Concurrent Collections:

Better performance in multi-threaded environments due to fine-grained locking or lock-free operations.
Allow multiple threads to access the collection concurrently without significant bottlenecks.
4. Iterators
Synchronized Collections:

Iterators are not fail-safe. If a thread modifies the collection while another thread is iterating, a ConcurrentModificationException might occur.
Requires external synchronization during iteration:
java
Copy code
List<String> list = Collections.synchronizedList(new ArrayList<>());
synchronized (list) { 
    for (String item : list) { 
        System.out.println(item); 
    }
}
Concurrent Collections:

Iterators are fail-safe or weakly consistent.
They do not throw ConcurrentModificationException and reflect the state of the collection at the time of iteration.
java
Copy code
ConcurrentHashMap<Integer, String> map = new ConcurrentHashMap<>();
for (Map.Entry<Integer, String> entry : map.entrySet()) {
    System.out.println(entry.getKey() + " : " + entry.getValue());
}
5. Examples
Synchronized Collections:

Collections.synchronizedList, Collections.synchronizedMap, Vector, Hashtable.
Concurrent Collections:

ConcurrentHashMap, ConcurrentLinkedQueue, CopyOnWriteArrayList, CopyOnWriteArraySet.
6. When to Use
Synchronized Collections:

Use in low-concurrency environments or when legacy code requires synchronized collections.
Example: When maintaining backward compatibility with pre-Java 1.5 code.
Concurrent Collections:

Use in high-concurrency environments for better performance and scalability.
Example: Modern, high-performance multi-threaded applications.

save() - inserts the record to database and generates the new unique identifier and return it.
saveOrUpdate() - save if no record is present and if present then updates the existing one.
The persist method is intended to add a new entity instance to the persistence context, i.e. transitioning an instance from a transient to persistent state.

We usually call it when we want to add a record to the database (persist an entity instance):

Person person = new Person();
person.setName("John");
session.persist(person);
What happens after we call the persist method? The person object has transitioned from a transient to persistent state. The object is in the persistence context now, but not yet 
saved to the database. The generation of INSERT statements will occur only upon committing the transaction, or flushing or closing the session.


Any entity instance in our application appears in one of the three main states in relation to the Session persistence context:

transient — This instance isn’t, and never was, attached to a Session. This instance has no corresponding rows in the database; it’s usually just a new object that we created to 
save to the database.
persistent — This instance is associated with a unique Session object. Upon flushing the Session to the database, this entity is guaranteed to have a corresponding consistent 
record in the database.
detached — This instance was once attached to a Session (in a persistent state), but now it’s not. An instance enters this state if we evict it from the context, clear or close 
the Session, or put the instance through serialization/deserialization process.

The given code snippet will not compile because the lambda expression x -> x + 1 does not produce a valid operation for the forEach method in a Stream.

Explanation:
list.stream().forEach(x -> x + 1);

The forEach method expects a Consumer, which is a functional interface representing an operation that accepts a single input and returns nothing (void).
The lambda expression x -> x + 1 is a computation that adds 1 to each element but does not perform any side effect like printing or modifying a collection. This violates the 
contract of Consumer.
Correct Usage of forEach: If you want to print the elements after incrementing them, you should write:

java
Copy code
list.stream().forEach(x -> System.out.println(x + 1));

To set the name of a bean after its instantiation in Spring, you can use the @BeanNameAware interface. This interface provides a method called setBeanName(String name) that Spring 
calls after the bean is instantiated but before its initialization phase.

Steps to Set the Name of a Bean
Implement the BeanNameAware Interface:

This allows you to capture the name of the bean defined in the Spring container.
Use the setBeanName Method:

Inside this method, you can set or log the bean name.
Example: Setting Bean Name After Instantiation
Java Class
java
Copy code
import org.springframework.beans.factory.BeanNameAware;

public class MyBean implements BeanNameAware {

    private String beanName;

    @Override
    public void setBeanName(String name) {
        this.beanName = name; // Capture the bean name
        System.out.println("Bean name set: " + name);
    }

    // Getter for demonstration
    public String getBeanName() {
        return beanName;
    }
}

The BeanFactory in Spring is a core interface of the Spring Framework's IoC (Inversion of Control) container. It is responsible for managing the lifecycle of beans, including 
their instantiation, configuration, and destruction.

Key Features of BeanFactory
Lightweight Container:

It provides basic functionality for managing and accessing beans.
Suitable for simple applications where advanced features are not required.
Lazy Initialization:

Beans are created only when they are requested, reducing the startup time of the application.
Core Interface:

It is the parent interface of more advanced containers like ApplicationContext.
Commonly Used Methods in BeanFactory
Method	Description
Object getBean(String name)	Retrieves a bean by its name.
Object getBean(Class<T> c)	Retrieves a bean by its type.
boolean containsBean(String name)	Checks if a bean with the given name exists in the container.
boolean isSingleton(String name)	Checks if a bean is a singleton.
boolean isPrototype(String name)	Checks if a bean is a prototype.
BeanFactory vs ApplicationContext
Feature	BeanFactory	ApplicationContext
Bean Initialization	Lazy by default	Eager by default (unless specified lazy)
Advanced Features	Basic IoC container	Includes features like event propagation, internationalization, etc.
Annotation Support	Limited	Full annotation and AOP support
Preloading Beans	No (on-demand creation)	Yes (all beans preloaded by default)

Spring bean lifecycle starts with initialising the bean and then injecting dependencies nad then firing init() method and then our utility method and after that the bean is 
destroyed using destroy() method and init and destroy method is implemented by implementing InitializingBean and DisposableBean. 

Cache is a temporary storage area that stores frequently accessed data or resources in order to speed up subsequent access to them. It typically stores data in the form of 
key-value pairs. Each piece of data or resource that is stored in the cache is associated with a unique key, which is used to quickly retrieve the data when it is needed.

Spring Boot caching is based on the concept of annotations. By adding annotations to the methods in your application that need to be cached, you can define the caching behaviour, 
such as the cache name, the key used to identify the data being cached, and the caching strategy.

The caching annotations in Spring Boot include:

@Cacheable: Indicates that the result of invoking a method should be cached. If the same method is called again with the same arguments, the cached value is returned instead of 
invoking the method again.
@CachePut: Indicates that the result of invoking a method should be cached, but the method should always be invoked. Here the Cache Value for a particular key will be updated with 
the new value returned from the invoked method.
@CacheEvict: Indicates that the cache entries associated with the method should be removed from the cache.

Use @Cacheable when the result is consistent for the same inputs and you want to avoid redundant computations.
Use @CachePut when the result changes frequently and you need the cache to reflect the latest value after every method execution.

@CachePut ensures that after a method modifies or generates a new result, this result is always written back to the cache. This is essential for use cases where the data changes 
frequently, and you want subsequent reads to fetch the updated value from the cache instead of outdated or invalid data.

Example:
Consider a scenario where you update a user's profile:

java
Copy code
@CachePut(value = "users", key = "#user.id")
public User updateUser(User user) {
    return userRepository.save(user); // Always saves to the database
}
The method always updates the cache with the latest user details after updating the database.
Benefit: Subsequent @Cacheable methods fetching this user's details will get the updated value directly from the cache, avoiding an expensive database call.

Most caching providers offer mechanisms for cache eviction and expiration, which are critical for managing memory and ensuring data freshness:

Eviction: Removes items based on a policy (e.g., least recently used, least frequently used).
Expiration: Removes items after a set time-to-live (TTL).
These behaviors are configured at the caching provider level.

Memcached is a popular distributed memory caching implementation often used in performance critical services.
Memcached was improved by and used by most of the modern day system. Eg: Facebook, Youtube, Twitter

Redis is a NoSQL database, so it doesn’t have any tables, rows, or columns. Also, it doesn’t allow statements like select, insert, update, or delete. Instead, Redis uses data 
structures to store data. As a result, it can serve frequently requested items with sub-millisecond response times and allow easy scaling for larger workloads without increasing 
the cost of a more expensive back-end system.

Remote Dictionary Server, aka Redis, an in-memory data store, is one of the many options for implementing caching in Spring Boot applications due to its speed, versatility, and 
simplicity of use. It is a versatile key-value store that supports several data structures, such as Strings, Sorted Sets, Hashes, Lists, Streams, Bitmaps, Sets, etc., because it 
is a NoSQL database and doesn’t need a predetermined schema.

Time To Live(TTL): A good practice for caching is to ensure that excess and redundant data is not accumulated indefinitely, as this can result in stale or outdated data being 
served to users. To serve this, we can take advantage of the time-to-live property, which is an optional setting that allows us to set the expiration time for cached data. After 
the specified time has elapsed, the cached entry is automatically removed from the cache. This makes space for new data to be fetched and stored in the cache the next time it’s 
requested. If no value is assigned to the property, it becomes -1 by default, which means the data will stay in the cache indefinitely.

The value attribute establishes a cache with a specific name, while the key attribute permits the use of Spring Expression Language to compute the key dynamically. Consequently, 
the method result is stored in the ‘product’ cache, where respective ‘product_id’ serves as the unique key. This approach optimizes caching by associating each result with a 
distinct key.

The cacheNames attribute is an alias for value, and can be used in a similar manner.

@Caching is used for multiple nested caching on the same method.

@PutMapping("/{id}")
@Caching(
     evict = {@CacheEvict(value = "productList", allEntries = true)},
     put = {@CachePut(value = "product", key = "#id")}
)
public Product editProduct(@PathVariable long id, @RequestBody Product product)

Conditional caching allows us to cache specific data using the unless and conditional attributes.

@GetMapping("/product/{id}")  
@Cacheable(value = "product", key = "#id" , unless = "#result.price > 1000")
public Product getProductById(@PathVariable long id) {...}

@PutMapping("/{id}")
@Caching( value = "product", condition = "#product.name == "Fridge""))
public Product editProduct(@PathVariable long id, @RequestBody Product product) {...}

Cache is a part of temporary memory (RAM). It lies between the application and the persistent database.

In-memory caching is a technique which is widely used. In this type of caching, data is stored in RAM. Memcached and Redis are examples of in-memory caching.

Memcached is a simple in-memory cache while Redis is advanced.

cacheManager :
It specifies the name of cache manager. It is used define your own cache manager and do not want to use spring’s default cache manager.

For example,

@Cacheable(value=”employees”, cacheManager=”customCacheManager”)
public Employee findByName(String name) {
  // some code
}

EhCache
The EhCache is an open source Java based cache used to boost performance. It stores the cache in memory and disk (SSD).

EhCache used a file called ehcache.xml. The EhCacheCacheManager is automatically configured if the application found the file on the classpath.

If we want to use EhCache then we need to add the following dependency :

<dependency>
  <groupId>org.ehcache</groupId>
  <artifactId>ehcache</artifactId>
</dependency>

HazelCast
The Hazelcast is a distributed in-memory data grid structure. It distributes the data equally among all the nodes. We can configure Hazelcast by using following property :

spring.hazelcast.config=classpath:config/demo-config.xml
If we want to use Hazelcast then we need to add the following dependency :

<dependency>
  <groupId>com.hazelcast</groupId>
  <artifactId>hazelcast</artifactId>
</dependency>

Cache managers in Spring Boot act as the backbone of the caching mechanism, managing the creation, access, and lifecycle of cache instances. Spring Boot supports a variety of 
cache managers, each suited to different application needs and environments. The ConcurrentMapCacheManager is often used for lightweight, in-memory caching suitable for development 
or standalone applications. For stronger needs, such as disk-based caching or distributed environments, EhCacheCacheManager or RedisCacheManager can be employed. The choice of 
cache manager greatly depends on the application's specific requirements for performance, scalability, and persistence.

In clustered environments, where applications are deployed across multiple nodes, maintaining cache consistency and scalability becomes crucial. Spring Boot can integrate with 
distributed caching solutions like Redis, Hazelcast, or Apache Geode to achieve this. These distributed caches provide a shared cache space accessible by all application instances, 
ensuring that cache updates are propagated across the cluster.

An example of configuring a Redis cache in Spring Boot:

@Bean
public RedisCacheManager cacheManager(RedisConnectionFactory connectionFactory) {
    return RedisCacheManager.builder(connectionFactory).build();
}
This configuration sets up Redis as the caching solution, ensuring that cache data is consistent across application instances in a clustered deployment.

Redis is an excellent choice for applications that require low latency and high throughput. It is highly scalable and can handle millions of requests per second, making it ideal 
for applications with heavy read and write loads. Redis also supports a wide range of data structures, including strings, hashes, lists, sets, and sorted sets, making it suitable 
for a variety of use cases.

Hazelcast, on the other hand, is ideal for applications that require automatic failover and replication. It provides a distributed in-memory data grid that automatically replicates 
data across multiple nodes, ensuring high availability even in the event of a node failure. Hazelcast also supports distributed caching, which allows applications to cache data 
across multiple nodes, improving performance and reducing latency.

Summary Table  load factor is same ie 0.75 ie on reaching load factor size increases
Collection	Default Initial Capacity	Resize Behavior
ArrayList		10							Grows by 50% of the current size
LinkedList		No fixed capacity			Dynamically grows
HashSet			16							Doubles when load factor exceeded
LinkedHashSet		16						Doubles when load factor exceeded
TreeSet			No fixed capacity			Dynamically grows (balanced tree)
HashMap			16								Doubles when load factor exceeded
LinkedHashMap		16						Doubles when load factor exceeded
TreeMap			No fixed capacity				Dynamically grows (balanced tree)
PriorityQueue		11						Dynamically grows
ArrayDeque		16							Doubles during resizing

What will be the output of the following Java code?

public class Main {
    public static void main(String[] args) {
        String s1 = "Java";
        String s2 = "Ja" + "va";
        System.out.println(s1 == s2);
    }
}
Output: true

Explanation:

Both s1 and s2 refer to the same string literal "Java" in the string pool, so == compares their references, which are identical.

@SuperBuilder is specifically designed for inheritance, while @Builder does not handle inherited fields in subclasses directly. In a hierarchy, if you only use @Builder, each 
subclass would only include its own fields in the builder, leading to complications if you need to initialize fields from parent classes as well.

@SuperBuilder is extremely useful when you need to create immutable and complex objects across an inheritance hierarchy in Java. It simplifies the builder pattern, maintains type 
safety, and supports inheritance, making code easier to maintain and more readable.

String str = "I like Marvel Movies";
boolean doesContainsWhiteSpace = StringUtils.containsWhitespace(str);
Here doesContainsWhiteSpace will be true as the string contains white spaces.

We can remove all the white space using another StringUtils method as below:

String newString = StringUtils.trimAllWhitespace(str);
The value of newString in the above case will be IlikeMarvelMovies

Validation for GET Request
Add @Validated to the Class or Method
@RestController
@Validated
public class WishListController {

    @GetMapping("/api/wish")
    public String getWishes(
            @RequestParam @NotBlank(message = "Category must not be blank") String category) {
        return String.format("Fetching wishes in category: %s", category);
    }
}

@RestController
@Validated
public class WishListController {

    @GetMapping("/api/wish/{year}")
    public String getWishesByTargetYear(
            @PathVariable @NotBlank(message = "Year must not be blank")
            @Size(min = 2, max = 4, message = "Year must be between 2 and 4 characters")
            String year) {
        return String.format("Fetching wishes for year : %s", year);
    }

}

RestTemplate is a synchronous, blocking client provided by Spring Framework for consuming RESTful web services. It executes requests and waits until the response is returned. 
While simple and widely used, its blocking nature makes it less suitable for high-throughput or low-latency applications.

WebClient is a non-blocking, reactive web client introduced as part of the Spring WebFlux framework. It is built to support asynchronous and streaming scenarios, making it ideal 
for applications requiring high concurrency and scalability.

We know that although many interview questions state there are 3, 4, or more ways to create a thread in Java, the only real way to create a thread is by using new Thread().start();

【Code Example 1】

public class Test {
    public static void main(String[] args) {
        Thread thread = new Thread(() -> {});
        System.out.println(thread.getName()+":"+thread.getState());
        thread.start();
        System.out.println(thread.getName()+":"+thread.getState());
    }
}
Output:

Thread-0:NEW
Thread-0:RUNNABLE
When a Thread is created, it is in the NEW state. Calling the start() method moves the thread to the RUNNABLE state.

【Code Example 2】

public class Test {
    public static void main(String[] args) {
        Thread thread = new Thread(() -> {});
        System.out.println(thread.getName()+":"+thread.getState());
        // First call to start
        thread.start();
        System.out.println(thread.getName()+":"+thread.getState());
        // Second call to start
        thread.start();
        System.out.println(thread.getName()+":"+thread.getState());
    }
}
Output:

Thread-0:NEW
Thread-0:RUNNABLE
Exception in thread "main" java.lang.IllegalThreadStateException
    at java.lang.Thread.start(Thread.java:708)
    at com.javabuild.server.pojo.Test.main(Test.java:17)
Calling start a second time throws an IllegalThreadStateException.

At this point, threadStatus is 0, indicating the thread is in the NEW state. Continuing with the breakpoint, when it reaches the native method start0(), threadStatus becomes 5, 
indicating the thread is in the RUNNABLE state. Now, let's step through the second start call.
At this time, threadStatus is 5, which satisfies the condition of not being equal to 0, resulting in an IllegalThreadStateException being thrown!
when Thread.start() method is invoked thread status internally is set to 5 and if again found this status as 5 it will throw IllegalThreadStateException.
For a thread in the TERMINATED state, the situation is similar to that of a RUNNABLE thread!

PostgREST

Find the maximum value in the List<Integer>.
List<Integer> numbers = Arrays.asList(1, 2, 3, 4, 5);
int max = numbers.stream()
                 .max(Integer::compareTo)
// Above code uses method referene, 
// Below commented code uses lamnda expression.
// numbers.stream().max((a, b) -> a.compareTo(b));
System.out.println(max); // Output: 5
2. Given List<Employee>, Find employee with higest salary.

List<Employee> employees = Arrays.asList(
            new Employee("John", 50000),
            new Employee("Jane", 60000),
            new Employee("Mark", 55000),
            new Employee("Sophia", 75000),
            new Employee("Alex", 40000)
        );

Optional<Employee> highestSalaryEmployee = employees.stream()
            .max(Comparator.comparingDouble(Employee::getSalary));
highestSalaryEmployee.ifPresent(System.out::println);
// Output
// Employee{name='Sophia', salary=75000.0}
3. Given List<String>, Return String concatenating all Strings of List.

List<String> words = Arrays.asList("apple", "banana", "cherry");
String result = words.stream()
                     .collect(Collectors.joining(", "));
System.out.println(result); 
// Output: apple, banana, cherry
4. Group List<Integer> by their evenness. (even or odd)

List<Integer> numbers = Arrays.asList(1, 2, 3, 4, 5, 6);
Map<Boolean, List<Integer>> grouped = numbers.stream()
                                             .collect(Collectors.groupingBy(n -> n % 2 == 0));
System.out.println(grouped); 
// Output: {false=[1, 3, 5], true=[2, 4, 6]}
Collectors.groupingBy: This is a collector that groups the elements of the stream based on a classifier function.

Classifier Function: The lambda expression n -> n % 2 == 0 checks if a number is even. It returns true for even numbers and false for odd numbers.

As a result, the stream will be split into two groups: one for numbers where the condition is true (even numbers) and another for numbers where the condition is false (odd numbers).

5. Given List<Employee>, Group them by Designation.

List<Employee> employees = Arrays.asList(
            new Employee("Alice", "Developer"),
            new Employee("Bob", "Manager"),
            new Employee("Charlie", "Developer"),
            new Employee("David", "Manager"),
            new Employee("Eve", "Designer")
        );

Map<String, List<Employee>> groupedByDesignation = employees.stream()
            .collect(Collectors.groupingBy(Employee::getDesignation));

// Output
// Developer: [Employee{name='Alice', designation='Developer'}, Employee{name='Charlie', designation='Developer'}]
// Manager: [Employee{name='Bob', designation='Manager'}, Employee{name='David', designation='Manager'}]
// Designer: [Employee{name='Eve', designation='Designer'}]
6. Given List<String>, Count the number of Strings that have length greater than 4

List<String> words = Arrays.asList("apple", "banana", "kiwi", "cherry");
long count = words.stream()
                  .filter(w -> w.length() > 4)
                  .count();
System.out.println(count); 
// Output: 3
7. Sort a List<String> in reverse alphabetical order.

List<String> words = Arrays.asList("apple", "banana", "cherry");
List<String> sortedWords = words.stream()
                                .sorted(Comparator.reverseOrder())
                                .collect(Collectors.toList());
System.out.println(sortedWords); 
// Output: [cherry, banana, apple]
8. Find all distinct elements.

List<Integer> numbers = Arrays.asList(1, 2, 2, 3, 4, 4, 5);
List<Integer> distinctNumbers = numbers.stream()
                                       .distinct()
                                       .collect(Collectors.toList());
System.out.println(distinctNumbers); // Output: [1, 2, 3, 4, 5]
9. Check if all elements in a List<Integer> are positive.

List<Integer> numbers = Arrays.asList(1, 2, 3, 4, 5);
boolean allPositive = numbers.stream()
                             .allMatch(n -> n > 0);
System.out.println(allPositive); 
// Output: true
allMatch(Predicate<T> predicate)

Purpose: Checks if all elements in the stream satisfy the given predicate.

Returns:

true if all elements match the predicate.

false if at least one element does not match the predicate or the stream is empty.

anyMatch(Predicate<T> predicate)

Purpose: Checks if at least one element in the stream matches the given predicate.

Returns:

true if at least one element matches the predicate.

false if no elements match the predicate or the stream is empty.

noneMatch(Predicate<T> predicate)

Purpose: Checks if no elements in the stream match the given predicate.

Returns:

true if no elements match the predicate.

false if at least one element matches the predicate or the stream is empty.

10. How to sort List<Employee> by salary


// Get a new sorted list by salary
 List<Employee> sortedEmployees = employees.stream()
                                           .sorted(Comparator.comparing(Employee::getSalary))
                                            .collect(Collectors.toList());
											
Given List<Employee>, Return Map with Employee Name and Salary

 Map<String, Double> employeeSalaryMap = employees.stream()
            .collect(Collectors.toMap(Employee::getName, Employee::getSalary);

GivenList<Employee> where each Employee has a List<String> of skills. Use flatMap to create a List<String> that contains all unique skills across all employees.

List<String> uniqueSkills = employees.stream()
    .flatMap(emp -> emp.getSkills().stream())
    .distinct()
    .collect(Collectors.toList());

Use flatMap to create a List<String> of all individual words from a list of sentences.

List<String> sentences = Arrays.asList("Hello world", "Java 8 Streams", "flatMap example");

List<String> words = sentences.stream()
    .flatMap(sentence -> Arrays.stream(sentence.split(" ")))
    .collect(Collectors.toList());

System.out.println(words); 
// Output: [Hello, world, Java, 8, Streams, flatMap, example]

Find the employee with the second highest salary from a List<Employee>

List<Employee> employees = Arrays.asList(
            new Employee("Alice", 70000),
            new Employee("Bob", 80000),
            new Employee("Charlie", 60000),
            new Employee("David", 90000),
            new Employee("Eve", 80000)
        );

Optional<Employee> secondHighest =  employees.stream()
            .sorted(Comparator.comparingDouble(Employee::getSalary).reversed())
            .distinct() // To avoid duplicates in case of equal salaries
            .skip(1) // Skip the highest salary
            .findFirst(); // Get the second highest salary
// Output
// Employee{name='Bob', salary=80000.0}

Scaling : 1 to 10 million User
Scaling from 1 to 10 million users requires optimizing infrastructure, improving application architecture, enhancing database scalability, and ensuring high availability. Key 
strategies include horizontal scaling, cloud-native solutions, efficient caching, and performance optimization. Continuous monitoring and iteration are essential to handle 
increased load and maintain a good user experience.

Points to remember :
Optimize Infrastructure: Use cloud services, auto-scaling, and load balancing.
Database Scaling: Implement sharding, replication, and caching to handle more data.
Improve Application Architecture: Adopt microservices, asynchronous processing, and horizontal scaling.
Enhance Network Capacity: Use CDNs and optimize data transfer.
Monitor Performance: Continuously monitor, set up alerts, and optimize bottlenecks.
Ensure High Availability: Implement redundancy, disaster recovery, and failover systems.
Focus on User Experience: Optimize for latency and handle traffic spikes effectively.
How do we do it ? Lets understand in details :
1. Optimize Infrastructure
Cloud Services: Move to a cloud provider (AWS, Azure, GCP) that offers scalable resources. These platforms allow you to dynamically scale compute, storage, and networking resources 
based on demand.
Auto-Scaling: Use cloud auto-scaling capabilities to automatically add or remove instances based on traffic. For example, AWS Auto Scaling or Kubernetes can automatically provision 
additional servers when needed.
Load Balancing: Distribute incoming user traffic across multiple servers using load balancers (e.g., AWS ELB, NGINX). This prevents any single server from becoming a bottleneck and 
ensures smooth user experiences during traffic spikes.



2. Database Scaling
Sharding: Split your database into smaller, manageable pieces (shards) to distribute data across multiple servers. For example, a user database could be sharded by user ID range or
 region.
Replication: Set up read replicas to offload read queries from the primary database. This improves performance by spreading the read traffic across multiple copies of the database.
Caching: Implement caching mechanisms (like Redis or Memcached) to store frequently accessed data in memory, reducing the load on databases and speeding up response times. Use CDNs
 for static content and in-memory caches for dynamic content.



3. Improve Application Architecture
Microservices: Break down your monolithic application into smaller, independent services that can be scaled separately. Each microservice can be deployed and scaled based on demand.

Asynchronous Processing: Offload time-consuming tasks (e.g., email sending, image processing) to background jobs or message queues (e.g., RabbitMQ, Kafka). This allows your main app
lication to handle more user requests concurrently without delays.
Horizontal Scaling: Instead of upgrading the power of a single server (vertical scaling), deploy more instances of your application across multiple machines. This allows the system 
to handle more users by distributing the load.



4. Enhance Network Capacity
Content Delivery Networks (CDNs): Use CDNs (e.g., Cloudflare, AWS CloudFront) to distribute static content like images, videos, CSS, and JavaScript files. CDNs cache content at edge
 locations near users, reducing latency and server load.
Optimize Data Transfer: Minimize large payloads by compressing data, optimizing APIs, and reducing unnecessary requests. Use HTTP/2 or gRPC for more efficient data transmission 
between the client and server.


5. Monitor Performance
Real-time Monitoring: Use monitoring tools (e.g., Datadog, Prometheus, New Relic) to track the performance of your system in real-time. Monitor CPU, memory, disk usage, database 
queries, response times, and error rates.
Set up Alerts: Configure alerts for critical metrics such as high CPU usage, slow queries, or an increase in error rates. This ensures you’re notified of potential issues before 
they affect users.
Load Testing: Simulate user load using tools like JMeter or Gatling to identify bottlenecks before actual user traffic increases. Test how your system performs under stress to make
adjustments.

6. Ensure High Availability
Redundancy: Deploy your application in multiple availability zones (or regions) to ensure that if one zone fails, the system can continue operating from another. Use cloud services like AWS Availability Zones or multi-region setups.
Disaster Recovery: Implement regular backups and automated failover systems to ensure that your system can recover from data loss or hardware failure quickly. Use tools like AWS RDS
 Multi-AZ or Google Cloud Spanner for automated failover and recovery.
Load Balancer Failover: Configure load balancers with health checks and automatic failover to ensure that traffic is always directed to healthy instances.

7. Focus on User Experience
Reduce Latency: Optimize backend APIs and database queries to minimize response times. Use techniques like lazy loading or pre-fetching content to improve perceived performance for 
users.
Handle Traffic Spikes: Use techniques like rate limiting, queuing, and graceful degradation to manage surges in traffic. For example, queue non-essential tasks for background 
processing or temporarily throttle less-critical user requests during high load periods.
Optimize Frontend: Minimize the size of assets (images, scripts, stylesheets) and use techniques like lazy loading, image optimization, and minification to ensure that pages load 
quickly. Tools like Webpack, ImageOptim, or Lighthouse can help automate frontend optimizations.
	
Can we declare the main method as final?
Yes, you can declare the main method as final. It will still be a valid entry point for the JVM.
The final keyword in Java is used to prevent method overriding. When applied to methods, it means that the method cannot be overridden in any subclass.
However, the main method is already static, and static methods belong to the class itself rather than to instances of the class. This means that static methods are not subject to 
method overriding in the first place.
public static final void main(String[] args) {
    System.out.println("Main with final modifier");
}

What happens if the main method is not static?
Answer: If the main method is not declared as static, the JVM will not be able to call it without creating an instance of the class.
This will result in a NoSuchMethodError when you try to run the program:
Exception in thread "main" java.lang.NoSuchMethodError: main

What happens if the main method is declared private?
Answer: If you declare the main method as private, the program will compile successfully, but at runtime, the JVM will throw an IllegalAccessError because it cannot access the private 
method:
Exception in thread "main" java.lang.IllegalAccessError: class Main

Can we run a Java program without the main method?
In modern versions of Java (from Java 7 onwards), it is mandatory to have a main method to run a Java program.
However, in earlier versions, you could use a static initializer block to execute code without a main method:

Can we change the parameter of the main method from String[] args to String args[]?
Yes, both String[] args and String args[] are valid in Java, and the JVM can interpret either form as an array of strings:

What happens if we pass null to the main method as an argument?
Answer: If null is passed to the main method as an argument, it will not cause a NullPointerException.
Instead, the args parameter will be null, and accessing its length or elements will throw a NullPointerException:
public static void main(String[] args) {
    System.out.println(args); // Prints: null
    System.out.println(args.length); // Throws NullPointerException
}

Static methods belong to the class, not to any instance, so they can’t access instance variables directly.
Instance variables are tied to specific objects, and each object has its own copy.
To access instance variables in a static method, create an instance of the class and use that instance to call non-static methods that access those variables.
Avoid calling static methods through an instance, as this can be misleading and makes your code harder to read.

In distributed systems, idempotency ensures that executing the same operation multiple times produces the same result as executing it once. For a payment system, this means:

If a user retries a payment due to a timeout or error, the system must not process the payment again.

Instead, it should recognize the retry and return the original response — whether success, failure, or pending.

Phase 1: Handling the Initial Request
The Client (e.g., Order Service or Refund Service) sends a payment request with a unique idempotency key to the Payment Service.

This key uniquely identifies the payment operation (e.g., order123-payment456).

2. The Payment Service checks its database for the idempotency key:

If the key exists:

It fetches the cached response (e.g., “Payment Success”) from the database and returns it to the client, preventing duplicate processing.

If the key doesn’t exist:

It creates a new record in the database to indicate the request is being processed.

3. The service acquires a row-level lock on the key to ensure no concurrent processes can modify or access it until the request is complete.

Phase 2: Communicating with the Payment Processor
The Payment Service constructs a request object, including the idempotency key, and sends it to the external Payment Processor (e.g., Stripe, PayPal, or a bank).

Many payment processors such as stripe and paypal support idempotency by requiring the idempotency key in the request headers or payload.

2. The processor returns a response (e.g., “Payment Successful” or “Card Declined”).

Phase 3: Finalizing the Payment
The Payment Service stores the processor’s response in the database:

For successful payments: The response is saved, marking the operation complete.

For retryable failures (e.g., temporary network errors): The client may retry later.

For non-retryable failures (e.g., “Insufficient Funds”): The response ensures the client doesn’t retry.

2. The row lock is released.

3. The service sends the response (success or failure) back to the Client.

Why is the Database Needed in This Flow?
The database ensures idempotency by storing the idempotency key to prevent duplicate processing. It tracks the state of requests (e.g., "in progress" or "completed"), provides 
concurrency control through row-level locks, and ensures reliability by persisting request data for safe recovery during failures. Without it, consistent and error-free payment 
processing would not be possible.

The main components of JPA include:
- Entity: A lightweight, persistent domain object.
- EntityManager: Manages lifecycle operations on entities.
- EntityManagerFactory: Factory for creating `EntityManager` instances.
- Persistence Unit: Defines all entity classes managed by an `EntityManager`.
- EntityTransaction: Manages transaction boundaries.
- Query: Interface for executing database queries.

- EntityManagerFactory: A factory for creating `EntityManager` instances. It is thread-safe and expensive to create, so it should be created once per application and reused.
- EntityManager: Manages the lifecycle of entity instances, performs CRUD operations, and executes queries. It is not thread-safe and should be used per transaction or per request.

A persistence unit is a logical grouping of related entity classes and their configurations, defined in the `persistence.xml` file. It specifies which entity classes are managed by 
an `EntityManager` and includes database connection details.

You define a primary key generation strategy using the `@GeneratedValue` annotation, specifying one of the following strategies:
- `GenerationType.AUTO`
- `GenerationType.IDENTITY`
- `GenerationType.SEQUENCE`
- `GenerationType.TABLE`

- AUTO: Lets the persistence provider choose the generation strategy.
- IDENTITY: Relies on the database’s identity column for primary key generation.
- SEQUENCE: Uses a database sequence to generate primary keys.
- TABLE: Uses a dedicated database table to generate primary keys.

The `EntityTransaction` interface manages transaction boundaries. It allows you to begin, commit, and rollback transactions to ensure data consistency and integrity.

JPQL (Java Persistence Query Language) is a query language for JPA, similar to SQL but operates on entity objects rather than database tables. It allows querying of entities using 
an object-oriented syntax.

In JPA, exceptions are handled using `try-catch` blocks. The `javax.persistence.PersistenceException` is the base class for all JPA-related exceptions.

The `Criteria API` is an alternative to JPQL for creating queries in a programmatic and type-safe manner. It uses a fluent API to construct queries dynamically.

@ConfigurationProperties is used with class with prefic attribute and the last attribute as fields and we can apply validations over it like @NotNull so nowadays it is preferred
over @Value annotation.

public class ListConcatenation {
    public static void main(String[] args) {
        List<String> heroList1 = Arrays.asList("Iron Man", "Thor", "Spider Man", "Deadpool", "Hulk");
        List<String> heroList2 = Arrays.asList("Wanda", "Ant Man", "Doctor Strange");

        List<String> allHeroes = Stream.concat(heroList1.stream(), heroList2.stream())
                .collect(Collectors.toList());
        System.out.println("All Heroes : " + allHeroes);

        // All Heroes : [Iron Man, Thor, Spider Man, Deadpool, Hulk, Wanda, Ant Man, Doctor Strange]

    }
}

public class JoinListElements {
    public static void main(String[] args) {
        List<String> heroes = Arrays.asList("Iron Man", "Thor", "Spider Man", "Deadpool", "Hulk");
        String commandSeparatedString = heroes.stream()
                .collect(Collectors.joining(", "));
        System.out.println("Final String: " + commandSeparatedString);

        // Final String: Iron Man, Thor, Spider Man, Deadpool, Hulk
    }
}

public class StreamFilter {
    public static void main(String[] args) {
        List<Integer> numList = Arrays.asList(1, 2, 3, 4, 5, 6, 7, 8, 9);
        int evenSum = numList.stream()
                .filter(n -> n%2 == 0)
                .mapToInt(Integer::intValue)
                .sum();
        System.out.println("Even Sum: " + evenSum);
        // Even Sum: 20
    }
}

Blue Green Deployment:
This deployment allows to run two identical environments. One represents the current version which is blue and the new version is called the green deployment. Blue-green deployment 
strategy increases application availability and simplify the rollback process. Once the testing has been done in the green environment the traffic is diverted to the green 
environment and the blue is destroyed.

Canary Deployment
Canary deployment incrementally deploys the new version making it visible to the traffic in a incremental fashion. It allows seamless rollback of the application if there is an 
error encountered during rolling over. Using this strategy we can monitor the performance and the user feedback in the new version before we roll out to a larger audience. It keeps 
the old stack alive up to 48 hours in case of rolling back to the previous version.

Some JVM flags to fine-tune as the use cases are:(Memory optimization)

-Xms: Initial Heap Size
-Xmx: Maximum Heap Size
-XX:+UseG1GC: Using G1 Garbage Collection for better performance
-XX:MaxMetaspaceSize: Limit metaspace memory

Implement Connection Pooling
Set appropriate Pool Sizes
Efficiently configure the Connection timeout and Idle time
Continuously Monitor Connection Pool metrics and make changes as required

Query Performance: Indexing speeds up data retrieval operations, particularly for SELECT queries with WHERE, JOIN, or ORDER BY clauses.
Write Performance Impact: While indexes make reads faster, they introduce some overhead for INSERT, UPDATE, and DELETE operations, as the index must also be updated when the table 
is modified.
Space Usage: Indexes consume additional storage space because they are separate data structures that store references to rows in the table.

Clustered Index
Only one clustered index can be there in a table
Sort the records and store them physically according to the order
Data retrieval is faster than non-clustered indexes
Do not need extra space to store logical structure

Non Clustered Index
There can be any number of non-clustered indexes in a table
Do not affect the physical order. Create a logical order for data rows and use pointers to physical data files
Data insertion/update is faster than clustered index
Use extra space to store logical structure

In Java programming language , class loading is the process by which the Java Virtual Machine (JVM) dynamically loads classes into memory at runtime.

The computeIfAbsent method checks whether a specified key exists in the map. If not, it computes the value using a provided mapping function, adds it to the map, and returns the 
computed value.

public class ComputeIfAbsentExample {
    public static void main(String[] args) {
        Map<String, Employee> employeeMap = new HashMap<>();

        // Fetch an employee with ID "E123". If not present, load and add to the map.
        Employee employee = employeeMap.computeIfAbsent("E123", id -> loadEmployeeById(id));

        System.out.println("Employee Details: " + employee);
    }

    private static Employee loadEmployeeById(String id) {
        System.out.println("Loading employee with ID: " + id);
        return new Employee(id, "Alice Johnson");
    }
}

Benefits of Using computeIfAbsent
Conciseness: Reduces boilerplate code by combining multiple operations into a single line.
Efficiency: The mappingFunction is called only if the key is absent.
Thread-Safe with ConcurrentHashMap: When used with ConcurrentHashMap, it ensures atomic operations, avoiding race conditions.

What is the single point of failure in your system design and what will you do to mitigate it?
Identifying and mitigating single points of failure (SPOF) is crucial in designing resilient systems. In a microservices architecture, there are several common SPOFs, and various strategies can be employed to address them:

1. Database:

• SPOF: A single database instance can be a SPOF if it fails or becomes unreachable.

• Mitigation: Implement database replication and clustering, use database failover solutions, and consider using distributed databases or sharded architectures to improve availability and fault tolerance.

2. Service Registry (e.g., Eureka Server):

• SPOF: If the service registry goes down, services cannot discover each other.

• Mitigation: Deploy the service registry in a cluster mode to ensure high availability. For example, run multiple Eureka Server instances in different availability zones or data centers.

3. API Gateway:

• SPOF: The API Gateway, as a single entry point for client requests, can become a SPOF if it fails.

• Mitigation: Deploy multiple instances of the API Gateway behind a load balancer. Use cloud-based solutions that provide built-in failover and scalability.

4. Load Balancer:

• SPOF: The load balancer itself can become a SPOF if not properly configured.

• Mitigation: Use redundant load balancers in an active-passive or active-active configuration. Many cloud providers offer managed load balancing solutions with built-in redundancy.

5. Authentication/Authorization Service:

• SPOF: If this service fails, users might not be able to authenticate, blocking access to the system.

• Mitigation: Implement redundant instances of these services and use caching strategies (e.g., tokens) to reduce dependency on a centralized service.

6. Network Infrastructure:

• SPOF: Network components like routers, switches, or firewalls can be SPOFs.

• Mitigation: Ensure redundant network paths and hardware are in place. Use virtual networking solutions with failover capabilities.

7. Monolithic Components:

• SPOF: Any monolithic component in a microservices system can be a SPOF.

• Mitigation: Gradually refactor monolithic components into microservices to distribute functionality and reduce dependency on a single point.

8. Physical Infrastructure:

• SPOF: A single data center or server.

• Mitigation: Use multiple availability zones or regions, implement cloud failover strategies, and use hybrid or multi-cloud deployments.

9. Configuration Management:

• SPOF: A single configuration server can be a SPOF.

• Mitigation: Use distributed configuration management tools like Spring Cloud Config Server in a clustered setup, or use cloud-based configuration solutions.

10. Logging and Monitoring:

• SPOF: Centralized logging or monitoring tools can become SPOFs.

• Mitigation: Use distributed logging and monitoring solutions with redundancy and failover capabilities.

Defining a swagger.json file for APIs typically involves using the OpenAPI Specification (formerly known as Swagger Specification) to document the API endpoints, request/response formats, authentication methods, and other relevant details. When working with Spring Boot applications, the process is often simplified using tools like Springfox or Springdoc OpenAPI. Here’s a general approach on how you can define your swagger.json for your APIs:

Using Springfox (for Spring Boot)

1. Add Dependencies:

• Add the Springfox dependencies to your pom.xml (if using Maven) or build.gradle (if using Gradle).

<! - For Maven →
<dependency>
<groupId>io.springfox</groupId>
<artifactId>springfox-boot-starter</artifactId>
<version>3.0.0</version>
</dependency>
2. Enable Swagger:

• Create a configuration class to enable Swagger and define its settings.

import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import springfox.documentation.builders.PathSelectors;
import springfox.documentation.builders.RequestHandlerSelectors;
import springfox.documentation.spi.DocumentationType;
import springfox.documentation.spring.web.plugins.Docket;
import springfox.documentation.swagger2.annotations.EnableSwagger2;
@Configuration
@EnableSwagger2
public class SwaggerConfig {
@Bean
public Docket api() {
return new Docket(DocumentationType.SWAGGER_2)
.select()
.apis(RequestHandlerSelectors.basePackage("com.example.yourpackage"))
.paths(PathSelectors.any())
.build();
}
}
3. Document Your APIs:

• Use annotations like @Api, @ApiOperation, @ApiParam, etc., to document your API endpoints, parameters, and responses within your controller classes.

import io.swagger.annotations.Api;
import io.swagger.annotations.ApiOperation;
import org.springframework.web.bind.annotation.GetMapping;
import org.springframework.web.bind.annotation.RequestMapping;
import org.springframework.web.bind.annotation.RestController;
@RestController
@RequestMapping("/api")
@Api(value = "Sample API", description = "Operations pertaining to sample API")
public class SampleController {
@GetMapping("/hello")
@ApiOperation(value = "Get hello message", response = String.class)
public String getHelloMessage() {
return "Hello, World!";
}
}
4. Access Swagger UI:

• Run your application and access the Swagger UI at http://localhost:8080/swagger-ui/ to visualize and interact with your API documentation. The swagger.json file can usually be 
accessed at http://localhost:8080/v2/api-docs.

Spring Boot applications are designed to start quickly and run efficiently with minimal configuration. Here’s a step-by-step explanation of how a Spring Boot project boots up:

1. Main Method Execution:

• The boot process begins with the execution of the main method in the main application class, which is typically annotated with @SpringBootApplication. This class serves as the 
entry point for the application.

import org.springframework.boot.SpringApplication;

import org.springframework.boot.autoconfigure.SpringBootApplication;

@SpringBootApplication

public class MySpringBootApplication {

public static void main(String[] args) {

SpringApplication.run(MySpringBootApplication.class, args);

}

}

2. SpringApplication Initialization:

• The SpringApplication.run() method is invoked, which creates an instance of SpringApplication. This class is responsible for starting the Spring application context.

3. Application Context Creation:

• Spring Boot determines the type of application context to create (e.g., AnnotationConfigApplicationContext for a standalone application or 
AnnotationConfigEmbeddedWebApplicationContext for a web application).

4. Auto-Configuration:

• The @EnableAutoConfiguration aspect of @SpringBootApplication kicks in. Spring Boot scans the classpath for dependencies and automatically configures beans based on those 
dependencies and your application properties. For example, if you have spring-boot-starter-web in your dependencies, Spring Boot will set up a web server and configure Spring MVC.

5. Component Scanning:

• The @ComponentScan aspect of @SpringBootApplication scans the package where the main application class is located (and its sub-packages) for Spring components, such as 
@Component, @Service, @Repository, and @Controller annotations. These beans are registered in the application context.

6. Bean Initialization:

• Beans are instantiated, and dependencies are injected based on the configuration and annotations. Spring’s dependency injection mechanism is used to wire beans together.

7. CommandLineRunners and ApplicationRunners:

• If any beans implement CommandLineRunner or ApplicationRunner, their run methods are called after the application context has been fully initialized, allowing for additional 
startup logic to be executed.

8. Embedded Server Start (if applicable):

• If the application is a web application, the embedded server (e.g., Tomcat, Jetty, or Undertow) is started. Spring Boot configures the server and binds it to the specified port, 
which can be set in the application.properties file.

9. Application Ready:

• The application is now fully initialized and ready to handle requests. At this point, any additional initializations, such as setting up schedulers or listeners, are completed.

10. Lifecycle Management:

• Spring Boot manages the lifecycle of the application, handling tasks such as shutting down the application context gracefully when the application is stopped.

What all are the status code that you define for APIs?
When designing APIs, especially RESTful APIs, it’s important to use HTTP status codes to indicate the result of a client’s request. Here are some common HTTP status codes that are 
typically used to convey different outcomes:

1xx: Informational

• 100 Continue: Indicates that the initial part of a request has been received and the client should continue with the request.

• 101 Switching Protocols: Indicates that the server is switching protocols as requested by the client.

2xx: Success

• 200 OK: The request was successful, and the server returned the requested resource (typically used for GET requests).

• 201 Created: The request was successful, and a new resource was created (typically used for POST requests).

• 202 Accepted: The request has been accepted for processing, but the processing is not complete.

• 204 No Content: The request was successful, but there is no content to return (often used for DELETE operations).

3xx: Redirection

• 301 Moved Permanently: The resource has been moved to a new URL permanently.

• 302 Found: The resource is temporarily located at a different URL.

• 304 Not Modified: Indicates that the resource has not been modified since the last request, allowing the client to use a cached version.

4xx: Client Errors

• 400 Bad Request: The request is malformed or contains invalid data.

• 401 Unauthorized: Authentication is required, or the provided authentication is invalid.

• 403 Forbidden: The client does not have permission to access the resource.

• 404 Not Found: The requested resource could not be found.

• 405 Method Not Allowed: The HTTP method used is not supported for the resource.

• 409 Conflict: The request could not be completed due to a conflict with the current state of the resource (e.g., duplicate entries).

• 422 Unprocessable Entity: The server understands the request, but it can’t be processed due to semantic errors (often used for validation errors).

5xx: Server Errors

• 500 Internal Server Error: A generic error message indicating that something went wrong on the server.

• 501 Not Implemented: The server does not support the functionality required to fulfill the request.

• 502 Bad Gateway: The server received an invalid response from an upstream server.

• 503 Service Unavailable: The server is currently unavailable (e.g., due to overload or maintenance).

• 504 Gateway Timeout: The server, acting as a gateway, did not receive a timely response from an upstream server.

Usage Considerations

• Consistent Use: Be consistent in how you use status codes. For example, always use 201 Created when a new resource is successfully created.

• Error Details: When returning error status codes (4xx and 5xx), include a message or error object in the response body to provide more context about the error.

• Client Guidance: Use status codes to guide clients on what to do next. For example, 401 Unauthorized should prompt the client to authenticate.

• Caching: Use status codes like 304 Not Modified to take advantage of HTTP caching mechanisms.

What is the difference between spring boot bean creation using ApplicationContext and beanFactory method ?
In Spring, both BeanFactory and ApplicationContext are interfaces used for accessing and managing beans. However, they serve different purposes and offer different levels of 
functionality. Here’s a breakdown of the differences between creating beans using ApplicationContext and BeanFactory:

BeanFactory

• Basic Container: BeanFactory is the simplest container providing basic dependency injection capabilities. It is the root interface for accessing the Spring container.

• Lazy Initialization: By default, BeanFactory creates beans in a lazy manner, meaning that a bean is only instantiated when it is requested for the first time. This can lead to a 
delay the first time a bean is needed.

• Minimal Features: It provides basic features for bean instantiation and dependency injection but does not support advanced features like event propagation, AOP, or declarative 
transactions.

• Use Cases: BeanFactory might be used in scenarios where lightweight containers are necessary, or when memory and resources are severely constrained, but it is generally not 
recommended for applications unless there’s a specific need for it.

ApplicationContext

• Advanced Container: ApplicationContext is a more advanced container that extends BeanFactory. It provides all the features of BeanFactory along with additional enterprise-level 
capabilities.

• Eager Initialization: By default, ApplicationContext pre-instantiates all singleton beans at startup. This ensures that all beans are created and wired together as soon as the 
context is initialized, leading to faster access times during runtime.

• Rich Features: ApplicationContext supports features such as:

• Internationalization (i18n)

• Event propagation

• Application lifecycle callbacks

• Integration with Spring’s AOP

• Declarative transaction management

• Support for loading properties and resource files

• Built-In Implementations: ApplicationContext has several built-in implementations like ClassPathXmlApplicationContext, FileSystemXmlApplicationContext, and 
AnnotationConfigApplicationContext.

• Use Cases: ApplicationContext is the preferred container in most Spring applications because it provides all necessary features for building robust enterprise applications.

Choosing Between ApplicationContext and BeanFactory

• Use ApplicationContext: For most applications, especially those built with Spring Boot, ApplicationContext is the recommended choice because of its comprehensive features and 
proactive bean initialization strategy.

• Use BeanFactory: If you have a specific use case that requires a minimal container with lazy initialization, and you’re not leveraging the additional features provided by 
ApplicationContext, then BeanFactory might be appropriate. However, such use cases are rare in modern Spring applications.

Enums are declared using the enum keyword.

public enum Direction {
 NORTH,
 SOUTH,
 EAST,
 WEST
}
Enum constants are public, static, and final implicitly. We can create Enums inside another class or in a separate file.

We compare Enums using == operator not .equals() method.

How to Iterate Over Enum Values?
for(Direction dir: Direction.values()){
 System.out.println(dir);
}

/*
NORTH
SOUTH
EAST
WEST
*/
In above case values() method returns the array of all predefined values for the Enum.

How do you add values and methods to Enums?
public enum Traffic{
 RED("Stop"),
 GREEN("Go"),
 YELLOW("Wait");

 private final String order;

 Traffic(String order){
  this.order = order;
 }

 public String getOrder() {
  return order;
 }
}
We can get the Enum names and values below.

Traffic traffic = Traffic.GREEN;
System.out.println(traffic + " : " + traffic.getOrder());

// GREEN : Go

What are the benefits of Using Enums?
Type Safety: Since the values of Enums are predefined, invalid values are prevented from being assigned to Enum values.
Readability: Enums make the code simple and cleaner.
Better Maintainability: Enum constants are central, making them easy to maintain and use in the code.

public static void display(int a) {
		System.out.println(a);
		if (a > 0) {
			display(a - 1);
		}
		System.out.println(a);
	}

	@Override
	public void run(String... args) throws Exception {

		int a = 11, b = 2, c;
		c = a / b;

		display(c);

	}
	
	output - 5 4 3 2 1 0 0 1 2 3 4 5
	
	class A
{
   {
       System.out.println(1);
   }
}
 
class B extends A
{
   {
       System.out.println(2);
   }
}
 
class C extends B
{
   {
       System.out.println(3);
   }
}
 
public class MainClass
{
   public static void main(String[] args)
   {
       C c = new C();
   }
}

 1
2
3

class A{
public void display(){
System.out.println("A");
}
}
 
class B extends A{
public void display(){
System.out.println("B");
}
}
 
class Main {
public static void main(String args[]){
B b1 = new A();
b1.display();
}
}

compile time error

public class MyClass {
  public static void main(String args[]) {
    int x=102;
    int y=25;
    int z=x+y;
   
    int sum = doSum(182);
    System.out.println("Sum of num = " + sum);
  }
  public static int doSum(int num){
      int rem=0,sum=0;
   
    while(num>0){
        rem=num%10;
        sum=sum+rem;
        num=num/10;
    }
    if(sum>9){
        num=sum;sum=0;
        while(num>0){
        rem=num%10;
        sum=sum+rem;
        num=num/10;
    }
    }
    return sum;
  }
}

⭕Overview high level :
1. Once we generate log files for our application
2. The log file will be given to Logstash . (Data processing)
3. Logstash will read those log file and sent it to ElasticSearch(Storage).
4. ElasticSearch is no SQL db , it will store the log data .
5. Then Kibana(Visualise) will keep on pulling the log data from ElasticSearch to show it to user interfaces.

Future: Represents a result of an asynchronous computation. It blocks the thread with get().
CompletableFuture: An extension of Future that allows non-blocking, chaining, and combining of asynchronous tasks.

Write a program to find second duplicate number from the given list.
import java.util.*;
import java.util.stream.*;

public class SecondDuplicateFinderWithStream {
    public static void main(String[] args) {
        // Example list of numbers
        List<Integer> numbers = Arrays.asList(5, 3, 8, 3, 2, 1, 8, 7, 2);

        // Find the second duplicate using Stream API
        Optional<Integer> secondDuplicate = findSecondDuplicate(numbers);

        // Print the result
        secondDuplicate.ifPresentOrElse(
            num -> System.out.println("The second duplicate number is: " + num),
            () -> System.out.println("No second duplicate found.")
        );
    }

    public static Optional<Integer> findSecondDuplicate(List<Integer> numbers) {
        Set<Integer> seen = new HashSet<>();

        return numbers.stream()
                .filter(num -> !seen.add(num)) // Filter only duplicate numbers
                .skip(1)                      // Skip the first duplicate
                .findFirst();                 // Find the second duplicate
    }
}
Output:

The second duplicate number is: 8
2. What will be the size of below Hashset?

import java.util.HashSet;

public class Main {
    public static void main(String[] args) {
        HashSet<Integer> set = new HashSet<>();
        set.add(5); // Input 1
        set.add(5); // Input 2 

        System.out.println("HashSet size: " + set.size());
    }
}
Explanation:

As Hashset does not allow duplicate elements, thus the second set.add(5) [in line Input 2] will be rejected and the size of Hashset will be 1.

3. What is the output of below program?

public class Main {
    public static void main(String[] args) {
        String s = "Java";
        s.concat(" World"); 
        System.out.println(s);    }
}
Output:

Java
Explanation:

Strings in Java are immutable. The concat method creates a new String but does not modify the original String (s). Since the new string is not assigned to s, the value of s 
remains "Java".

To update s with the concatenated value, assign the result of concat back to s:

Fixed Code:

public class Main {
    public static void main(String[] args) {
        String s = "Java";
        s = s.concat(" World"); // Assigns the concatenated string back to s
        System.out.println(s); // Prints the updated value of s
    }
}
Output:

Java World

4. What is the output of below program?

try {
    // Code that may throw exceptions
} catch (Exception e) {
    // Print StackTrace
} catch (NullPointerException npe) {
    // Print StackTrace
}
Explanation:

In Java, you cannot catch a larger exception (e.g., Exception) before a smaller one (e.g., NullPointerException) because exception classes follow an inheritance hierarchy. 
NullPointerException is a subclass of Exception, so if you catch Exception first, it would already handle all exceptions of its subclasses (like NullPointerException), leaving the 
smaller, more specific exceptions unreachable.

7. How to create an Immutable class in Java?

Immutable class in java means that once an object is created, we cannot change its content. In Java, all the wrapper classes (like Integer, Boolean, Byte, Short) and String class 
are immutable.

We can create our own immutable class with below steps:

The class must be declared as final so that child classes can’t be created.
Data members in the class must be declared private so that direct access is not allowed.
Data members in the class must be declared as final so that we can’t change the value of it after object creation.
A parameterized constructor should initialize all the fields performing a deep copy so that data members can’t be modified with an object reference.
Deep Copy of objects should be performed in the getter methods to return a copy rather than returning the actual object reference)
Note: There should be no setters or in simpler terms, there should be no option to change the value of the instance variable.

8. In which scenario do we get NoClassDefFound Error?

A NoClassDefFoundError occurs when the JVM or a class loader cannot find a class at runtime, even though it was available during compile-time. This typically happens due to 
missing class files in the classpath, incorrect classpath configuration, or if a class was removed, renamed, or failed to load due to a static initialization error. It can also 
occur if there's a version mismatch between the compiled class and the runtime environment.

10. What is Qualifier annotation and what is it’s alternative?

The @Qualifier annotation in Java is used in dependency injection to specify which bean should be injected when there are multiple candidates of the same type. It helps 
disambiguate between beans that share the same type but are meant for different purposes.

Example:

@Qualifier("serviceA")
@Autowired
private Service service;
In this example, @Qualifier("serviceA") ensures that the serviceA bean is injected, even if there are other Service beans available.

Alternatives:

Primary Annotation (@Primary):
Used to mark a bean as the default choice when multiple beans of the same type are available.
Example:
@Primary 
@Bean 
public Service serviceA() 
{     
  return new ServiceA(); 
}
2. Qualifying by Name:

You can specify a bean by its name directly during injection, without using @Qualifier.
Example:
@Autowired 
@Qualifier("serviceA") 
private Service service;
You can also inject by name directly if you prefer.
In summary, @Qualifier is typically used to resolve ambiguity when multiple beans of the same type exist, and @Primary can be used to mark the default bean.

11. What is the default scope in Spring boot?

The default scope in Spring Boot (and Spring Framework) is singleton. This means that by default, Spring creates only one instance of a bean and uses that single instance across 
the entire Spring container. Every time the bean is needed, the same instance is injected.

@Service
public class MyService {
    // This will be a singleton by default
}
In this example, MyService will be created only once, and Spring will inject the same instance wherever it is required.

12. What is HTTP status code 403? When is it implemented as expected to 401?

HTTP Status Code 403 — Forbidden:

The server understands the request, but the client is not authorized to access the requested resource, even though authentication may have been provided. This means the server is 
refusing to process the request, typically due to insufficient permissions or access restrictions.

403 is used when:

The client is authenticated, but does not have permission to access the resource.
The server knows who the user is, but is deliberately blocking access due to permissions, roles, or restrictions.
401 should be used when:

The client is not authenticated or does not provide valid authentication credentials (e.g., missing or incorrect token, session, or login details).
Example:

403: A user is logged in but tries to access an admin page without admin privileges.
401: A user tries to access a page without logging in, so the server requires authentication.

@DynamicInsert generates SQL INSERT statements that include only the columns with non-null values or values that have been explicitly set. It allows the database to use default 
values for columns not included in the statement.

@DynamicUpdate generates SQL UPDATE statements that include only the columns whose values have changed. It reduces the data sent to the database and can improve performance.

Filter Chain

The request goes through a list of filters in order, before it reaches Dispatcher Servlet. Filters can transform the request or response. These filters can operate on incoming 
requests 
by validating or modifications. We can also write our filters and inject them into the Filter Chain.

Dispatcher Servlet

This is responsible for intercepting requests and dispatching them to appropriate handlers. It uses HandlerMappings to decide which controller is responsible for handling the 
request.

As soon as we add this dependency and restart our application, we see something unusual in logs. We see that a password in generated.

2024-09-28T01:45:59.001+05:30  INFO 5956 --- [security-poc] [           main] w.s.c.ServletWebServerApplicationContext : Root WebApplicationContext: initialization completed in 489 ms
2024-09-28T01:45:59.056+05:30  WARN 5956 --- [security-poc] [           main] .s.s.UserDetailsServiceAutoConfiguration : 

Using generated security password: ad1f45ff-70e2-4fed-ae8c-1184b095eeac

This generated password is for development use only. Your security configuration must be updated before running your application in production.

2024-09-28T01:45:59.069+05:30  INFO 5956 --- [security-poc] [           main] r$InitializeUserDetailsManagerConfigurer : Global AuthenticationManager configured with UserDetailsService bean with name inMemoryUserDetailsManager
2024-09-28T01:45:59.243+05:30  INFO 5956 --- [security-poc] [           main] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat started on port 9095 (http) with context path '/security-poc'
2024-09-28T01:45:59.249+05:30  INFO 5956 --- [security-poc] [           main] d.s.security_poc.SecurityPocApplication  : Started SecurityPocApplication in 0.987 seconds (process running for 1.188)
This is because we have not specified any password in properties file. Let us define username and password as below in application.properties file:

spring.security.user.name=user
spring.security.user.password=user123
When we try to access any end-points we are presented with login form for username and password.

How Security is enabled by just adding a dependency?
Spring Security adds filters to the filter chain. All HTTP requests go through these filters. Each filter performs a specific task and passes control to the next filter.

These are some filters added when we add Spring Security dependency:

SecurityContextPersistenceFilter
UsernamePasswordAuthenticationFilter
BasicAuthenticationFilter
ExceptionTranslationFilter
BearerTokenAuthenticationFilter
SessionManagementFilter
CsrfFilter
RequestCacheAwareFilter
AuthorizationFilter

Immutable @ Data: @ Value
If we want that all our fields are private and final by default without any setter we can use @ Value as below

@Value
public class UserInfo {
    private String username;
    private String password;
    private String email;
}
No More NullPointerException : @ NonNull
This annotation is used to generate null-check statement for any method or constructor parameter.

public void setEmail(@NonNull String email){
 this.email = email;
}

A filter is part of the Servlet API. In Spring Boot, it is used to process HTTP requests globally, typically before they are handled by the Spring framework.
while interceptor are executed before any controller call.

pring Boot interceptors are a powerful mechanism to intercept HTTP requests and responses in your application.

They are part of the Spring Web MVC framework and allow you to perform operations before and after the processing of a request by a controller.

Creating an Interceptor

To create an interceptor, you need to implement the HandlerInterceptor interface or extend the HandlerInterceptorAdapter class.

package com.javacodeex.interceprots;

import jakarta.servlet.http.HttpServletRequest;
import jakarta.servlet.http.HttpServletResponse;
import lombok.extern.slf4j.Slf4j;
import org.springframework.stereotype.Component;
import org.springframework.web.servlet.HandlerInterceptor;
import org.springframework.web.servlet.ModelAndView;

@Slf4j
@Component
public class LoggingInterceptor implements HandlerInterceptor {


    @Override
    public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception {
        log.info("Request URL: " + request.getRequestURL());
        log.info("Request Method: " + request.getMethod());
        return true;
    }

    @Override
    public void postHandle(HttpServletRequest request, HttpServletResponse response, Object handler, ModelAndView modelAndView) throws Exception {
        // Can log response details here
    }

    @Override
    public void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, Exception exception) throws Exception {
        // Can log completion details here
        log.info("Request Process completed");

    }
}
package com.javacodeex.interceprots;

import jakarta.servlet.http.HttpServletRequest;
import jakarta.servlet.http.HttpServletResponse;
import lombok.extern.slf4j.Slf4j;
import org.springframework.stereotype.Component;
import org.springframework.web.servlet.HandlerInterceptor;
import org.springframework.web.servlet.ModelAndView;

@Slf4j
@Component
public class ProductInterceptor implements HandlerInterceptor {


    @Override
    public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception {
        log.info("Request URL: " + request.getRequestURL());
        log.info("Request Method: " + request.getMethod());
        return true;
    }

    @Override
    public void postHandle(HttpServletRequest request, HttpServletResponse response, Object handler, ModelAndView modelAndView) throws Exception {
        // Can log response details here
    }

    @Override
    public void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, Exception exception) throws Exception {
        // Can log completion details here
        log.info("Request Process completed");

    }
}
Registering the Interceptor

package com.javacodeex.config;

import com.javacodeex.interceprots.LoggingInterceptor;
import com.javacodeex.interceprots.ProductInterceptor;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.context.annotation.Configuration;
import org.springframework.web.servlet.config.annotation.InterceptorRegistry;
import org.springframework.web.servlet.config.annotation.WebMvcConfigurer;

@Configuration
public class WebConfig implements WebMvcConfigurer {
    @Autowired
    private LoggingInterceptor loggingInterceptor;

    @Autowired
    private ProductInterceptor productInterceptor;

    @Override
    public void addInterceptors(InterceptorRegistry registry) {
        registry.addInterceptor(loggingInterceptor)
                .addPathPatterns("/api/**") // Define the URL patterns to be intercepted
                .excludePathPatterns("/api/auth/**"); // Define the URL patterns to be excluded from interception

        registry.addInterceptor(productInterceptor)
                .addPathPatterns("/product/**") // Define the URL patterns to be intercepted
                .excludePathPatterns("/api/auth/**");
    }
}
package com.javacodeex.controller;

import lombok.extern.slf4j.Slf4j;
import org.springframework.web.bind.annotation.GetMapping;
import org.springframework.web.bind.annotation.RequestMapping;
import org.springframework.web.bind.annotation.RestController;

@RestController
@RequestMapping("/api")
@Slf4j
public class CustomerController {

    @GetMapping
    public String getMessage(){
        log.info("getMessage()");
        return "welcomoe to the interceptors session";
    }
}
package com.javacodeex.controller;

import lombok.extern.slf4j.Slf4j;
import org.springframework.web.bind.annotation.GetMapping;
import org.springframework.web.bind.annotation.RequestMapping;
import org.springframework.web.bind.annotation.RestController;

@RestController
@RequestMapping("/product")
@Slf4j
public class ProductController {

    @GetMapping
    public String getProduct(){

        log.info("getProduct()");
        return "product information";
    }
}
Spring Boot Interceptor Lifecycle

preHandle: Called before the actual handler is executed. This method can be used for pre-processing (e.g., logging, authentication, etc.). If it returns false, the execution chain 
is halted.
postHandle: Called after the handler is executed but before the ModelAndView is rendered. This method can be used for modifying the ModelAndView before it is rendered.
afterCompletion: Called after the complete request has finished. This method can be used for cleanup activities.
Filters and interceptors are both mechanisms for processing HTTP requests and responses in a Spring Boot application, but they operate at different levels and serve different 
purposes. Here are the key differences between them:

What is type erasure in Java?
Type erasure is a mechanism in Java where generic types are replaced with their bounds or Object during runtime. This means the generic type information is removed after 
compilation to ensure backward compatibility with older versions of Java.

List<String> list = new ArrayList<>();

At runtime, list is treated as List without the <String> type information.
Why can’t you create an array of generic types? Why does Java disallow creating arrays of generic types?
Generic types in Java are not reified, meaning their type information is erased at runtime. Arrays, on the other hand, preserve their type information at runtime. Allowing arrays 
of generic types would break type safety.

List<String>[] listArray = new ArrayList<String>[10]; // Compile-time error

This works for situations where we aren’t looking for a result of the thread execution, such as incoming events logging:

public interface Runnable {
    public void run();
}
Copy
Let’s understand this with an example:

public class EventLoggingTask implements  Runnable{
    private Logger logger
      = LoggerFactory.getLogger(EventLoggingTask.class);

    @Override
    public void run() {
        logger.info("Message");
    }
}
Copy
In this example, the thread will just read a message from the queue and log it in a log file. There’s no value returned from the task.

We can launch the task using ExecutorService:

public void executeTask() {
    executorService = Executors.newSingleThreadExecutor();
    Future future = executorService.submit(new EventLoggingTask());
    executorService.shutdown();
}
Copy
In this case, the Future object will not hold any value.

With Callable
The Callable interface is a generic interface containing a single call() method that returns a generic value V:

public interface Callable<V> {
    V call() throws Exception;
}
Copy
Let’s look at calculating the factorial of a number:

public class FactorialTask implements Callable<Integer> {
    int number;

    // standard constructors

    public Integer call() throws InvalidParamaterException {
        int fact = 1;
        // ...
        for(int count = number; count > 1; count--) {
            fact = fact * count;
        }

        return fact;
    }
}
Copy
The result of call() method is returned within a Future object:

@Test
public void whenTaskSubmitted_ThenFutureResultObtained(){
    FactorialTask task = new FactorialTask(5);
    Future<Integer> future = executorService.submit(task);
 
    assertEquals(120, future.get().intValue());
}

Asynchronous computation is difficult to reason about. Usually, we want to think of any computation as a series of steps, but in the case of asynchronous computation, actions 
represented as callbacks tend to be either scattered across the code or deeply nested inside each other. Things get even worse when we need to handle errors that might occur during 
one of the steps.

The Future interface was added in Java 5 to serve as a result of an asynchronous computation, but it did not have any methods to combine these computations or handle possible 
errors.

Java 8 introduced the CompletableFuture class. Along with the Future interface, it also implemented the CompletionStage interface. This interface defines the contract for an 
asynchronous computation step that we can combine with other steps.

CompletableFuture<String> completableFuture
  = CompletableFuture.supplyAsync(() -> "Hello");

CompletableFuture<String> future = completableFuture
  .thenApply(s -> s + " World");

assertEquals("Hello World", future.get());

What is Autoboxing?
Autoboxing is the automatic conversion that the Java compiler makes between primitive types (like int, double, char, etc.) and their corresponding wrapper classes 
(Integer, Double, Character, etc.).

Why does it matter?
In object-oriented programming, there are cases where only objects are allowed. For instance, in collections like ArrayList or HashMap, primitive types cannot be directly used. 
Instead of manually converting between primitives and wrapper classes, Java handles it for you behind the scenes.

Let’s take a look at a simple example of autoboxing:

List<Integer> integerList = new ArrayList<>();
integerList.add(5);  // Autoboxing happens here
Even though 5 is a primitive int, the compiler automatically converts it to its wrapper class, Integer. Internally, the compiler transforms the above code into:

integerList.add(Integer.valueOf(5));  // Autoboxing converts int to Integer
When does Autoboxing Occur?
The compiler applies autoboxing in the following scenarios:

When a primitive is passed as a parameter to a method that expects an object of its corresponding wrapper class.
When a primitive is assigned to a variable of the corresponding wrapper class.
Real-World Example of Autoboxing:
Imagine you’re working with a collection that stores objects. Here’s how autoboxing can simplify your code:

Map<String, Integer> studentScores = new HashMap<>();
studentScores.put("Alice", 90);  // Autoboxing converts int 90 to Integer object
Without autoboxing, you would have to manually convert the primitive int into an Integer object, which would make the code more verbose.

Best Practices to Avoid Performance Issues:
Use primitives where possible: If you don’t need the extra functionality of wrapper classes (like null values), prefer primitive types to avoid unnecessary object creation.
Be cautious with collections: Collections like ArrayList and HashMap can only store objects, so autoboxing is unavoidable. However, keep an eye on performance when working with 
large datasets.
Watch out for loops: Autoboxing and unboxing in tight loops can degrade performance. In critical sections of code, manually converting between primitives and wrapper objects can 
sometimes be more efficient.

In Java, a "synchronized method" locks the entire method body, preventing multiple threads from executing that method simultaneously, while a "synchronized block" allows you to 
specify a specific part of a method to be synchronized, giving you finer-grained control over thread access by locking only that critical section of code on a chosen object; 
essentially, a synchronized block is a more flexible way to manage thread synchronization within a method. 
Example:
Code

class Counter {

    private int count = 0;



    // Synchronized method - locks the entire 'increment' method

    public synchronized void increment() {

        count++;

    }



    // Synchronized block - only locks the critical section of code updating 'count'

    public void safeIncrement() {

        synchronized (this) { // Lock on 'this' object

            count++;

        }

    }

}


Key points to remember: 
Granularity:
A synchronized block allows for more precise control over which part of a method is synchronized, while a synchronized method locks the entire method body.
Lock Object:
When using a synchronized block, you explicitly specify the object to lock on using the synchronized(object) syntax, whereas a synchronized method automatically uses the "this" 
object as the lock. 
When to use which: 
Use a synchronized method:
When you need to synchronize the entire operation of a method, and the entire method body is considered critical. 
Use a synchronized block:
When only a specific part of a method needs to be synchronized, allowing other parts to execute concurrently. 

Concurrency can be achieved using various approaches, such as multithreading, multiprocessing, or asynchronous programming.

Multithreading involves running multiple threads within a single process. Threads share the same memory space but run “concurrently” by taking turns executing.

Labeled break statements in Java are powerful tools for controlling complex loops. They provide a clean and efficient way to exit multiple loops without resorting to convoluted 
logic. By understanding how and when to use them, you can write clearer, more maintainable code.

Consider a scenario where you have nested loops, and you want to exit an outer loop based on a condition met in an inner loop.

Problem Example Without Labeled Break:

for (int i = 0; i < 5; i++) {
    for (int j = 0; j < 5; j++) {
        if (i * j > 6) {
            break; // Only breaks out of the inner loop
        }
        System.out.println("i = " + i + ", j = " + j);
    }
}
In this code, the break statement only exits the inner for loop. The outer loop continues to run, which might not be what you want.

Introducing Labeled Break Statements
A labeled break statement allows you to specify which loop you want to break out of by labeling it.

Syntax:

labelName:
loop {
    // Code
    if (condition) {
        break labelName;
    }
}
Example with Labeled Break:

outerLoop:
for (int i = 0; i < 5; i++) {
    innerLoop:
    for (int j = 0; j < 5; j++) {
        if (i * j > 6) {
            break outerLoop; // Exits the outer loop
        }
        System.out.println("i = " + i + ", j = " + j);
    }
}
Output:

i = 0, j = 0
i = 0, j = 1
i = 0, j = 2
i = 0, j = 3
i = 0, j = 4
i = 1, j = 0
i = 1, j = 1
i = 1, j = 2
i = 1, j = 3
i = 1, j = 4
i = 2, j = 0
i = 2, j = 1
i = 2, j = 2
i = 2, j = 3
When i * j becomes greater than 6, the break outerLoop; statement exits the outer loop entirely.

Atomic classes (e.g., AtomicInteger, AtomicBoolean): Classes in the java.util.concurrent.atomic package that provide atomic operations on single variables, ensuring visibility 
without explicit locking.

Recommended Caching Solutions:
Redis: A highly popular, in-memory key-value store known for its speed and versatility.

Supports advanced data structures like lists, sets, hashes, and sorted sets.

Provides persistence options, allowing data to be saved to disk.

Ideal for use cases like leaderboards, real-time analytics, pub/sub messaging, and session storage.

Memcached: A lightweight, distributed, in-memory caching solution.

Requires minimal setup and overhead.

Best suited for applications with straightforward caching requirements, such as storing session data or query results.

 What is the difference between IntStream and Stream of Integer?

int Stream - operates of primitives while Stream of Integer operates on wrapper class.
Stram of Integer requires overhead for autoboxing and unboxing adding overhead and reduces performance.

What is the output of the below code?

class A {
    public void m(String p) {
        System.out.println("String method called");
    }

    public void m(Object o) {
        System.out.println("Object method called");
    }

    public static void main(String[] args) {
        A a = new A();
        a.m(null);
    }
}
Output:
String method called
Explanation:
When you call a.m(null);, Java needs to determine which method to invoke between m(String p) and m(Object o). Since String is more specific (it's a subclass of Object), 
Java prefers the m(String p) method. Java always chooses the most specific method when null is passed.

Case 2: A a = new B();

This is valid because B is a subclass of A, and in Java, you can assign an object of a subclass (B) to a reference of its superclass (A), which is known as upcasting.

What will be the output of below code:

import java.util.HashMap;
import java.util.Map;

class Employee {
    int id;
    String name;
    // Constructor to initialize Employee object
    public Employee(int id, String name) {
        this.id = id;
        this.name = name;
    }
    // Override hashCode() to calculate hash based on id and name
    @Override
    public int hashCode() {
        return id + name.hashCode(); // Hash code based on id and name
    }
    // Override equals() to compare Employee objects based on id and name
    @Override
    public boolean equals(Object obj) {
        if (this == obj) return true; // Same reference check
        if (obj == null || getClass() != obj.getClass()) return false; // Null check and class type check
        Employee employee = (Employee) obj;
        return id == employee.id && name.equals(employee.name); // Compare id and name
    }
    // Override toString() for better representation
    @Override
    public String toString() {
        return "Employee{id=" + id + ", name='" + name + "'}";
    }
}
public class Main {
    public static void main(String[] args) {
        // Creating a new HashMap with Employee as key and Integer as value (salary)
        Map<Employee, Integer> salaryMap = new HashMap<>();
        // Adding employees to the map with their respective salaries
        salaryMap.put(new Employee(1, "Ram"), 1000); // Adding first employee
        salaryMap.put(new Employee(1, "Ram"), 200);  // Adding second employee (same id and name)
        // Retrieving the salary using the same Employee object (id=1, name="Ram")
        System.out.println("Salary of employee Ram: " + salaryMap.get(new Employee(1, "Ram")));
    }
}
Output:
Salary of employee Ram: 200
Explanation of Output:
First put() Call:
The first put() inserts an employee object with id=1 and name="Ram" into the map with a salary of 1000.
2. Second put() Call:

The second put() call inserts another employee with the same id=1 and name="Ram", but the salary is 200.
Since both employee objects are considered equal (as defined by the equals() and hashCode() methods), the second call overwrites the first one.
3. Get Salary:

The salaryMap.get(new Employee(1, "Ram")) retrieves the salary of the employee with id=1 and name="Ram".
Despite the fact that the get() method creates a new Employee object, the map treats it as equal to the employee already in the map, so the most recently inserted salary value 
(200) is returned.
Thus, the final output shows “Salary of employee Ram: 200” because the salary was updated in the map during the second put() call.

What are the differences between Bean and Component
@Bean - defines beans in @Configuration class.
works at method level in @Configuration.
explicitly registers beans.
for manually onfiguring third party or external classes.

@Component - defines beans bu annotating a class directly.
works at class level.
automatically detects during component scanning.
for automatic registeration of application classes.

Understanding Inheritance in JPA Entities
Inheritance allows us to define common fields and behaviors in a base class, which can be extended by other entities. In JPA (Java Persistence API), you can define an abstract 
base 
entity with shared properties.

Example: BaseEntity Class
import jakarta.persistence.*;
import org.hibernate.envers.Audited;
import java.time.LocalDateTime;

@MappedSuperclass
@Audited
public abstract class BaseEntity {
    
    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    private Long id;

    @Column(name = "created_at", updatable = false)
    private LocalDateTime createdAt;

    @Column(name = "updated_at")
    private LocalDateTime updatedAt;

    @PrePersist
    public void prePersist() {
        createdAt = LocalDateTime.now();
    }

    @PreUpdate
    public void preUpdate() {
        updatedAt = LocalDateTime.now();
    }

    // Getters and Setters
}
@MappedSuperclass ensures that the fields in BaseEntity are inherited by other entities.
@PrePersist and @PreUpdate annotations are used to automatically set the timestamps.
3. Entity-Specific Inheritance
Once you have a BaseEntity, individual entities can extend it and inherit common properties. Each entity can have its own specific attributes while also getting the audit fields 
from the base class.

Here’s how you can create the Product and Order entities by extending the BaseEntity you defined.

1. Product Entity
import jakarta.persistence.Entity;
import jakarta.persistence.Table;
import lombok.Getter;
import lombok.Setter;

@Entity
@Table(name = "products")
@Getter
@Setter
public class ProductEntity extends BaseEntity {

    private String name;
    private String description;
    private Double price;

    // Additional product-specific fields and methods
}
2. Order Entity
import jakarta.persistence.Entity;
import jakarta.persistence.Table;
import lombok.Getter;
import lombok.Setter;

@Entity
@Table(name = "orders")
@Getter
@Setter
public class OrderEntity extends BaseEntity {

    private String orderNumber;
    private Long customerId;
    private Double totalAmount;

    // Additional order-specific fields and methods
}
Explanation:
Both entities extend BaseEntity, inheriting common fields like id, createdAt, updatedAt, createdBy, updatedBy, version, and status.
You can customize the ProductEntity and OrderEntity with specific fields such as name, price for the product, and orderNumber, customerId, totalAmount for the order.
The auditing fields (createdAt, updatedAt, createdBy, updatedBy) are automatically managed by the BaseEntity class using the @PrePersist and @PreUpdate annotations.

Conclusion
By using inheritance in JPA entities, you can avoid redundancy and manage common properties in a single base class. Combining this with Spring Data JPA’s auditing features provides 
a powerful solution for tracking entity creation and updates in your application.

Will the following code compile? If not, why?

class Parent {
    protected void show() {
        System.out.println("Parent show() method");
    }
}
class Child extends Parent {
    private void show() {  // Cannot reduce visibility of overridden method
        System.out.println("Child show() method");
    }
}
public class Main {
    public static void main(String[] args) {
        Parent obj = new Child();
        obj.show();
    }
}
Output:
Compilation Error: show() in Child cannot override show() in Parent; attempting to reduce the visibility from protected to private.

Explanation:
In Java, when overriding a method, the access modifier cannot be more restrictive than the parent class method. The Parent class show() method is protected, but in the Child class, 
it's private, which is not allowed. The compiler throws an error.

What will be the output of the following code?

class Parent {
    Parent() {
        System.out.println("Parent Constructor");
    }
}

class Child extends Parent {
    Child() {
        super();  // Calls the Parent class constructor
        System.out.println("Child Constructor");
    }
}
public class Main {
    public static void main(String[] args) {
        Child obj = new Child();
    }
}
Output:

Parent Constructor
Child Constructor
Explanation:
Constructors cannot be overridden. In this case, the Child class constructor calls the Parent class constructor using super(). This prints the message from the Parent constructor 
first, followed by the message from the Child constructor.

Given the following code, will it compile or give a compilation error?

class Parent {
    String show() {
        return "Parent show() method";
    }
}

class Child extends Parent {
    int show() {  // Return type differs, this will cause an error
        return 10;
    }
}
public class Main {
    public static void main(String[] args) {
        Child obj = new Child();
        System.out.println(obj.show());
    }
}
Output:
Compilation Error: show() in Child cannot override show() in Parent; attempting to use incompatible return type int.

Explanation:
In Java, for method overriding, the return type must be the same or a subclass of the return type in the parent class. Here, the parent class show() returns a String, but the child 
class show() returns an int, causing a compilation error.

What will be the output of the following code?

class Parent {
    void show(int num) {
        System.out.println("Parent show() method with int: " + num);
    }
}
class Child extends Parent {
    @Override
    void show(int num) {
        System.out.println("Child show() method with int: " + num);
    }
}
public class Main {
    public static void main(String[] args) {
        Parent obj = new Child();
        obj.show(10);
    }
}
Output:

Child show() method with int: 10
Explanation:
The method is overridden with the same parameter type (int). At runtime, the method from the Child class is called due to polymorphism, printing "Child show() method with int: 10".

What will be the output of the following code?

class Parent {
    void show() {
        System.out.println("Parent show() method");
    }
}
class Child extends Parent {
    @Override
    void show() {
        super.show();  // Calls the Parent class method
        System.out.println("Child show() method");
    }
}
public class Main {
    public static void main(String[] args) {
        Child obj = new Child();
        obj.show();
    }
}
Output:

Parent show() method
Child show() method
Explanation:
The super.show() in the Child class explicitly calls the show() method from the Parent class before executing the Child class's own logic. Therefore, the output is "Parent show() 
method" followed by "Child show() method".

Given the following code, what will happen?

interface Interface1 {
    default void show() {
        System.out.println("Interface1 show() method");
    }
}

interface Interface2 {
    default void show() {
        System.out.println("Interface2 show() method");
    }
}
class Child implements Interface1, Interface2 {
    @Override
    public void show() {
        Interface1.super.show();  // Resolving the conflict
    }
}
public class Main {
    public static void main(String[] args) {
        Child obj = new Child();
        obj.show();
    }
}
Output:

Interface1 show() method
Explanation:
When a class implements multiple interfaces with the same default method, it must override the method to resolve the conflict. Here, the Child class explicitly calls 
Interface1.super.show(), so the output is "Interface1 show() method".

What will happen when you try to call a constructor from the subclass after the superclass constructor?

class Parent {
    Parent() {
        System.out.println("Parent Constructor");
    }
}
class Child extends Parent {
    Child() {
        super();  // Calls Parent constructor explicitly
        System.out.println("Child Constructor");
    }
}
public class Main {
    public static void main(String[] args) {
        Child obj = new Child();
    }
}
Output:

Parent Constructor
Child Constructor
Explanation:
The super() keyword is used in the Child constructor to invoke the Parent class constructor. As a result, "Parent Constructor" is printed first, followed by "Child Constructor".

What will be the output when trying to override a final method?

class Parent {
    final void show() {
        System.out.println("Parent show() method");
    }
}
class Child extends Parent {
    @Override
    void show() {  // Compilation Error: Cannot override final method
        System.out.println("Child show() method");
    }
}
public class Main {
    public static void main(String[] args) {
        Parent obj = new Child();
        obj.show();
    }
}
Output:

Compilation Error: Cannot override final method show() in Parent.

Explanation:
The final keyword in Java prevents a method from being overridden. In this case, the show() method in the Parent class is final, so the compiler throws an error when trying to 
override it in the Child class.

Materialized View
A materialized view physically stores the result of a query, allowing faster data access. Unlike a regular view, a materialized view holds a copy of the data.
Materialized views are used to optimize performance for expensive queries, especially when the data doesn’t change often.
Querying data from a materialized view is much faster because the result set is precomputed and stored. It’s especially effective for complex queries that involve aggregations or 
joins over large datasets.
Since the data is stored separately from the base table(s), a materialized view requires manual or scheduled refreshing to keep the data in sync. Additionally, materialized views 
consume storage space.

When to Use Materialized Views
Materialized views are perfect when:

You have large datasets that don’t change frequently.
Performance is critical, and you need quick access to data.
The underlying query is expensive to run repeatedly.
They should be avoided when:

Data changes frequently, as constant refreshing would negate the performance benefits.
Storage space is a concern since materialized views can take up significant storage.

#Steps to create your own annotation - 
@Retention: Specifies how long the annotation should be retained, either at runtime or source level.
@Target: Defines where the annotation can be applied, such as methods, fields, or classes.
@Documented: Ensures that the annotation is included in the generated Javadoc.

Step 2: Implement Custom Annotation Logic Using AOP

Now that we have defined the annotation, we need to implement the behavior it will trigger. We can use Spring AOP to intercept method invocations and log execution time. Here’s how to implement this with an aspect:

@Aspect
@Component
public class LoggingAspect {
    @Around("@annotation(com.example.annotations.LogExecutionTime)")
    public Object logExecutionTime(ProceedingJoinPoint joinPoint) throws Throwable {
        long startTime = System.currentTimeMillis();
        
        Object proceed = joinPoint.proceed();
        
        long executionTime = System.currentTimeMillis() - startTime;
        System.out.println(joinPoint.getSignature() + " executed in " + executionTime + "ms");
        
        return proceed;
    }
}
In this code:

The @Aspect annotation defines a Spring AOP aspect.
The @Around advice intercepts methods annotated with @LogExecutionTime, capturing the execution time before and after the method runs.
Step 3: Apply Your Custom Annotation

Now that both the annotation and its behavior are implemented, you can apply it to any method in your Spring application:

@Service
public class UserService {
    @LogExecutionTime
    public void performTask() {
        // Method logic
    }
}
Every time the performTask() method is invoked, the aspect will log how long it takes to execute.
